oooooooooo001111:Computer System Design
July 1983
Butler W. Lampson
Computer Science Laboratory
Xerox Palo Alto Research Center
Palo Alto, CA 94304

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf

This paper was originally presented at the 9th ACM Symposium on Operating
Systems Principles and appeared in Operating Systems Review 15, 5, Oct.
1983, p 33-48. The present version is slightly revised.

Abstract

Studying the design and implementation of a number of computer has led to
some general hints for system design. They are described here and
illustrated by many examples, ranging from hardware such as the Alto and
the Dorado to application programs such as Bravo and Star.

1. Introduction

Designing a computer system is very different from designing an algorithm:

    The external interface (that is, the requirement) is less precisely
    defined, more complex, and more subject to change.

    The system has much more internal structure, and hence many internal interfaces.

    The measure of success is much less clear.

The designer usually finds himself floundering in a sea of possibilities,
unclear about how one choice will limit his freedom to make other choices,
or affect the size and performance of the entire system. There probably
isn't a ‘best' way to build the system, or even any major part of it; much
more important is to avoid choosing a terrible way, and to have clear
division of responsibilities among the parts.

I have designed and built a number of computer systems, some that worked
and some that didn't.  I have also used and studied many other systems,
both successful and unsuccessful. From this experience come some general
hints for designing successful systems. I claim no originality for them;
most are part of the folk wisdom of experienced designers. Nonetheless,
even the expert often forgets, and after the second system [6] comes the
fourth one.

Disclaimer: These are not
    novel (with a few exceptions),
    foolproof recipes,
    laws of system design or operation,
    precisely formulated,
    consistent,
    always appropriate,
    approved by all the leading experts, or
    guaranteed to work.

They are just hints. Some are quite general and vague; others are specific
techniques which are more widely applicable than many people know. Both the
hints and the illustrative examples are necessarily oversimplified. Many
are controversial.

I have tried to avoid exhortations to modularity, methodologies for
top-down, bottom-up, or iterative design, techniques for data abstraction,
and other schemes that have already been widely disseminated. Sometimes I
have pointed out pitfalls in the reckless application of popular methods
for system design.

The hints are illustrated by a number of examples, mostly drawn from
systems I have worked on.  They range from hardware such as the Ethernet
local area network and the Alto and Dorado personal computers, through
operating systems such as the SDS 940 and the Alto operating system and
programming systems such as Lisp and Mesa, to application programs such as
the Bravo editor and the Star office system and network servers such as the
Dover printer and the Grapevine mail system. I have tried to avoid the most
obvious examples in favor of others which show unexpected uses for some
well-known methods. There are references for nearly all the specific
examples but for only a few of the ideas; many of these are part of the
folklore, and it would take a lot of work to track down their multiple
sources.

    And these few precepts in thy memory
    Look thou character.

It seemed appropriate to decorate a guide to the doubtful process of system
design with quotations from Hamlet. Unless otherwise indicated, they are
taken from Polonius' advice to Laertes (I iii 58-82). Some quotations are
from other sources, as noted. Each one is intended to apply to the text
which follows it.

Each hint is summarized by a slogan that when properly interpreted reveals
the essence of the hint. Figure 1 organizes the slogans along two axes:

    Why it helps in making a good system: with functionality (does it
    work?), speed (is it fast enough?), or fault-tolerance (does it keep
    working?).

    Where in the system design it helps: in ensuring completeness, in
    choosing interfaces, or in devising implementations.

Fat lines connect repetitions of the same slogan, and thin lines connect
related slogans.

Figure 1: Summary of the slogans (see
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf)

 Why? Functionality
Does it work?
Speed
Is it fast enough?
Fault-tolerance
Does it keep working?
Where?
Completeness Separate normal and
 worst case
Shed load
End-to-end
Safety first
End-to-end
Interface Do one thing well:
 Don't generalize
 Get it right
 Don't hide power
 Use procedure arguments
 Leave it to the client
Keep basic interfaces stable
Keep a place to stand
Make it fast
Split resources
Static analysis
Dynamic translation
End-to-end
Log updates
Make actions atomic
Implementation Plan to throw one away
Keep secrets
Use a good idea again
Divide and conquer
Cache answers
Use hints
Use brute force
Compute in background
Batch processing
Make actions atomic
Use hints

The body of the paper is in three sections, according to the why headings:
functionality (section 2), speed (section 3), and fault-tolerance (section
4).

2. Functionality

The most important hints, and the vaguest, have to do with obtaining the
right functionality from a system, that is, with getting it to do the
things you want it to do. Most of these hints depend on the notion of an
interface that separates an implementation of some abstraction from the
clients who use the abstraction. The interface between two programs
consists of the set of assumptions that each programmer needs to make about
the other program in order to demonstrate the correctness of his program
(paraphrased from [5]). Defining interfaces is the most important part of
system design. Usually it is also the most difficult, since the interface
design must satisfy three conflicting requirements: an interface should be
simple, it should be complete, and it should admit a sufficiently small and
fast implementation.  Alas, all too often the assumptions embodied in an
interface turn out to be misconceptions instead. Parnas' classic paper [38]
and a more recent one on device interfaces [5] offer excellent practical
advice on this subject.

The main reason interfaces are difficult to design is that each interface
is a small programming language: it defines a set of objects and the
operations that can be used to manipulate the objects.  Concrete syntax is
not an issue, but every other aspect of programming language design is
present. Hoare's hints on language design [19] can thus be read as a
supplement to this paper.

2.1 Keep it simple

    Perfection is reached not when there is no longer anything to add, but when
    there is no longer anything to take away. (A. Saint-Exupery)

    Those friends thou hast, and their adoption tried,
    Grapple them unto thy soul with hoops of steel;
    But do not dull thy palm with entertainment
    Of each new-hatch'd unfledg'd comrade.

• Do one thing at a time, and do it well. An interface should capture the
minimum essentials of an abstraction. Don't generalize; generalizations are
generally wrong.

We are faced with an insurmountable opportunity. (W. Kelley)

When an interface undertakes to do too much its implementation will
probably be large, slow and complicated. An interface is a contract to
deliver a certain amount of service; clients of the interface depend on the
contract, which is usually documented in the interface specification.  They
also depend on incurring a reasonable cost (in time or other scarce
resources) for using the interface; the definition of ‘reasonable' is
usually not documented anywhere. If there are six levels of abstraction,
and each costs 50% more than is ‘reasonable', the service delivered at the
top will miss by more than a factor of 10.

    KISS: Keep It Simple, Stupid. (Anonymous)

    If in doubt, leave if out. (Anonymous)

    Exterminate features. (C. Thacker)

On the other hand,

    Everything should be made as simple as possible, but no simpler. (A. Einstein)

Thus, service must have a fairly predictable cost, and the interface must
not promise more than the implementer knows how to deliver. Especially, it
should not promise features needed by only a few clients, unless the
implementer knows how to provide them without penalizing others. A better
implementer, or one who comes along ten years later when the problem is
better understood, might be able to deliver, but unless the one you have
can do so, it is wise to reduce your aspirations.

For example, PL/1 got into serious trouble by attempting to provide
consistent meanings for a large number of generic operations across a wide
variety of data types. Early implementations tended to handle all the cases
inefficiently, but even with the optimizing compilers of 15 years later, it
is hard for the programmer to tell what will be fast and what will be slow
[31]. A language like Pascal or C is much easier to use, because every
construct has a roughly constant cost that is independent of context or
arguments, and in fact most constructs have about the same cost.

Of course, these observations apply most strongly to interfaces that
clients use heavily, such as virtual memory, files, display handling, or
arithmetic. It is all right to sacrifice some performance for functionality
in a seldom used interface such as password checking, interpreting user
commands, or printing 72 point characters. (What this really means is that
though the cost must still be predictable, it can be many times the minimum
achievable cost.) And such cautious rules don't apply to research whose
object is learning how to make better implementations. But since research
may well fail, others mustn't depend on its success.

    Algol 60 was not only an improvement on its predecessors, but also on
    nearly all its successors. (C. Hoare)

Examples of offering too much are legion. The Alto operating system [29]
has an ordinary read/write-n-bytes interface to files, and was extended for
Interlisp-D [7] with an ordinary paging system that stores each virtual
page on a dedicated disk page. Both have small implementations (about 900
lines of code for files, 500 for paging) and are fast (a page fault takes
one disk access and has a constant computing cost that is a small fraction
of the disk access time, and the client can fairly easily run the disk at
full speed). The Pilot system [42] which succeeded the Alto OS follows
Multics and several other systems in allowing virtual pages to be mapped to
file pages, thus subsuming file input/output within the virtual memory
system. The implementation is much larger (about 11,000 lines of code) and
slower (it often incurs two disk accesses to handle a page fault and cannot
run the disk at full speed). The extra functionality is bought at a high
price.

This is not to say that a good implementation of this interface is
impossible, merely that it is hard. This system was designed and coded by
several highly competent and experienced people.  Part of the problem is
avoiding circularity: the file system would like to use the virtual memory,
but virtual memory depends on files. Quite general ways are known to solve
this problem [22], but they are tricky and easily lead to greater cost and
complexity in the normal case.

    And, in this upshot, purposes mistook
    Fall'n on th' inventors' heads. (V ii 387)

Another example illustrates how easily generality can lead to unexpected
complexity. The Tenex system [2] has the following innocent-looking
combination of features:

    It reports a reference to an unassigned virtual page by a trap to
    the user program.

    A system call is viewed as a machine instruction for an extended
    machine, and any reference it makes to an unassigned virtual page is
    thus similarly reported to the user program.

    Large arguments to system calls, including strings, are passed by reference.

    There is a system call CONNECT to obtain access to another directory;
    one of its arguments is a string containing the password for the
    directory. If the password is wrong, the call fails after a three
    second delay, to prevent guessing passwords at high speed.

    CONNECT is implemented by a loop of the form
	for i := 0 to Length(directoryPassword) do
	    if directoryPassword[i] ≠ passwordArgument[i] then
		Wait three seconds; return BadPassword
	    end if
	end loop;
    connect to directory; return Success

The following trick finds a password of length n in 64n tries on the
average, rather than 128n /2 (Tenex uses 7 bit characters in strings).
Arrange the passwordArgument so that its first character is the last
character of a page and the next page is unassigned, and try each possible
character as the first. If CONNECT reports BadPassword, the guess was
wrong; if the system reports a reference to an unassigned page, it was
correct. Now arrange the passwordArgument so that its second character is
the last character of the page, and proceed in the obvious way.

This obscure and amusing bug went unnoticed by the designers because the
interface provided by a Tenex system call is quite complex: it includes the
possibility of a reported reference to an unassigned page. Or looked at
another way, the interface provided by an ordinary memory reference
instruction in system code is quite complex: it includes the possibility
that an improper reference will be reported to the client without any
chance for the system code to get control first.

    An engineer is a man who can do for a dime
    what any fool can do for a dollar. (Anonymous)

At times, however, it's worth a lot of work to make a fast implementation
of a clean and powerful interface. If the interface is used widely enough,
the effort put into designing and tuning the implementation can pay off
many times over. But do this only for an interface whose importance is
already known from existing uses. And be sure that you know how to make it
fast.

For example, the BitBlt or RasterOp interface for manipulating raster
images [21, 37] was devised by Dan Ingalls after several years of
experimenting with the Alto's high-resolution interactive display. Its
implementation costs about as much microcode as the entire emulator for the
Alto's standard instruction set and required a lot of skill and experience
to construct. But the performance is nearly as good as the special-purpose
character-to-raster operations that preceded it, and its simplicity and
generality have made it much easier to build display applications.

The Dorado memory system [8] contains a cache and a separate high-bandwidth
path for fast input/output. It provides a cache read or write in every 64
ns cycle, together with 500 MBits/second of I/O bandwidth, virtual
addressing from both cache and I/O, and no special cases for the
microprogrammer to worry about. However, the implementation takes 850 MSI
chips and consumed several man-years of design time. This could only be
justified by extensive prior experience (30 years!) with this interface,
and the knowledge that memory access is usually the limiting factor in
performance. Even so, it seems in retrospect that the high I/O bandwidth is
not worth the cost; it is used mainly for displays, and a dual-ported frame
buffer would almost certainly be better.

Finally, lest this advice seem too easy to take,

• Get it right. Neither abstraction nor simplicity is a substitute for
getting it right. In fact, abstraction can be a source of severe
difficulties, as this cautionary tale shows. Word processing and office
information systems usually have provision for embedding named fields in
the documents they handle. For example, a form letter might have ‘address'
and ‘salutation' fields.  Usually a document is represented as a sequence
of characters, and a field is encoded by something like {name: contents}.
Among other operations, there is a procedure FindNamedField that finds the
field with a given name. One major commercial system for some time used a
FindNamedField procedure that ran in time O(n2 ), where n is the length of
the document. This remarkable result was achieved by first writing a
procedure FindIthField to find the ith field (which must take time O(n) if
there is no auxiliary data structure), and then implementing
FindNamedField(name) with the very natural program

    for i := 0 to numberofFields do
	FindIthField; if its name is name then exit
    end loop

Once the (unwisely chosen) abstraction FindIthField is available, only a
lively awareness of its cost will avoid this disaster. Of course, this is
not an argument against abstraction, but it is well to be aware of its
dangers.

2.2 Corollaries

The rule about simplicity and generalization has many interesting corollaries.

    Costly thy habit as thy purse can buy,
    But not express'd in fancy; rich, not gaudy.

• Make it fast, rather than general or powerful. If it's fast, the client
can program the function it wants, and another client can program some
other function. It is much better to have basic operations executed quickly
than more powerful ones that are slower (of course, a fast, powerful
operation is best, if you know how to get it). The trouble with slow,
powerful operations is that the client who doesn't want the power pays more
for the basic function. Usually it turns out that the powerful operation is
not the right one.

    Had I but time (as this fell sergeant, death,
    Is strict in his arrest) O, I could tell you —
    But let it be. (V ii 339)

For example, many studies (such as [23, 51, 52]) have shown that programs
spend most of their time doing very simple things: loads, stores, tests for
equality, adding one. Machines like the 801 [41] or the RISC [39] with
instructions that do these simple operations quickly can run programs
faster (for the same amount of hardware) than machines like the VAX with
more general and powerful instructions that take longer in the simple
cases. It is easy to lose a factor of two in the running time of a program,
with the same amount of hardware in the implementation. Machines with still
more grandiose ideas about what the client needs do even worse [18].

To find the places where time is being spent in a large system, it is
necessary to have measurement tools that will pinpoint the time-consuming
code. Few systems are well enough understood to be properly tuned without
such tools; it is normal for 80% of the time to be spent in 20% of the
code, but a priori analysis or intuition usually can't find the 20% with
any certainty. The performance tuning of Interlisp-D sped it up by a factor
of 10 using one set of effective tools [7].

• Don't hide power. This slogan is closely related to the last one. When a
low level of abstraction allows something to be done quickly, higher levels
should not bury this power inside something more general. The purpose of
abstractions is to conceal undesirable properties; desirable ones should
not be hidden. Sometimes, of course, an abstraction is multiplexing a
resource, and this necessarily has some cost. But it should be possible to
deliver all or nearly all of it to a single client with only slight loss of
performance.

For example, the Alto disk hardware [53] can transfer a full cylinder at
disk speed. The basic file system [29] can transfer successive file pages
to client memory at full disk speed, with time for the client to do some
computing on each sector; thus with a few sectors of buffering the entire
disk can be scanned at disk speed. This facility has been used to write a
variety of applications, ranging from a scavenger that reconstructs a
broken file system, to programs that search files for substrings that match
a pattern. The stream level of the file system can read or write n bytes to
or from client memory; any portions of the n bytes that occupy full disk
sectors are transferred at full disk speed. Loaders, compilers, editors and
many other programs depend for their performance on this ability to read
large files quickly. At this level the client gives up the facility to see
the pages as they arrive; this is the only price paid for the higher level
of abstraction.

• Use procedure arguments to provide flexibility in an interface. They can
be restricted or encoded in various ways if necessary for protection or
portability. This technique can greatly simplify an interface, eliminating
a jumble of parameters that amount to a small programming language. A
simple example is an enumeration procedure that returns all the elements of
a set satisfying some property. The cleanest interface allows the client to
pass a filter procedure that tests for the property, rather than defining a
special language of patterns or whatever.

But this theme has many variations. A more interesting example is the Spy
system monitoring facility in the 940 system at Berkeley [10], which allows
an untrusted user program to plant patches in the code of the supervisor. A
patch is coded in machine language, but the operation that installs it
checks that it does no wild branches, contains no loops, is not too long,
and stores only into a designated region of memory dedicated to collecting
statistics. Using the Spy, the student of the system can fine-tune his
measurements without any fear of breaking the system, or even perturbing
its operation much.

Another unusual example that illustrates the power of this method is the
FRETURN mechanism in the Cal time-sharing system for the CDC 6400 [30].
From any supervisor call C it is possible to make another one CF that
executes exactly like C in the normal case, but sends control to a
designated failure handler if C gives an error return. The CF operation can
do more (for example, it can extend files on a fast, limited-capacity
storage device to larger files on a slower device), but it runs as fast as
C in the (hopefully) normal case.

It may be better to have a specialized language, however, if it is more
amenable to static analysis for optimization. This is a major criterion in
the design of database query languages, for example.

• Leave it to the client. As long as it is cheap to pass control back and
forth, an interface can combine simplicity, flexibility and high
performance by solving only one problem and leaving the rest to the client.
For example, many parsers confine themselves to doing context free
recognition and call client-supplied “semantic routines” to record the
results of the parse. This has obvious advantages over always building a
parse tree that the client must traverse to find out what happened.

The success of monitors [20, 25] as a synchronization device is partly due
to the fact that the locking and signaling mechanisms do very little,
leaving all the real work to the client programs in the monitor procedures.
This simplifies the monitor implementation and keeps it fast; if the client
needs buffer allocation, resource accounting or other frills, it provides
these functions itself or calls other library facilities, and pays for what
it needs. The fact that monitors give no control over the scheduling of
processes waiting on monitor locks or condition variables, often cited as a
drawback, is actually an advantage, since it leaves the client free to
provide the scheduling it needs (using a separate condition variable for
each class of process), without having to pay for or fight with some
built-in mechanism that is unlikely to do the right thing.

The Unix system [44] encourages the building of small programs that take
one or more character streams as input, produce one or more streams as
output, and do one operation. When this style is imitated properly, each
program has a simple interface and does one thing well, leaving the client
to combine a set of such programs with its own code and achieve precisely
the effect desired.

The end-to-end slogan discussed in section 3 is another corollary of
keeping it simple.

2.3 Continuity

There is a constant tension between the desire to improve a design and the
need for stability or continuity.

• Keep basic interfaces stable. Since an interface embodies assumptions
that are shared by more than one part of a system, and sometimes by a great
many parts, it is very desirable not to change the interface. When the
system is programmed in a language without type-checking, it is nearly out
of the question to change any public interface because there is no way of
tracking down its clients and checking for elementary incompatibilities,
such as disagreements on the number of arguments or confusion between
pointers and integers. With a language like Mesa [15] that has complete
type-checking and language support for interfaces, it is much easier to
change an interface without causing the system to collapse. But even if
type-checking can usually detect that an assumption no longer holds, a
programmer must still correct the assumption. When a system grows to more
than 250K lines of code the amount of change becomes intolerable; even when
there is no doubt about what has to be done, it takes too long to do it.
There is no choice but to break the system into smaller pieces related only
by interfaces that are stable for years.  Traditionally only the interface
defined by a programming language or operating system kernel is this
stable.

• Keep a place to stand if you do have to change interfaces. Here are two
rather different examples to illustrate this idea. One is the compatibility
package, which implements an old interface on top of a new system. This
allows programs that depend on the old interface to continue working. Many
new operating systems (including Tenex [2] and Cal [50]) have kept old
software usable by simulating the supervisor calls of an old system
(TOPS-10 and Scope, respectively). Usually these simulators need only a
small amount of effort compared to the cost of reimplementing the old
software, and it is not hard to get acceptable performance. At a different
level, the IBM 360/370 systems provided emulation of the instruction sets
of older machines like the 1401 and 7090. Taken a little further, this
leads to virtual machines, which simulate (several copies of) a machine on
the machine itself [9].

A rather different example is the world-swap debugger, which works by
writing the real memory of the target system (the one being debugged) onto
a secondary storage device and reading in the debugging system in its
place. The debugger then provides its user with complete access to the
target world, mapping each target memory address to the proper place on
secondary storage.  With care it is possible to swap the target back in and
continue execution. This is somewhat clumsy, but it allows very low levels
of a system to be debugged conveniently, since the debugger does not depend
on the correct functioning of anything in the target except the very simple
world-swap mechanism. It is especially useful during bootstrapping. There
are many variations. For instance, the debugger can run on a different
machine, with a small ‘teledebugging' nub in the target world that can
interpret ReadWord, WriteWord, Stop and Go commands arriving from the
debugger over a network. Or if the target is a process in a timesharing
system, the debugger can run in a different process.

2.4 Making implementations work

    Perfection must be reached by degrees; she requires the slow hand of time.
    (Voltaire)

• Plan to throw one away; you will anyhow [6]. If there is anything new
about the function of a system, the first implementation will have to be
redone completely to achieve a satisfactory (that is, acceptably small,
fast, and maintainable) result. It costs a lot less if you plan to have a
prototype. Unfortunately, sometimes two prototypes are needed, especially
if there is a lot of innovation. If you are lucky you can copy a lot from a
previous system; thus Tenex was based on the SDS 940 [2]. This can even
work even if the previous system was too grandiose; Unix took many ideas
from Multics [44].

Even when an implementation is successful, it pays to revisit old decisions
as the system evolves; in particular, optimizations for particular
properties of the load or the environment (memory size, for example) often
come to be far from optimal.

    Give thy thoughts no tongue,
    Nor any unproportion'd thought his act.

• Keep secrets of the implementation. Secrets are assumptions about an
implementation that client programs are not allowed to make (paraphrased
from [5]). In other words, they are things that can change; the interface
defines the things that cannot change (without simultaneous changes to both
implementation and client). Obviously, it is easier to program and modify a
system if its parts make fewer assumptions about each other. On the other
hand, the system may not be easier to design—it's hard to design a good
interface. And there is a tension with the desire not to hide power.

    An efficient program is an exercise in logical brinkmanship. (E. Dijkstra)

There is another danger in keeping secrets. One way to improve performance
is to increase the number of assumptions that one part of a system makes
about another; the additional assumptions often allow less work to be done,
sometimes a lot less. For instance, if a set of size n is known to be
sorted, a membership test takes time log n rather than n. This technique is
very important in the design of algorithms and the tuning of small modules.
In a large system the ability to improve each part separately is usually
more important. Striking the right balance remains an art.

    O throw away the worser part of it,
    And live the purer with the other half. (III iv 157)

• Divide and conquer. This is a well known method for solving a hard
problem: reduce it to several easier ones. The resulting program is usually
recursive. When resources are limited the method takes a slightly different
form: bite off as much as will fit, leaving the rest for another iteration.

A good example is in the Alto's Scavenger program, which scans the disk and
rebuilds the index and directory structures of the file system from the
file identifier and page number recorded on each disk sector [29]. A recent
rewrite of this program has a phase in which it builds a data structure in
main storage, with one entry for each contiguous run of disk pages that is
also a contiguous set of pages in a file. Normally files are allocated more
or less contiguously and this structure is not too large. If the disk is
badly fragmented, however, the structure will not fit in storage. When this
happens, the Scavenger discards the information for half the files and
continues with the other half. After the index for these files is rebuilt,
the process is repeated for the other files. If necessary the work is
further subdivided; the method fails only if a single file's index won't
fit.

Another interesting example arises in the Dover raster printer [26, 53],
which scan-converts lists of characters and rectangles into a large m × n
array of bits, in which ones correspond to spots of ink on the paper and
zeros to spots without ink. In this printer m=3300 and n=4200, so the array
contains fourteen million bits and is too large to store in memory. The
printer consumes bits faster than the available disks can deliver them, so
the array cannot be stored on disk. Instead, the entire array is divided
into 16 × 4200 bit slices called bands, and the printer electronics
contains two one-band buffers. The characters and rectangles are sorted
into buckets, one for each band; a bucket receives the objects that start
in the corresponding band. Scan conversion proceeds by filling one band
buffer from its bucket, and then playing it out to the printer and zeroing
it while filling the other buffer from the next bucket. Objects that spill
over the edge of one band are added to the next bucket; this is the trick
that allows the problem to be subdivided.  Sometimes it is convenient to
artificially limit the resource, by quantizing it in fixed-size units; this
simplifies bookkeeping and prevents one kind of fragmentation. The
classical example is the use of fixed-size pages for virtual memory, rather
than variable-size segments. In spite of the apparent advantages of keeping
logically related information together, and transferring it between main
storage and backing storage as a unit, paging systems have worked out
better. The reasons for this are complex and have not been systematically
studied.

    And makes us rather bear those ills we have
    Than fly to others that we know not of. (III i 81)

• Use a good idea again instead of generalizing it. A specialized
implementation of the idea may be much more effective than a general one.
The discussion of caching below gives several examples of applying this
general principle. Another interesting example is the notion of replicating
data for reliability. A small amount of data can easily be replicated
locally by writing it on two or more disk drives [28]. When the amount of
data is large or the data must be recorded on separate machines, it is not
easy to ensure that the copies are always the same. Gifford [16] shows how
to solve this problem by building replicated data on top of a transactional
storage system, which allows an arbitrarily large update to be done as an
atomic operation (see section 4). The transactional storage itself depends
on the simple local replication scheme to store its log reliably. There is
no circularity here, since only the idea is used twice, not the code. A
third way to use replication in this context is to store the commit record
on several machines [27].

The user interface for the Star office system [47] has a small set of
operations (type text, move, copy, delete, show properties) that apply to
nearly all the objects in the system: text, graphics, file folders and file
drawers, record files, printers, in and out baskets, etc. The exact meaning
of an operation varies with the class of object, within the limits of what
the user might find natural.  For instance, copying a document to an out
basket causes it to be sent as a message; moving the endpoint of a line
causes the line to follow like a rubber band. Certainly the implementations
are quite different in many cases. But the generic operations do not simply
make the system easier to use; they represent a view of what operations are
possible and how the implementation of each class of object should be
organized.

2.5 Handling all the cases

    Diseases desperate grown
    By desperate appliance are reliev'd
    or not at all. (III vii 9)

    Therefore this project
    Should have a back or second, that might hold,
    If this should blast in proof. (IV iii 151)

• Handle normal and worst cases separately as a rule, because the
requirements for the two are quite different:

    The normal case must be fast.
    The worst case must make some progress.

In most systems it is all right to schedule unfairly and give no service to
some of the processes, or even to deadlock the entire system, as long as
this event is detected automatically and doesn't happen too often. The
usual recovery is by crashing some processes, or even the entire system.
At first this sounds terrible, but one crash a week is usually a cheap
price to pay for 20% better performance. Of course the system must have
decent error recovery (an application of the end-toend principle; see
section 4), but that is required in any case, since there are so many other
possible causes of a crash.

Caches and hints (section 3) are examples of special treatment for the
normal case, but there are many others. The Interlisp-D and Cedar
programming systems use a reference-counting garbage collector [11] that
has an important optimization of this kind. Pointers in the local frames or
activation records of procedures are not counted; instead, the frames are
scanned whenever garbage is collected. This saves a lot of
reference-counting, since most pointer assignments are to local variables.
There are not very many frames, so the time to scan them is small and the
collector is nearly real-time. Cedar goes farther and does not keep track
of which local variables contain pointers; instead, it assumes that they
all do. This means that an integer that happens to contain the address of
an object which is no longer referenced will keep that object from being
freed. Measurements show that less than 1% of the storage is incorrectly
retained [45].

Reference-counting makes it easy to have an incremental collector, so that
computation need not stop during collection. However, it cannot reclaim
circular structures that are no longer reachable. Cedar therefore has a
conventional trace-and-sweep collector as well. This is not suitable for
real time applications, since it stops the entire system for many seconds,
but in interactive applications it can be used during coffee breaks to
reclaim accumulated circular structures.

Another problem with reference-counting is that the count may overflow the
space provided for it. This happens very seldom, because only a few objects
have more than two or three references.  It is simple to make the maximum
value sticky. Unfortunately, in some applications the root of a large
structure is referenced from many places; if the root becomes sticky, a lot
of storage will unexpectedly become permanent. An attractive solution is to
have an ‘overflow count' table, which is a hash table keyed on the address
of an object. When the count reaches its limit it is reduced by half, the
overflow count is increased by one, and an overflow flag is set in the
object.  When the count reaches zero, the process is reversed if the
overflow flag is set. Thus even with as few as four bits there is room to
count up to seven, and the overflow table is touched only in the rare case
that the count swings by more than four.

There are many cases when resources are dynamically allocated and freed
(for example, real memory in a paging system), and sometimes additional
resources are needed temporarily to free an item (some table might have to
be swapped in to find out where to write out a page). Normally there is a
cushion (clean pages that can be freed with no work), but in the worst case
the cushion may disappear (all pages are dirty). The trick here is to keep
a little something in reserve under a mattress, bringing it out only in a
crisis. It is necessary to bound the resources needed to free one item;
this determines the size of the reserve under the mattress, which must be
regarded as a fixed cost of the resource multiplexing. When the crisis
arrives, only one item should be freed at a time, so that the entire
reserve is devoted to that job; this may slow things down a lot but it
ensures that progress will be made.

Sometimes radically different strategies are appropriate in the normal and
worst cases. The Bravo editor [24] uses a ‘piece table' to represent the
document being edited. This is an array of pieces, pointers to strings of
characters stored in a file; each piece contains the file address of the
first character in the string and its length. The strings are never
modified during normal editing.  Instead, when some characters are deleted,
for example, the piece containing the deleted characters is split into two
pieces, one pointing to the first undeleted string and the other to the
second. Characters inserted from the keyboard are appended to the file, and
the piece containing the insertion point is split into three pieces: one
for the preceding characters, a second for the inserted characters, and a
third for the following characters. After hours of editing there are
hundreds of pieces and things start to bog down. It is then time for a
cleanup, which writes a new file containing all the characters of the
document in order. Now the piece table can be replaced by a single piece
pointing to the new file, and editing can continue. Cleanup is a
specialized kind of garbage collection. It can be done in background so
that the user doesn't have to stop editing (though Bravo doesn't do this).

3. Speed

This section describes hints for making systems faster, forgoing any
further discussion of why this is important. Bentley's excellent book [55]
says more about some of these ideas and gives many others.

    Neither a borrower, nor a lender be;
    For loan oft loses both itself and friend,
    And borrowing dulleth edge of husbandry.

• Split resources in a fixed way if in doubt, rather than sharing them. It
is usually faster to allocate dedicated resources, it is often faster to
access them, and the behavior of the allocator is more predictable. The
obvious disadvantage is that more total resources are needed, ignoring
multiplexing overheads, than if all come from a common pool. In many cases,
however, the cost of the extra resources is small, or the overhead is
larger than the fragmentation, or both.  For example, it is always faster
to access information in the registers of a processor than to get it from
memory, even if the machine has a high-performance cache. Registers have
gotten a bad name because it can be tricky to allocate them intelligently,
and because saving and restoring them across procedure calls may negate
their speed advantages. But when programs are written in the approved
modern style with lots of small procedures, 16 registers are nearly always
enough for all the local variables and temporaries, so that allocation is
not a problem. With n sets of registers arranged in a stack, saving is
needed only when there are n successive calls without a return [14, 39].

Input/output channels, floating-point coprocessors, and similar specialized
computing devices are other applications of this principle. When extra
hardware is expensive these services are provided by multiplexing a single
processor, but when it is cheap, static allocation of computing power for
various purposes is worthwhile.

The Interlisp virtual memory system mentioned earlier [7] needs to keep
track of the disk address corresponding to each virtual address. This
information could itself be held in the virtual memory (as it is in several
systems, including Pilot [42]), but the need to avoid circularity makes
this rather complicated. Instead, real memory is dedicated to this purpose.
Unless the disk is ridiculously fragmented the space thus consumed is less
than the space for the code to prevent circularity.

• Use static analysis if you can; this is a generalization of the last
slogan. Static analysis discovers properties of the program that can
usually be used to improve its performance. The hooker is “if you can”;
when a good static analysis is not possible, don't delude yourself with a
bad one, but fall back on a dynamic scheme.

The remarks about registers above depend on the fact that the compiler can
easily decide how to allocate them, simply by putting the local variables
and temporaries there. Most machines lack multiple sets of registers or
lack a way of stacking them efficiently. Good allocation is then much more
difficult, requiring an elaborate inter-procedural analysis that may not
succeed, and in any case must be redone each time the program changes. So a
little bit of dynamic analysis (stacking the registers) goes a long way. Of
course the static analysis can still pay off in a large procedure if the
compiler is clever.

A program can read data much faster when it reads the data sequentially.
This makes it easy to predict what data will be needed next and read it
ahead into a buffer. Often the data can be allocated sequentially on a
disk, which allows it to be transferred at least an order of magnitude
faster. These performance gains depend on the fact that the programmer has
arranged the data so that it is accessed according to some predictable
pattern, that is, so that static analysis is possible.  Many attempts have
been made to analyze programs after the fact and optimize the disk
transfers, but as far as I know this has never worked. The dynamic analysis
done by demand paging is always at least as good.

Some kinds of static analysis exploit the fact that some invariant is
maintained. A system that depends on such facts may be less robust in the
face of hardware failures or bugs in software that falsify the invariant.

• Dynamic translation from a convenient (compact, easily modified or easily
displayed) representation to one that can be quickly interpreted is an
important variation on the old idea of compiling. Translating a bit at a
time is the idea behind separate compilation, which goes back at least to
Fortran 2. Incremental compilers do it automatically when a statement,
procedure or whatever is changed. Mitchell investigated smooth motion on a
continuum between the convenient and the fast representation [34]. A
simpler version of his scheme is to always do the translation on demand and
cache the result; then only one interpreter is required, and no decisions
are needed except for cache replacement.

For example, an experimental Smalltalk implementation [12] uses the
bytecodes produced by the standard Smalltalk compiler as the convenient (in
this case, compact) representation, and translates a single procedure from
byte codes into machine language when it is invoked. It keeps a cache with
room for a few thousand instructions of translated code. For the scheme to
pay off, the cache must be large enough that on the average a procedure is
executed at least n times, where n is the ratio of translation time to
execution time for the untranslated code.

The C-machine stack cache [14] provides a rather different example. In this
device instructions are fetched into an instruction cache; as they are
loaded, any operand address that is relative to the local frame pointer FP
is converted into an absolute address, using the current value of FP (which
remains constant during execution of the procedure). In addition, if the
resulting address is in the range of addresses currently in the stack data
cache, the operand is changed to register mode; later execution of the
instruction will then access the register directly in the data cache.  The
FP value is concatenated with the instruction address to form the key of
the translated instruction in the cache, so that multiple activations of
the same procedure will still work.

    If thou didst ever hold me in thy heart. (V ii 349)

• Cache answers to expensive computations, rather than doing them over. By
storing the triple [f, x, f(x)] in an associative store with f and x as
keys, we can retrieve f(x) with a lookup. This is faster if f(x) is needed
again before it gets replaced in the cache, which presumably has limited
capacity. How much faster depends on how expensive it is to compute f(x). A
serious problem is that when f is not functional (can give different
results with the same arguments), we need a way to invalidate or update a
cache entry if the value of f(x) changes. Updating depends on an equation
of the form f(x + ∆) = g(x, ∆, f(x)) in which g is much cheaper to compute
than f. For example, x might be an array of 1000 numbers, f the sum of the
array elements, and ∆ a new value for one of them, that is, a pair [i, v].
Then g(x, [i, v], sum) is sum - xi + v.

A cache that is too small to hold all the ‘active' values will thrash, and
if recomputing f is expensive performance will suffer badly. Thus it is
wise to choose the cache size adaptively, making it bigger when the hit
rate decreases and smaller when many entries go unused for a long time.

The classic example of caching is hardware that speeds up access to main
storage; its entries are triples [Fetch, address, contents of address]. The
Fetch operation is certainly not functional: Fetch(x) gives a different
answer after Store(x) has been done. Hence the cache must be updated or
invalidated after a store. Virtual memory systems do exactly the same
thing; main storage plays the role of the cache, disk plays the role of
main storage, and the unit of transfer is the page, segment or whatever.

But nearly every non-trivial system has more specialized applications of
caching. This is especially true for interactive or real-time systems, in
which the basic problem is to incrementally update a complex state in
response to frequent small changes. Doing this in an ad - hoc way is
extremely error-prone. The best organizing principle is to recompute the
entire state after each change but cache all the expensive results of this
computation. A change must invalidate at least the cache entries that it
renders invalid; if these are too hard to identify precisely, it may
invalidate more entries at the price of more computing to reestablish them.
The secret of success is to organize the cache so that small changes
invalidate only a few entries.

For example, the Bravo editor [24] has a function DisplayLine(document,
firstChar) that returns the bitmap for the line of text in the displayed
document that has document[firstChar] as its first character. It also
returns lastChar and lastCharUsed, the numbers of the last character
displayed on the line and the last character examined in computing the
bitmap (these are usually not the same, since it is necessary to look past
the end of the line in order to choose the line break). This function
computes line breaks, does justification, uses font tables to map
characters into their raster pictures, etc. There is a cache with an entry
for each line currently displayed on the screen, and sometimes a few lines
just above or below. An edit that changes characters i through j
invalidates any cache entry for which [firstChar .. lastCharUsed]
intersects [i .. j]. The display is recomputed by

    loop
	(bitMap, lastChar, ) := DisplayLine(document, firstChar); Paint(bitMap);
	firstChar := lastChar + 1
    end loop

The call of DisplayLine is short-circuited by using the cache entry for
[document, firstChar] if it exists. At the end any cache entry that has not
been used is discarded; these entries are not invalid, but they are no
longer interesting because the line breaks have changed so that a line no
longer begins at these points.

The same idea can be applied in a very different setting. Bravo allows a
document to be structured into paragraphs, each with specified left and
right margins, inter-line leading, etc. In ordinary page layout all the
information about the paragraph that is needed to do the layout can be
represented very compactly:

    the number of lines;
    the height of each line (normally all lines are the same height);
    any keep properties;
    the pre and post leading.

In the usual case this can be encoded in three or four bytes. A 30 page
chapter has perhaps 300 paragraphs, so about 1k bytes are required for all
this data; this is less information than is required to specify the
characters on a page. Since the layout computation is comparable to the
line layout computation for a page, it should be possible to do the
pagination for this chapter in less time than is required to render one
page. Layout can be done independently for each chapter.

What makes this idea work is a cache of [paragraph,
ParagraphShape(paragraph)] entries. If the paragraph is edited, the cache
entry is invalid and must be recomputed. This can be done at the time of
the edit (reasonable if the paragraph is on the screen, as is usually the
case, but not so good for a global substitute), in background, or only when
repagination is requested.

    For the apparel oft proclaims the man.

• Use hints to speed up normal execution. A hint, like a cache entry, is
the saved result of some computation. It is different in two ways: it may
be wrong, and it is not necessarily reached by an associative lookup.
Because a hint may be wrong, there must be a way to check its correctness
before taking any unrecoverable action. It is checked against the ‘truth',
information that must be correct but can be optimized for this purpose
rather than for efficient execution. Like a cache entry, the purpose of a
hint is to make the system run faster. Usually this means that it must be
correct nearly all the time.

For example, in the Alto [29] and Pilot [42] operating systems each file
has a unique identifier, and each disk page has a ‘label' field whose
contents can be checked before reading or writing the data without slowing
down the data transfer. The label contains the identifier of the file that
contains the page and the number of that page in the file. Page zero of
each file is called the ‘leader page' and contains, among other things, the
directory in which the file resides and its string name in that directory.
This is the truth on which the file systems are based, and they take great
pains to keep it correct.

With only this information, however, there is no way to find the identifier
of a file from its name in a directory, or to find the disk address of page
i, except to search the entire disk, a method that works but is
unacceptably slow. Each system therefore maintains hints to speed up these
operations. Both systems represent directory by a file that contains
triples [string name, file identifier, address of first page]. Each file
has a data structure that maps a page number into the disk address of the
page. The Alto uses a link in each label that points to the next label;
this makes it fast to get from page n to page n + 1. Pilot uses a B-tree
that implements the map directly, taking advantage of the common case in
which consecutive file pages occupy consecutive disk pages. Information
obtained from any of these hints is checked when it is used, by checking
the label or reading the file name from the leader page. If it proves to be
wrong, all of it can be reconstructed by scanning the disk. Similarly, the
bit table that keeps track of free disk pages is a hint; the truth is
represented by a special value in the label of a free page, which is
checked when the page is allocated and before the label is overwritten with
a file identifier and page number.

Another example of hints is the store and forward routing first used in the
Arpanet [32]. Each node in the network keeps a table that gives the best
route to each other node. This table is updated by periodic broadcasts in
which each node announces to all the other nodes its opinion about the
quality of its links to its neighbors. Because these broadcast messages are
not synchronized and are not guaranteed to be delivered, the nodes may not
have a consistent view at any instant. The truth in this case is that each
node knows its own identity and hence knows when it receives a packet
destined for itself. For the rest, the routing does the best it can; when
things aren't changing too fast it is nearly optimal.

A more curious example is the Ethernet [33], in which lack of a carrier
signal on the cable is used as a hint that a packet can be sent. If two
senders take the hint simultaneously, there is a collision that both can
detect; both stop, delay for a randomly chosen interval, and then try
again. If n successive collisions occur, this is taken as a hint that the
number of senders is 2n , and each sender sets the mean of its random delay
interval to 2n times its initial value. This ‘exponential backoff' ensures
that the net does not become overloaded.  A very different application of
hints speeds up execution of Smalltalk programs [12]. In Smalltalk the code
executed when a procedure is called is determined dynamically by the type
of the first argument. Thus Print(x, format) invokes the Print procedure
that is part of the type of x.  Since Smalltalk has no declarations, the
type of x is not known statically. Instead, each object has a pointer to a
table of pairs [procedure name, address of code], and when this call is
executed, Print is looked up x's table (I have normalized the unusual
Smalltalk terminology and syntax, and oversimplified a bit). This is
expensive. It turns out that usually the type of x is the same as it was
last time. So the code for the call Print(x, format) can be arranged like
this:

    push format; push x;
    push lastType; call lastProc

and each Print procedure begins with

    lastT := Pop(); x := Pop(); t := type of x;
    if t ≠ lastT then LookupAndCall(x, “Print”) else the body of the procedure end if.

Here lastType and lastProc are immediate values stored in the code. The
idea is that LookupAndCall should store the type of x and the code address
it finds back into the lastType and lastProc fields. If the type is the
same next time, the procedure is called directly. Measurements show that
this cache hits about 96% of the time. In a machine with an instruction
fetch unit, this scheme has the nice property that the transfer to lastProc
can proceed at full speed; thus when the hint is correct the call is as
fast as an ordinary subroutine call. The check of t ≠ lastT can be arranged
so that it normally does not branch.

The same idea in a different guise is used in the S-1 [48], which has an
extra bit for each instruction in its instruction cache. It clears the bit
when the instruction is loaded, sets it when the instruction causes a
branch to be taken, and uses it to choose the path that the instruction
fetch unit follows. If the prediction turns out to be wrong, it changes the
bit and follows the other path.

• When in doubt, use brute force. Especially as the cost of hardware
declines, a straightforward, easily analyzed solution that requires a lot
of special-purpose computing cycles is better than a complex, poorly
characterized one that may work well if certain assumptions are satisfied.
For example, Ken Thompson's chess machine Belle relies mainly on
special-purpose hardware to generate moves and evaluate positions, rather
than on sophisticated chess strategies. Belle has won the world computer
chess championships several times. Another instructive example is the
success of personal computers over time-sharing systems; the latter include
much more cleverness and have many fewer wasted cycles, but the former are
increasingly recognized as the most cost-effective way to do interactive
computing.

Even an asymptotically faster algorithm is not necessarily better. There is
an algorithm that multiplies two n × n matrices faster than O(n2.5), but
the constant factor is prohibitive. On a more mundane note, the 7040 Watfor
compiler uses linear search to look up symbols; student programs have so
few symbols that the setup time for a better algorithm can't be recovered.

• Compute in background when possible. In an interactive or real-time
system, it is good to do as little work as possible before responding to a
request. The reason is twofold: first, a rapid response is better for the
users, and second, the load usually varies a great deal, so there is likely
to be idle processor time later in which to do background work. Many kinds
of work can be deferred to background. The Interlisp and Cedar garbage
collectors [7, 11] do nearly all their work this way. Many paging systems
write out dirty pages and prepare candidates for replacement in background.
Electronic mail can be delivered and retrieved by background processes,
since delivery within an hour or two is usually acceptable. Many banking
systems consolidate the data on accounts at night and have it ready the
next morning. These four examples have successively less need for
synchronization between foreground and background tasks. As the amount of
synchronization increases more care is needed to avoid subtle errors; an
extreme example is the on-the-fly garbage collection algorithm given in
[13]. But in most cases a simple producer-consumer relationship between two
otherwise independent processes is possible.

• Use batch processing if possible. Doing things incrementally almost
always costs more, even aside from the fact that disks and tapes work much
better when accessed sequentially. Also, batch processing permits much
simpler error recovery. The Bank of America has an interactive system that
allows tellers to record deposits and check withdrawals. It is loaded with
current account balances in the morning and does its best to maintain them
during the day. But early the next morning the on-line data is discarded
and replaced with the results of night's batch run. This design makes it
much easier to meet the bank's requirements for trustworthy long-term data,
and there is no significant loss in function.

    Be wary then; best safety lies in fear. (I iii 43)

• Safety first. In allocating resources, strive to avoid disaster rather
than to attain an optimum.  Many years of experience with virtual memory,
networks, disk allocation, database layout, and other resource allocation
problems has made it clear that a general-purpose system cannot optimize
the use of resources. On the other hand, it is easy enough to overload a
system and drastically degrade the service. A system cannot be expected to
function well if the demand for any resource exceeds two-thirds of the
capacity, unless the load can be characterized extremely well. Fortunately
hardware is cheap and getting cheaper; we can afford to provide excess
capacity. Memory is especially cheap, which is especially fortunate since
to some extent plenty of memory can allow other resources like processor
cycles or communication bandwidth to be utilized more fully.

The sad truth about optimization was brought home by the first paging
systems. In those days memory was very expensive, and people had visions of
squeezing the most out of every byte by clever optimization of the
swapping: putting related procedures on the same page, predicting the next
pages to be referenced from previous references, running jobs together that
share data or code, etc. No one ever learned how to do this. Instead,
memory got cheaper, and systems spent it to provide enough cushion for
simple demand paging to work. We learned that the only important thing is
to avoid thrashing, or too much demand for the available memory. A system
that thrashes spends all its time waiting for the disk.

The only systems in which cleverness has worked are those with very
well-known loads. For instance, the 360/50 APL system [4] has the same size
workspace for each user and common system code for all of them. It makes
all the system code resident, allocates a contiguous piece of disk for each
user, and overlaps a swap-out and a swap-in with each unit of computation.
This works fine.

    The nicest thing about the Alto is that it doesn't run faster at
    night. (J. Morris)

A similar lesson was learned about processor time. With interactive use the
response time to a demand for computing is important, since a person is
waiting for it. Many attempts were made to tune the processor scheduling as
a function of priority of the computation, working set size, memory
loading, past history, likelihood of an I/O request, etc.. These efforts
failed. Only the crudest parameters produce intelligible effects:
interactive vs. non-interactive computation or high, foreground and
background priority levels. The most successful schemes give a fixed share
of the cycles to each job and don't allocate more than 100%; unused cycles
are wasted or, with luck, consumed by a background job. The natural
extension of this strategy is the personal computer, in which each user has
at least one processor to himself.

    Give every man thy ear, but few thy voice;
    Take each man's censure, but reserve thy judgment.

• Shed load to control demand, rather than allowing the system to become
overloaded. This is a corollary of the previous rule. There are many ways
to shed load. An interactive system can refuse new users, or even deny
service to existing ones. A memory manager can limit the jobs being served
so that all their working sets fit in the available memory. A network can
discard packets. If it comes to the worst, the system can crash and start
over more prudently.  Bob Morris suggested that a shared interactive system
should have a large red button on each terminal. The user pushes the button
if he is dissatisfied with the service, and the system must either improve
the service or throw the user off; it makes an equitable choice over a
sufficiently long period. The idea is to keep people from wasting their
time in front of terminals that are not delivering a useful amount of
service.

The original specification for the Arpanet [32] was that a packet accepted
by the net is guaranteed to be delivered unless the recipient machine is
down or a network node fails while it is holding the packet. This turned
out to be a bad idea. This rule makes it very hard to avoid deadlock in the
worst case, and attempts to obey it lead to many complications and
inefficiencies even in the normal case. Furthermore, the client does not
benefit, since it still has to deal with packets lost by host or network
failure (see section 4 on end-to-end). Eventually the rule was abandoned.
The Pup internet [3], faced with a much more variable set of transport
facilities, has always ruthlessly discarded packets at the first sign of
congestion.

4. Fault-toleranch####Wkh unavoidable price of reliability is simplicity. (C. Hoare)

Making a system reliable is not really hard, if you know how to go about
it. But retrofitting reliability to an existing design is very difficult.

    This above all: to thine own self be true,
    And it must follow, as the night the day,
    Thou canst not then be false to any man.

• End-to-end. Error recovery at the application level is absolutely
necessary for a reliable system, and any other error detection or recovery
is not logically necessary but is strictly for performance. This
observation was first made by Saltzer [46] and is very widely applicable.

For example, consider the operation of transferring a file from a file
system on a disk attached to machine A, to another file system on another
disk attached to machine B. To be confident that the right bits are really
on B's disk, you must read the file from B's disk, compute a checksum of
reasonable length (say 64 bits), and find that it is equal to a checksum of
the bits on A's disk.  Checking the transfer from A's disk to A's memory,
from A over the network to B, or from B's memory to B's disk is not
sufficient, since there might be trouble at some other point, the bits
might be clobbered while sitting in memory, or whatever. These other checks
are not necessary either, since if the end-to-end check fails the entire
transfer can be repeated. Of course this is a lot of work, and if errors
are frequent, intermediate checks can reduce the amount of work that must
be repeated. But this is strictly a question of performance, irrelevant to
the reliability of the file transfer. Indeed, in the ring based system at
Cambridge it is customary to copy an entire disk pack of 58 MBytes with
only an end-to-end check; errors are so infrequent that the 20 minutes of
work very seldom needs to be repeated [36].

Many uses of hints are applications of this idea. In the Alto file system
described earlier, for example, the check of the label on a disk sector
before writing the sector ensures that the disk address for the write is
correct. Any precautions taken to make it more likely that the address is
correct may be important, or even critical, for performance, but they do
not affect the reliability of the file system.

The Pup internet [4] adopts the end-to-end strategy at several levels. The
main service offered by the network is transport of a data packet from a
source to a destination. The packet may traverse a number of networks with
widely varying error rates and other properties. Internet nodes that store
and forward packets may run short of space and be forced to discard
packets. Only rough estimates of the best route for a packet are available,
and these may be wildly wrong when parts of the network fail or resume
operation. In the face of these uncertainties, the Pup internet provides
good service with a simple implementation by attempting only “best efforts”
delivery.  A packet may be lost with no notice to the sender, and it may be
corrupted in transit. Clients must provide their own error control to deal
with these problems, and indeed higher-level Pup protocols do provide more
complex services such as reliable byte streams. However, the packet
transport does attempt to report problems to its clients, by providing a
modest amount of error control (a 16-bit checksum), notifying senders of
discarded packets when possible, etc. These services are intended to
improve performance in the face of unreliable communication and
overloading; since they too are best efforts, they don't complicate the
implementation much.

There are two problems with the end-to-end strategy. First, it requires a
cheap test for success.  Second, it can lead to working systems with severe
performance defects that may not appear until the system becomes
operational and is placed under heavy load.

    Remember thee?
    Yea, from the table of my memory
    I'll wipe away all trivial fond records,
    All saws of books, all forms, all pressures past,
    That youth and observation copied there;
    And thy commandment all alone shall live
    Within the book and volume of my brain,
    Unmix'd with baser matter. (I v 97)

• Log updates to record the truth about the state of an object. A log is a
very simple data structure that can be reliably written and read, and
cheaply forced out onto disk or other stable storage that can survive a
crash. Because it is append-only, the amount of writing is minimized, and
it is fairly easy to ensure that the log is valid no matter when a crash
occurs. It is also easy and cheap to duplicate the log, write copies on
tape, or whatever. Logs have been used for many years to ensure that
information in a data base is not lost [17], but the idea is a very general
one and can be used in ordinary file systems [35, 49] and in many other
less obvious situations. When a log holds the truth, the current state of
the object is very much like a hint (it isn't exactly a hint because there
is no cheap way to check its correctness).

To use the technique, record every update to an object as a log entry
consisting of the name of the update procedure and its arguments. The
procedure must be functional: when applied to the same arguments it must
always have the same effect. In other words, there is no state outside the
arguments that affects the operation of the procedure. This means that the
procedure call specified by the log entry can be re-executed later, and if
the object being updated is in the same state as when the update was first
done, it will end up in the same state as after the update was first done.
By induction, this means that a sequence of log entries can be re-executed,
starting with the same objects, and produce the same objects that were
produced in the original execution.

For this to work, two requirements must be satisfied:

    • The update procedure must be a true function:

	Its result does not depend on any state outside its arguments.
	It has no side effects, except on the object in whose log it appears.

    • The arguments must be values, one of:

	Immediate values, such as integers, strings, etc. An immediate
	value can be a large thing, like an array or even a list, but the
	entire value must be copied into the log entry.

	References to immutable objects.

Most objects of course are not immutable, since they are updated. However,
a particular version of an object is immutable; changes made to the object
change the version. A simple way to refer to an object version
unambiguously is with the pair [object identifier, number of updates]. If
the object identifier leads to the log for that object, then replaying the
specified number of log entries yields the particular version. Of course
doing this replay may require finding some other object versions, but as
long as each update refers only to existing versions, there won't be any
cycles and this process will terminate.

For example, the Bravo editor [24] has exactly two update functions for
editing a document:

    Replace(old: Interval, new: Interval)
    ChangeProperties(where: Interval, what: FormattingOp)

An Interval is a triple [document version, first character, last
character]. A FormattingOp is a function from properties to properties; a
property might be italic or leftMargin, and a FormattingOp might be
leftMargin: = leftMargin + 10 or italic: = true. Thus only two kinds of log
entries are needed. All the editing commands reduce to applications of
these two functions.

    Beware
    Of entrance to a quarrel, but, being in,
    Bear 't that th' opposed may beware of thee.

• Make actions atomic or restartable. An atomic action (often called a
transaction) is one that either completes or has no effect. For example, in
most main storage systems fetching or storing a word is atomic. The
advantages of atomic actions for fault-tolerance are obvious: if a failure
occurs during the action it has no effect, so that in recovering from a
failure it is not necessary to deal with any of the intermediate states of
the action [28]. Database systems have provided atomicity for some time
[17], using a log to store the information needed to complete or cancel an
action. The basic idea is to assign a unique identifier to each atomic
action and use it to label all the log entries associated with that action.
A commit record for the action [42] tells whether it is in progress,
committed (logically complete, even if some cleanup work remains to be
done), or aborted (logically canceled, even if some cleanup remains);
changes in the state of the commit record are also recorded as log entries.
An action cannot be committed unless there are log entries for all of its
updates. After a failure, recovery applies the log entries for each
committed action and undoes the updates for each aborted action. Many
variations on this scheme are possible [54].

For this to work, a log entry usually needs to be restartable. This means
that it can be partially executed any number of times before a complete
execution, without changing the result; sometimes such an action is called
‘idempotent'. For example, storing a set of values into a set of variables
is a restartable action; incrementing a variable by one is not. Restartable
log entries can be applied to the current state of the object; there is no
need to recover an old state.

This basic method can be used for any kind of permanent storage. If things
are simple enough a rather distorted version will work. The Alto file
system described above, for example, in effect uses the disk labels and
leader pages as a log and rebuilds its other data structures from these if
necessary. As in most file systems, it is only the file allocation and
directory actions that are atomic; the file system does not help the client
to make its updates atomic. The Juniper file system [35, 49] goes much
further, allowing each client to make an arbitrary set of updates as a
single atomic action. It uses a trick known as ‘shadow pages', in which
data pages are moved from the log into the files simply by changing the
pointers to them in the B-tree that implements the map from file addresses
to disk addresses; this trick was first used in the Cal system [50].
Cooperating clients of an ordinary file system can also implement atomic
actions, by checking whether recovery is needed before each access to a
file; when it is they carry out the entries in specially named log files
[40].

Atomic actions are not trivial to implement in general, although the
preceding discussion tries to show that they are not nearly as hard as
their public image suggests. Sometimes a weaker but cheaper method will do.
The Grapevine mail transport and registration system [1], for example,
maintains a replicated data base of names and distribution lists on a large
number of machines in a nationwide network. Updates are made at one site
and propagated to other sites using the mail system itself. This guarantees
that the updates will eventually arrive, but as sites fail and recover and
the network partitions, the order in which they arrive may vary greatly.
Each update message is time-stamped, and the latest one wins. After enough
time has passed, all the sites will receive all the updates and will all
agree. During the propagation, however, the sites may disagree, for example
about whether a person is a member of a certain distribution list. Such
occasional disagreements and delays are not very important to the
usefulness of this particular system.

5. Conclusion

    Most humbly do I take my leave, my lord.

Such a collection of good advice and anecdotes is rather tiresome to read;
perhaps it is best taken in small doses at bedtime. In extenuation I can
only plead that I have ignored most of these rules at least once, and
nearly always regretted it. The references tell fuller stories about the
systems or techniques that I have only sketched. Many of them also have
more complete rationalizations.

All the slogans are collected in Figure 1 near the beginning of the paper.

Acknowledgments

I am indebted to many sympathetic readers of earlier drafts of this paper
and to the comments of the program committee.

References

1. Birrell, A.D. et al. Grapevine: An exercise in distributed computing.
Comm. ACM 25, 4, April 1982, pp 260-273.

2. Bobrow, D.G. et al. Tenex: A paged time-sharing system for the PDP-10.
Comm. ACM 15, 3, March 1972, pp 135-143.

3. Boggs, D.R. et al. Pup: An internetwork architecture. IEEE Trans.
Communications COM-28, 4, April 1980, pp 612-624.

4. Breed, L.M and Lathwell, R.H. The implementation of APL/360. In
Interactive Systems for Experimental Applied Mathematics, Klerer and
Reinfelds, eds., Academic Press, 1968, pp 390-399.

5. Britton, K.H., et al. A procedure for designing abstract interfaces for
device interface modules. Proc.  5th Int'l Conf. Software Engineering, IEEE
Computer Society order no. 332, 1981, pp 195-204.

6. Brooks, F.H. The Mythical Man-Month, Addison-Wesley, 1975.

7. Burton, R.R. et al. Interlisp-D overview. In Papers on Interlisp-D,
Technical report SSL-80-4, Xerox Palo Alto Research Center, 1981.

8. Clark, D.W. et al. The memory system of a high-performance personal
computer. IEEE Trans.  Computers TC-30, 10, Oct. 1981, pp 715-733.

9. Creasy, R.J. The origin of the VM/370 time-sharing system. IBM J. Res.
Develop. 25, 5, Sep. 1981, pp 483-491.

10. Deutsch, L.P. and Grant. C.A. A flexible measurement tool for software
systems. Proc. IFIP Congress 1971, North-Holland.

11. Deutsch, L.P. and Bobrow, D.G. An efficient incremental automatic
garbage collector. Comm. ACM 19, 9, Sep. 1976, pp 522-526.

12. Deutsch, L.P. Efficient implementation of the Smalltalk-80 system.
Proc. 11th ACM Symposium on Principles of Programming Languages, 1984..

13. Dijkstra. E.W. et al. On-the-fly garbage collection: An exercise in
cooperation. Comm. ACM 21, 11, Nov. 1978, pp 966-975.

14. Ditzel, D.R. and McClellan, H.R. Register allocation for free: The C
machine stack cache. SIGPLAN Notices 17, 4, April 1982, pp 48-56.

15. Geschke, C.M, et al. Early experience with Mesa. Comm. ACM 20, 8, Aug.
1977, pp 540-553.

16. Gifford, D.K. Weighted voting for replicated data. Operating Systems
Review 13, 5, Dec. 1979, pp 150-162.

17. Gray, J. et al. The recovery manager of the System R database manager.
Computing Surveys 13, 2, June 1981, pp 223-242.

18. Hansen, P.M. et al. A performance evaluation of the Intel iAPX 432,
Computer Architecture News 10, 4, June 1982, pp 17-26.

19. Hoare, C.A.R. Hints on programming language design. SIGACT/SIGPLAN
Symposium on Principles of Programming Languages, Boston, Oct. 1973.

20. Hoare, C.A.R. Monitors: An operating system structuring concept. Comm.
ACM 17, 10, Oct. 1974, pp 549-557.

21. Ingalls, D. The Smalltalk graphics kernel. Byte 6, 8, Aug. 1981, pp
168-194.

22. Janson, P.A. Using type-extension to organize virtual-memory
mechanisms. Operating Systems Review 15, 4, Oct. 1981, pp 6-38.

23. Knuth, D.E. An empirical study of Fortran programs, Software−Practice
and Experience 1, 2, Mar.  1971, pp 105-133.

24. Lampson. B.W. Bravo manual. In Alto Users Handbook, Xerox Palo Alto
Research Center, 1976.

25. Lampson, B.W. and Redell, D.D. Experience with processes and monitors
in Mesa. Comm. ACM 23, 2, Feb. 1980, pp 105-117.

26. Lampson, B.W. et al. Electronic image processing system, U.S. Patent
4,203,154, May 1980.

27. Lampson, B.W. Replicated commit. Circulated at a workshop on
Fundamental Principles of Distributed Computing, Pala Mesa, CA, Dec. 1980.

28. Lampson, B.W. and Sturgis, H.E. Atomic transactions. In Distributed
Systems — An Advanced Course, Lecture Notes in Computer Science 105,
Springer, 1981, pp 246-265.

29. Lampson, B.W. and Sproull, R.S. An open operating system for a
single-user machine. Operating Systems Review 13, 5, Dec. 1979, pp 98-105.

30. Lampson, B.W. and Sturgis, H.E. Reflections on an operating system
design. Comm. ACM 19, 5, May 1976, pp 251-265.

31. McNeil, M. and Tracz, W. PL/1 program efficiency. SIGPLAN Notices 5, 6,
June 1980, pp 46-60.

32. McQuillan, J.M. and Walden, D.C. The ARPA network design decisions.
Computer Networks 1, Aug.  1977, pp 243-299.

33. Metcalfe, R.M. and Boggs, D.R. Ethernet: Distributed packet switching
for local computer networks.  Comm. ACM 19, 7, July 1976, pp 395-404.

34. Mitchell, J.G. Design and Construction of Flexible and Efficient
Interactive Programming Systems.  Garland, 1979.

35. Mitchell, J.G. and Dion, J. A comparison of two network-based file
servers. Comm. ACM 25, 4, April 1982, pp 233-245.

36. Needham, R.M. Personal communication. Dec. 1980.

37. Newman, W.M. and Sproull, R.F. Principles of Interactive Computer
Graphics, 2nd ed., McGrawHill, 1979.

38. Parnas, D.L. On the criteria to be used in decomposing systems into
modules. Comm. ACM 15, 12, Dec. 1972, pp 1053-1058.

39. Patterson, D.A. and Sequin, C.H. RISC 1: A reduced instruction set VLSI
computer. 8th Symp.  Computer Architecture, IEEE Computer Society order no.
346, May 1981, pp 443-457.

40. Paxton, W.H. A client-based transaction system to maintain data
integrity. Operating Systems Review 13, 5, Dec. 1979, pp 18-23.

41. Radin, G.H. The 801 minicomputer, SIGPLAN Notices 17, 4, April 1992, pp
39-47.

42. Redell, D.D. et al. Pilot: An operating system for a personal computer.
Comm. ACM 23, 2, Feb. 1980, pp 81-91.

43. Reed, D. Naming and Synchronization in a Decentralized Computer System,
MIT LCS TR-205. Sep.  1978.

44. Ritchie, D.M. and Thompson, K. The Unix time-sharing system. Bell
System Tech. J. 57, 6, July 1978, pp 1905-1930.

45. Rovner, P. Personal communication. Dec. 1982.

46. Saltzer, J.H. et al. End-to-end arguments in system design. Proc. 2nd
Int'l. Conf. Distributed Computing Systems, Paris, April 1981, pp 509-512.

47. Smith, D.C. et al. Designing the Star user interface. Byte 7,4, April
1982, pp 242-282 .

48. Smith, J.E. A study of branch prediction strategies. 8th Symp. Computer
Architecture, IEEE Computer Society order no. 346, May 1981, pp 135-148.

49. Sturgis, H.E, et al. Issues in the design and use of a distributed file
system. Operating Systems Review 14, 3, July 1980, pp 55-69.

50. Sturgis, H.E. A Postmortem for a Time Sharing System. Technical Report
CSL-74-l, Xerox Palo Alto Research Center, 1974.

51. Sweet, R., and Sandman, J. Static analysis of the Mesa instruction set.
SIGPLAN Notices 17, 4, April 1982, pp 158-166.

52. Tanenbaum, A. Implications of structured programming for machine
architecture. Comm. ACM 21, 3, March 1978, pp 237-246.

53. Thacker, C.P. et al. Alto: A personal computer. In Computer Structures:
Principles and Examples, 2nd ed., Siewiorek, Bell, and Newell, eds.,
McGraw-Hill,1982.

54. Traiger, I.L. Virtual memory management for data base systems.
Operating Systems Review 16, 4, Oct. 1982, pp 26-48.

55. Bentley, J.L. Writing Efficient Programs. Prentice-Hall, 1982.
1. Introduction

The UNIX TimeSharing System
Dennis M. Ritchie and Ken Thompson
Bell Laboratories

UNIX is a general-purpose, multi-user, interactive
operating system for the Digital Equipment Corporation
PDP-11/40 and 11/45 computers. It offers a number of
features seldom found even in larger operating systems,
including: (1) a hierarchical file system incorporating
demountable volumes; (2) compatible file, device, and
inter-process I/O; (3) the ability to initiate asynchronous
processes; (4) system command language selectable on a
per-user basis; and (5) over 100 subsystems including a
dozen languages. This paper discusses the nature and
implementation of the file system and of the user
command interface.
Key Words and Phrases: time-sharing, operating
system, file system, command language, PDP-11
CR Categories: 4.30, 4.32

Copyright © 1974, Association for Computing Machinery, Inc.
General permission to republish, but not for profit, all or part
of this material is granted provided that A C M ' s copyright notice
is given and that reference is made to the publication, to its date
of issue, and to the fact that reprinting privileges were granted
by permission of the Association for Computing Machinery.
This is a revised version of a paper presented at the Fourth
ACM Symposium on Operating Systems Principles, IBM Thomas
J. Watson Research Center, Yorktown Heights, New York, October
15-17, 1973. Authors' address: Bell Laboratories, Murray Hill,
NJ 07974.
365

There have been three versions of UNIX. The earliest
version (circa 196%70) ran on the Digital Equipment
Corporation PDP-7 and -9 computers. The second version ran on the unprotected PDP-11/20 computer. This
paper describes only the PDP-I 1/40 and /45 [1] system
since it is more modern and many of the differences
between it and older UNIX systems result from redesign
of features found to be deficient or lacking.
Since PDP-11 UNIX became operational in February
1971, about 40 installations have been put into service;
they are generally smaller than the system described
here. Most of them are engaged in applications such as
the preparation and formatting of patent applications
and other textual material, the collection and processing
of trouble data from various switching machines within
the Bell System, and recording and checking telephone
service orders. Our own installation is used mainly
for research in operating systems, languages, computer networks, and other topics in computer science,
and also for document preparation.
Perhaps the most important achievement of UNIX
is to demonstrate that a powerful operating system
for interactive use need not be expensive either in
equipment or in human effort: UNIXcan run on hardware
costing as little as $40,000, and less than two manyears were spent on the main system software. Yet
UNIX contains a number of features seldom offered even
in much larger systems. It is hoped, however, the users
of UNIX will find that the most important characteristics
of the system are its simplicity, elegance, and ease of use.
Besides the system proper, the major programs
available under UNIX are: assembler, text editor based
on QED [2], linking loader, symbolic debugger, compiler
for a language resembling BCPL [3] with types and
structures (C), interpreter for a dialect of BASIC, text
formatting program, Fortran compiler, Snobol interpreter, top-down compiler-compiler (TMC) [4], bottom-up compiler-compiler (YACC), form letter generator,
macro processor (M6) [5], and permuted index program.
There is also a host of maintenance, utility, recreation, and novelty programs. All of these programs were
written locally. It is worth noting that the system is
totally self-supporting. All UNIX software is maintained
under UNIX; likewise, UNIX documents are generated
and formatted by the UNIX editor and text formatting
program.

2. Hardware and Software Environment
The PDP-11/45 on which our UNIX installation is
implemented is a 16-bit word (8-bit byte) computer with
144K bytes of core memory; UNIX occupies 42K bytes.
This system, however, includes a very large number of
device drivers and enjoys a generous allotment of space
for I/O buffers and system tables; a minimal system
Communications
of
the ACM

July 1974
Volume 17
Number 7

capable of running the software mentioned above can
require as little as 50K bytes of core altogether.
The vDv-11 has a 1M byte fixed-head disk, used for
file system storage and swapping, four moving-head
disk drives which each provide 2.5M bytes on removable
disk cartridges, and a single moving-head disk drive
which uses removable 40M byte disk packs. There are
also a high-speed paper tape reader-punch, nine-track
magnetic tape, and DEctape (a variety of magnetic
tape facility in which individual records may be addressed and rewritten). Besides the console typewriter,
there are 14 variable-speed communications interfaces
attached to 100-series datasets and a 201 dataset interface used primarily for spooling printout to a communal line printer. There are also several one-of-a-kind
devices including a Picturephone ® interface, a voice
response unit, a voice synthesizer, a phototypesetter, a
digital switching network, and a satellite PDP-11/20
which generates vectors, curves, and characters on a
Tektronix 611 storage-tube display.
The greater part of UNIX software is written in the
above-mentioned C language [6]. Early versions of the
operating system were written in assembly language,
but during the summer of 1973, it was rewritten in C.
The size of the new system is about one third greater
than the old. Since the new system is not only much
easier to understand and to modify but also includes
m a n y functional improvements, including multiprogramming and the ability to share reentrant code
a m o n g several user programs, we considered this increase in size quite acceptable.

3. The File System
The most important role of UNIX is to provide a
file system. F r o m the point of view of the user, there
are three kinds of files: ordinary disk files, directories,
and special files.
3.1 Ordinary Files
A file contains whatever information the user places
on it, for example symbolic or binary (object) programs.
No particular structuring is expected by the system.
Files of text consist simply of a string of characters,
with lines demarcated by the new-line character. Binary
programs are sequences of words as they will appear
in core memory when the program starts executing. A
few user programs manipulate files with more structure:
the assembler generates and the loader expects an
object file in a particular format. However, the structure
of files is controlled by the programs which use them,
not by the system.
3.2 Directories
Directories provide the mapping between the names
of files and the files themselves, and thus induce a
structure on the file system as a whole. Each user has a
366

directory of his own files; he may also create subdirectories to contain groups of files conveniently treated
together. A directory behaves exactly like an ordinary
file except that it cannot be written on by unprivileged
programs, so that the system controls the contents
of directories. However, anyone with appropriate permission may read a directory just like any other file.
The system maintains several directories for its own
use. One of these is the root directory. All files in the
system can be found by tracing a path through a chain
of directories until the desired file is reached. The
starting point for such searches is often the root. Another
system directory contains all the programs provided for
general use; that is, all the commands. As will be seen,
however, it is by no means necessary that a program
reside in this directory for it to be executed.
Files are named by sequences of 14 or fewer
characters. When the name of a file is specified to the
system, it may be in the form of a path name, which is a
sequence of directory names separated by slashes " / "
and ending in a file name. If the sequence begins with a
slash, the search begins in the root directory. The
name /alpha/beta/gamma causes the system to search
the root for directory alpha, then to search alpha for
beta, finally to find gamma in beta. Gamma may be an
ordinary file, a directory, or a special file. As a limiting
case, the name " / " refers to the root itself.
A path name not starting with " / " causes the system to begin the search in the user's current directory.
Thus, the name alpha/beta specifies the file named
beta in subdirectory alpha of the current directory.
The simplest kind of name, for example alpha, refers to
a file which itself is found in the current directory. As
another limiting case, the null file name refers to the
current directory.
The same nondirectory file may appear in several
directories under possibly different names. This feature
is called/inking; a directory entry for a file is sometimes
called a link. UNIX differs from other systems in which
linking is permitted in that all links to a file have equal
status. That is, a file does not exist within a particular
directory; the directory entry for a file consists merely
of its name and a pointer to the information actually
describing the file. Thus a file exists independently of
any directory entry, although in practice a file is made
to disappear along with the last link to it.
Each directory always has at least two entries. The
name . . . . in each directory refers to the directory itself.
Thus a program may read the current directory under
the name " . " without knowing its complete path name.
The name " . . " by convention refers to the parent of
the directory in which it appears, that is, to the directory
in which it was created.
The directory structure is constrained to have the
form of a rooted tree. Except for the special entries
" " and " . . " , each directory must appear as an entry
in exactly one other, which is its parent. The reason
for this is to simplify the writing of programs which
Communications
of
the ACM

July 1974
Volume 17
Number 7

visit subtrees of the directory structure, and more important, to avoid the separation of portions of the
hierarchy. If arbitrary links to directories were permitted, it would be quite difficult to detect when the
last connection from the root to a directory was severed.

keeping which would otherwise be required to assure
removal of the links when the removable volume is
finally dismounted. In particular, in the root directories
of all file systems, removable or not, the name " . . "
refers to the directory itself instead of to its parent.

3.3 Special Files
Special files constitute the most unusual feature of
the UNIX file system. Each I/O device supported by
UNIX is associated with at least one such file. Special
files are read and written just like ordinary disk files,
but requests to read or write result in activation of the
associated device. An entry for each special file resides in
directory /dev, although a link may be made to one of
these files just like an ordinary file. Thus, for example,
to punch paper tape, one may write on the file/dev/ppt.
Special files exist for each communication line, each
disk, each tape drive, and for physical core memory.
Of course, the active disks and the core special file are
protected from indiscriminate access.
There is a threefold advantage in treating I/O devices
this way: file and device I / o are as similar as possible;
file and device names have the same syntax and meaning, so that a program expecting a file name as a parameter can be passed a device name; finally, special files
are subject to the same protection mechanism as regular
files.

3.5 Protection
Although the access control scheme in UNIX is quite
simple, it has some unusual features. Each user of the
system is assigned a unique user identification number.
When a file is created, it is marked with the user ID of
its owner. Also given for new files is a set of seven
protection bits. Six of these specify independently read,
write, and execute permission for the owner of the
file and for all other users.
If the seventh bit is on, the system will temporarily
change the user identification of the current user to
that of the creator of the file whenever the file is executed
as a program. This change in user ID is effective only
during the execution of the program which calls for it.
The set-user-ID feature provides for privileged programs which may use files inaccessible to other users.
For example, a program may keep an accounting file
which should neither be read nor changed except by
the program itself. If the set-user-identification bit is on
for the program, it may access the file although this
access might be forbidden to other programs invoked by
the given program's user. Since the actual user ID of
the invoker of any program is always available, setuser-Io programs may take any measures desired to
satisfy themselves as to their invoker's credentials. This
mechanism is used to allow users to execute the carefully written c o m m a n d s which call privileged system
entries. For example, there is a system entry invokable
only by the "super-user" (below) which creates an
empty directory. As indicated above, directories are
expected to have entries for " . " and " . . " . The command which creates a directory is owned by the superuser and has the set-user-ID bit set. After it checks its
invoker's authorization to create the specified directory,
it creates it and makes the entries for " " and " . . " .
Since anyone may set the set-user-ID bit on one of
his own files, this mechanism is generally available without administrative intervention. For example, this protection scheme easily solves the MOO accounting problem posed in [7].
The system recognizes one particular user ID (that of
the "super-user") as exempt from the usual constraints
on file access; thus (for example) programs may be
written to dump and reload the file system without unwanted interference from the protection system.

3.4 Removable File Systems
Although the root of the file system is always stored
on the same device, it is not necessary that the entire
file system hierarchy reside on this device. There is a
mount system request which has two arguments: the
name of an existing ordinary file, and the name of a
direct-access special file whose associated storage volume (e.g. disk pack) should have the structure of an
independent file system containing its own directory
hierarchy. The effect of mount is to cause references to
the heretofore ordinary file to refer instead to the root
directory of the file system on the removable volume.
In effect, mount replaces a leaf of the hierarchy tree
(the ordinary file) by a whole new subtree (the hierarchy
stored on the removable volume). After the mount,
there is virtually no distinction between files on the
removable volume and those in the permanent file
system. In our installation, for example, the root
directory resides on the fixed-head disk, and the large
disk drive, which contains user's files, is mounted by
the system initialization program; the four smaller disk
drives are available to users for mounting their own
disk packs. A mountable file system is generated by
writing on its corresponding special file. A utility program is available to create an empty file system, or one
may simply copy an existing file system.
There is only one exception to the rule of identical
treatment of files on different devices: no link may exist
between one file system hierarchy and another. This
restriction is enforced so as to avoid the elaborate book367

3.6 I / o Calls
The system calls to do I / o are designed to eliminate
the differences between the various devices and styles of
access. There is no distinction between " r a n d o m " and
"sequential" I/O, nor is any logical record size imposed
by the system. The size of an ordinary file is determined
Communications
of
the ACM

July 1974
Volume 17
Number 7

by the highest byte written on it; no predetermination
of the size of a file is necessary or possible.
To illustrate the essentials of I/O in UNIX, some of
the basic calls are summarized below in an anonymous
language which will indicate the required parameters
without getting into the complexities of machine
language programming. Each call to the system may
potentially result in an error return, which for simplicity is not represented in the calling sequence.
To read or write a file assumed to exist already, it
must be opened by the following call:
filep = open (name, flag)

Name indicates the name of the file. An arbitrary path
name may be given. The flag argument indicates whether
the file is to be read, written, or "updated," that is
read and written simultaneously.
The returned value filep is called a file descriptor.
It is a small integer used to identify the file in subsequent calls to read, write, or otherwise manipulate it.
To create a new file or completely rewrite an old
one, there is a create system call which creates the given
file if it does not exist, or truncates it to zero length if it
does exist. Create also opens the new file for writing
and, like open, returns a file descriptor.
There are no user-visible locks in the file system,
nor is there any restriction on the number of users who
may have a file open for reading or writing. Although
it is possible for the contents of a file to become
scrambled when two users write on it simultaneously,
in practice, difficulties do not arise. We take the view
that locks are neither necessary nor sufficient, in our
environment, to prevent interference between users of
the same file. They are unnecessary because we are
not faced with large, single-file data bases maintained
by independent processes. They are insufficient because
locks in the ordinary sense, whereby one user is prevented from writing on a file which another user is
reading, cannot prevent confusion when, for example,
both users are editing a file with an editor which makes a
copy of the file being edited.
It should be said that the system has sufficient
internal interlocks to maintain the logical consistency
of the file system when two users engage simultaneously
in such inconvenient activities as writing on the same
file, creating files in the same directory, or deleting
each other's open files.
Except as indicated below, reading and writing are
sequential. This means that if a particular byte in the
file was the last byte written (or read), the next I / o
call implicitly refers to the first following byte. For
each open file there is a pointer, maintained by the
system, which indicates the next byte to be read or
written. I f n bytes are read or written, the pointer
advances by n bytes.
Once a file is open, the following calls may be used:
n = read(filep, buffer, count)
n = write(filep, buffer, count)
368

Up to count bytes are transmitted between the file
specified by filep and the byte array specified by buffer.
The returned value n is the number of bytes actually
transmitted. In the write case, n is the same as count
except under exceptional conditions like I/O errors or
end of physical medium on special files; in a read,
however, n may without error be less than count. If the
read pointer is so near the end of the file that reading
count characters would cause reading beyond the end,
only sufficient bytes are transmitted to reach the end
of the file; also, typewriter-like devices never return
more than one line of input. When a read call returns
with n equal to zero, it indicates the end of the file.
For disk files this occurs when the read pointer becomes
equal to the current size of the file. It is possible to
generate an end-of-file from a typewriter by use of an
escape sequence which depends on the device used.
Bytes written on a file affect only those implied by
the position of the write pointer and the count; no other
part of the file is changed. If the last byte lies beyond
the end of the file, the file is grown as needed.
To do random (direct access) I/O, it is only necessary
to move the read or write pointer to the appropriate
location in the file.
location = seek(filep, base, offset)
The pointer associated with filep is moved to a position
offset bytes from the beginning of the file, from the
current position of the pointer, or from the end of the
file, depending on base. Offset may be negative. For
some devices (e.g. paper tape and typewriters) seek
calls are ignored. The actual offset from the beginning
of the file to which the pointer was moved is returned
in location.
3.6.1 Other I/O Calls. There are several additional
system entries having to do with I / o and with the file
system which will not be discussed. For example:
close a file, get the status of a file, change the protection mode or the owner of a file, create a directory,
make a link to an existing file, delete a file.
4. Implementation of the File System
As mentioned in §3.2 above, a directory entry contains only a name for the associated file and a pointer
to the file itself. This pointer is an integer called the
i-number (for index number) of the file. When the file
is accessed, its i-number is used as an index into a
system table (the i-list) stored in a known part of the
device on which the directory resides. The entry thereby
found (the file's i-node) contains the description of the
file as follows.
1. Its owner.
2. Its protection bits.
3. The physical disk or tape addresses for the file
contents.
4. Its size.
Communications
of
the ACM

July 1974
Volume 17
Number 7

5. Time of last modification.
6. The number of links to the file, that is, the number
of times it appears in a directory.
7. A bit indicating whether the file is a directory.
8. A bit indicating whether the file is a special file.
9. A bit indicating whether the file is "large" or "small."
The purpose of an open or create system call is to t a m
the path name given by the user into an i-number by
searching the explicitly or implicitly named directories.
Once a file is open, its device, i-number, and read/write
pointer are stored in a system table indexed by the
file descriptor returned by the open or create. Thus
the file descriptor supplied during a subsequent call to
read or write the file may be easily related to the information necessary to access the file.
When a new file is created, an i-node is allocated for
it and a directory entry is made which contains the
name of the file and the i-node number. Making a link
to an existing file involves creating a directory
entry with the new name, copying the i-number from
the original file entry, and incrementing the link-count
field of the i-node. Removing (deleting) a file is done by
decrementing the link-count of the i-node specified by
its directory entry and erasing the directory entry. If the
link-count drops to 0, any disk blocks in the file are
freed and the i-node is deallocate&
The space on all fixed or removable disks which
contain a file system is divided into a number of 512byte blocks logically addressed from 0 up to a limit
which depends on the device. There is space in the
i-node of each file for eight device addresses. A small
(nonspecial) file fits into eight or fewer blocks; in this
case the addresses of the blocks themselves are stored.
For large (nonspecial) files, each of the eight device
addresses may point to an indirect block of 256 addresses
of blocks constituting the file itself. Thus files may be as
large as 8.256-512, or 1,048,576 (22o) bytes.
The foregoing discussion applies to ordinary files.
When an I/O request is made to a file whose i-node
indicates that it is special, the last seven device address
words are immaterial, and the first is interpreted as a
pair of bytes which constitute an internal device name.
These bytes specify respectively a device type and subdevice number. The device type indicates which system
routine will deal with I / o on that device; the subdevice
number selects, for example, a disk drive attached to a
particular controller or one of several similar typewriter interfaces.
In this environment, the implementation of the
mount system call (§3.4) is quite straightforward. M o u n t
maintains a system table whose argument is the i-number and device name of the ordinary file specified during
the mount, and whose corresponding value is the device
name of the indicated special file. This table is searched
for each (i-number, device)-pair which turns up while
a path name is being scanned during an open or create;
if a match is found, the i-number is replaced by 1 (which
369

is the i-number of the root directory on all file systems),
and the device name is replaced by the table value.
To the user, both reading and writing of files appear
to be synchronous and unbuffered. That is, immediately
after return from a read call the data are available, and
conversely after a write the user's workspace may be
reused. In fact the system maintains a rather complicated
buffering mechanism which reduces greatly the number
of I/O operations required to access a file. Suppose a
write call is made specifying transmission of a single
byte.
UNiX will search its buffers to see whether the
affected disk block currently resides in core memory;
if not, it will be read in from the device. Then the
affected byte is replaced in the buffer, and an entry is
made in a list of blocks to be written. The return from
the write call may then take place, although the actual
I/O may not be completed until a later time. Conversely, if a single byte is read, the system determines whether the secondary storage block in which
the byte is located is already in one of the system's
buffers; if so, the byte can be returned immediately. If
not, the block is read into a buffer and the byte picked
out.
A program which reads or writes files in units of
512 bytes has an advantage over a program which
reads or writes a single byte at a time, but the gain is
not immense; it comes mainly from the avoidance of
system overhead. A program which is used rarely or
which does no great volume of I/O may quite reasonably
read and write in units as small as it wishes.
The notion of the i-list is an unusual feature of
UNIX. In practice, this method of organizing the file
system has proved quite reliable and easy to deal with.
To the system itself, one of its strengths is the fact that
each file has a short, unambiguous name which is
related in a simple way to the protection, addressing,
and other information needed to access the file. It also
permits a quite simple and rapid algorithm for checking
the consistency of a file system, for example verification that the portions of each device containing useful
information and those free to be allocated are disjoint
and together exhaust the space on the device. This
algorithm is independent of the directory hierarchy, since
it need only scan the linearly-organized i-list. At the
same time the notion of the i-list induces certain
peculiarities not found in other file system organizations. For example, there is the question of who is to
be charged for the space a file occupies, since all directory
entries for a file have equal status. Charging the owner
of a file is unfair, in general, since one user may create a
file, another may link to it, and the first user may delete
the file. The first user is still the owner of the file, but
it should be charged to the second user. The simplest
reasonably fair algorithm seems to be t o spread the
charges equally among users who have links to a file.
The current version of UNIX avoids the issue by not
charging any fees at all.
Communications
of
the ACM

July 1974
Volume 17
Number 7

4.1 Efficiency of the File System
To provide an indication of the overall efficiency of
UNIX and of the file system in particular, timings were
made of the assembly of a 7621-1ine program. The
assembly was run alone on the machine; the total
clock time was 35.9 sec, for a rate of 212 lines per sec.
The time was divided as follows: 63.5 percent assembler
execution time, 16.5 percent system overhead, 20.0
percent disk wait time. We will not attempt any interpretation of these figures nor any comparison with other
systems, but merely note that we are generally satisfied
with the overall performance of the system.

5. Processes and Images
An image is a computer execution environment. It
includes a core image, general register values, status of
open files, current directory, and the like. An image is
the current state of a pseudo computer.
A process is the execution of an image. While the
processor is executing on behalf of a process, the image
must reside in core; during the execution of other processes it remains in core unless the appearance of an
active, higher-priority process forces it to be swapped
out to the fixed-head disk.
The user-core part of an image is divided into three
logical segments. The program text segment begins at
location 0 in the virtual address space. During execution,
this segment is write-protected and a single copy of it
is shared among all processes executing the same program. At the first 8K byte boundary above the program
text segment in the virtual address space begins a nonshared, writable data segment, the size of which may be
extended by a system call. Starting at the highest address
in the virtual address space is a stack segment, which
automatically grows downward as the hardware's stack
pointer fluctuates.
5.1 Processes
Except while UNIX is bootstrapping itself into operation, a new process can come into existence only by
use of the fork system call:
processid = fork(label)
When fork is executed by a process, it splits into two
independently executing processes. The two processes
have independent copies of the original core image, and
share any open files. The new processes differ only in
that one is considered the parent process: in the parent,
control returns directly from the fork, while in the
child, control is passed to location label. The processid
returned by the fork call is the identification of the
other process.
Because the return points in the parent and child
process are not the same, each image existing after a
fork may determine whether it is the parent or child
process.
370

5.2 Pipes
Processes may communicate with related processes
using the same system read and write calls that are
used for file system I/O. The call
filep = pipe( )
returns a file descriptor filep and creates an interprocess
channel called a pipe. This channel, like other open
files, is passed from parent to child process in the image
by the fork call. A read using a pipe file descriptor waits
until another process writes using the file descriptor for
the same pipe. At this point, data are passed between
the images of the two processes. Neither process need
know that a pipe, rather than an ordinary file, is involved.
Although interprocess communication via pipes is
a quite valuable tool (see §6.2), it is not a completely
general mechanism since the pipe must be set up by a
c o m m o n ancestor of the processes involved.
5.3 Execution of Programs
Another major system primitive is invoked by
execute(file, args, argo, ..., arg,+)
which requests the system to read in and execute the
program named by file, passing it string arguments
argl, arg..,, ..., arg,+. Ordinarily, argl should be the same
string as file, so that the program may determine the
name by. which it was invoked. All the code and data
in the process using execute is replaced from thefile, but
open files, current directory, and interprocess relationships are unaltered. Only if the call fails, for example
because file could not be found or because its executepermission bit was not set, does a return take place from
the execute primitive; it resembles a " j u m p " machine
instruction rather than a subroutine call.
5.4 Process Synchronization
Another process control system call
processid = wait( )
causes its caller to suspend execution until one of its
children has completed execution. Then wait returns
the processid of the terminated process. An error return
is taken if the calling process has no descendants.
Certain status from the child process is also available.
Wait may also present status from a grandchild or
more distant ancestor; see §5.5.
5.5 Termination
Lastly,
exit (status)
terminates a process, destroys its image, closes its open
files, and generally obliterates it. When the parent is
notified through the wait primitive, the indicated status
is available to the parent; if the parent has already
terminated, the status is available to the grandparent,
Communications
of
the ACM

July 1974
Volume 17
Number 7

and so on. Processes may also terminate as a result of
various illegal actions or user-generated signals (§7
below).

there." On the other hand,
ed
ordinarily enters the editor, which takes requests from
the user via his typewriter. The c o m m a n d

6. The Shell
For most users, communication with UNIX is carried
on with the aid of a program called the Shell. The Shell
is a c o m m a n d line interpreter: it reads lines typed by
the user and interprets them as requests to execute
other programs. In simplest form, a c o m m a n d line
consists of the c o m m a n d name followed by arguments
to the command, all separated by spaces:
c o m m a n d argl arg~ • • - argn
The Shell splits up the c o m m a n d name and the arguments into separate strings. Then a file with name
command is sought; command may be a path name including the "/" character to specify any file in the system. If command is found, it is brought into core and
executed. The arguments collected by the Shell are
accessible to the command. When the c o m m a n d is
finished, the Shell resumes its own execution, and indicates its readiness to accept another c o m m a n d by
typing a p r o m p t character.
If file command cannot be found, the Shell prefixes
the string /bin/ to command and attempts again to
find the file. D i r e c t o r y / b i n contains all the commands
intended to be generally used.
6.1 Standard I/O
The discussion of I/O in §3 above seems to imply
that every file used by a program must be opened or
created by the program in order to get a file descriptor
for the file. Programs executed by the Shell, however,
start off with two open files which have file descriptors
0 and 1. As such a program begins execution, file 1 is
open for writing, and is best understood as the standard
output file. Except under circumstances indicated below, this file is the user's typewriter. Thus programs
which wish to write informative or diagnostic information ordinarily use file descriptor 1. Conversely, file 0
starts off open for reading, and programs which wish
to read messages typed by the user usually read this file.
The Shell is able to change the standard assignments
of these file descriptors from the user's typewriter
printer and keyboard. If one of the arguments to a
c o m m a n d is prefixed by ")", file descriptor 1 will, for
the duration of the command, refer to the file named
after the ")". For example,
Is
ordinarily lists, on the typewriter, the names of the files
in the current directory. The c o m m a n d
1s )there
creates a file called there and places the listing there.
Thus the argument ")there" means, "place output on
371

ed (script
interprets script as a file of editor commands; thus
"(script" means, "take input from script."
Although the file name following "(" or ")" appears
to be an argument to the command, in fact it is interpreted completely by the Shell and is not passed to
the c o m m a n d at all. Thus no special coding to handle
I/O redirection is needed within each command; the
c o m m a n d need merely use the standard file descriptors
0 and l where appropriate.
6.2 Filters
An extension of the standard l / o notion is used to
direct output from one c o m m a n d to the input of another.
A sequence of commands separated by vertical bars
causes the Shell to execute all the commands simultaneously and to arrange that the standard output of
each c o m m a n d be delivered to the standard input of
the next c o m m a n d in the sequence. Thus in the command line
ls l p r - 2 [ o p r

ls lists the names of the files in the current directory; its
output is passed to pr, which paginates its input with
dated headings. The argument "--2" means double
column. Likewise the output from pr is input to opr.
This c o m m a n d spools its input onto a file for off-line
printing.
This process could have been carried out more
clumsily by
ls )temp 1
pr -- 2 (temp 1 )temp2
opr (temp2
followed by removal of the temporary files. In the
absence of the ability to redirect output and input, a
still clumsier method would have been to require t h e / s
c o m m a n d to accept user requests to paginate its output, to print in multi-column format, and to arrange
that its output be delivered off-line. Actually it would
be surprising, and in fact unwise for efficiency reasons,
to expect authors of commands such as Is to provide
such a wide variety of output options.
A program such as pr which copies its standard input
to its standard output (with processing) is called a
.filter. Some filters which we have found useful perform character transliteration, sorting of the input, and
encryption and decryption.
6.3 Command Separators: Multitasking
Another feature provided by the Shell is relatively
straightforward. C o m m a n d s need not be on different
Communications
of
the ACM

July 1974
Volume 17
Number 7

lines; instead they may be separated by semicolons.
ls; ed
will first list the contents of the current directory, then
enter the editor.
A related feature is more interesting. If a c o m m a n d
is followed by "&", the Shell will not wait for the
c o m m a n d to finish before prompting again; instead, it
is ready immediately to accept a new command. For
example,
as source )output &
causes source to be assembled, with diagnostic output
going to output; no matter how long the assembly
takes, the Shell returns immediately. When the Shell
does not wait for the completion of a command, the
identification of the process running that c o m m a n d is
printed. This identification may be used to wait for
the completion of the c o m m a n d or to terminate it.
The " & " may be used several times in a line:
as source )output & ls )files &
does both the assembly and the listing in the background. In the examples above using "&", an output
file other than the typewriter was provided; if this had
not been done, the outputs of the various commands
would have been intermingled.
The Shell also allows parentheses in the above
operations. For example
(date; ls) )x &
prints the current date and time followed by a list of
the current directory onto the file x. The Shell also
returns immediately for another request.
6.4 The Shell as a Command: Command files
The Shell is itself a command, and may be called
recursively. Suppose file tryout contains the lines
as source
mv a. out testprog
testprog
The my c o m m a n d causes the file a.out to be renamed
testprog, a.out is the (binary) output of the assembler,
ready to be executed. Thus if the three lines above were
typed on the console, source would be assembled, the
resulting program named testprog, and testprog executed. When the lines are in tryout, the c o m m a n d
sh (tryout
would cause the Shell sh to execute the c o m m a n d s
sequentially.
The Shell has further capabilities, including the
ability to substitute parameters and to construct argument lists from a specified subset of the file names in a
directory. It is also possible to execute c o m m a n d s
conditionally on character string comparisons or on
existence of given files and to perform transfers of
control within filed c o m m a n d sequences.
372

6.5 Implementation of the Shell
The outline of the operation of the Shell can now
be understood. Most of the time, the Shell is waiting
for the user to type a command. When the new-line
character ending the line is typed, the Shell's read
call returns. The Shell analyzes the c o m m a n d line,
putting the arguments in a form appropriate for execute.
Then fork is called. The child process, whose code of
course is still that of the Shell, attempts to perform an
execute with the appropriate arguments. If successful,
this will bring in and start execution of the program
whose name was given. Meanwhile, the other process
resulting from the fork, which is the parent process,
waits for the child process to die. When this happens,
the Shell knows the c o m m a n d is finished, so it types
its prompt and reads the typewriter to obtain another
command.
Given this framework, the implementation of background processes is trivial; whenever a c o m m a n d line
contains "&", the Shell merely refrains from waiting
for the process which it created to execute the command.
Happily, all of this mechanism meshes very nicely
with the notion of standard input and output files.
When a process is created by the fork primitive, it
inherits not only the core image of its parent but also
all the files currently open in its parent, including those
with file descriptors 0 and 1. The Shell, of course, uses
these files to read c o m m a n d lines and to write its
prompts and diagnostics, and in the ordinary case its
children--the c o m m a n d programs--inherit them automatically. When an argument with "(" or ")" is given
however, the offspring process, just before it performs
execute, makes the standard I/O file descriptor 0 or 1
respectively refer to the named file. This is easy because,
by agreement, the smallest unused file descriptor is
assigned when a new file is opened (or created); it is
only necessary to close file 0 (or 1) and open the named
file. Because the process in which the c o m m a n d program runs simply terminates when it is through, the
association between a file specified after " ( " or ")"
and file descriptor 0 or 1 is ended automatically when
the process dies. Therefore the Shell need not know the
actual names of the files which are its own standard
input and output since it need never reopen them.
Filters are straightforward extensions of standard
I / o redirection with pipes used instead of files.
In ordinary circumstances, the main loop of the
Shell never terminates. (The main loop includes that
branch of the return from fork belonging to the parent
process; that is, the branch which does a wait, then
reads another c o m m a n d line.) The one thing which
causes the Shell to terminate is discovering an end-of-file
condition on its input file. Thus, when the Shell is
executed as a c o m m a n d with a given input file, as in

sh (comfile
the commands in comfile will be executed until the
end of comfile is reached; then the instance of the
Communications
of
the ACM

July 1974
Volume 17
Number 7

Shell invoked by sh will terminate. Since this Shell
process is the child of another instance of the Shell,
the wait executed in the latter will return, and another
c o m m a n d may be processed.
6.6 Initialization
The instances of the Shell to which users type commands are themselves children of another process.
The last step in the initialization of UNIX is the creation
of a single process and the invocation (via execute)
of a program called init. The role of init is to create one.
process for each typewriter channel which may be dialed
up by a user. The various subinstances of init open the
appropriate typewriters for input and output. Since
when init was invoked there were no files open, in each
process the typewriter keyboard will receive file descriptor 0 and the printer file descriptor 1. Each process
types out a message requesting that the user log in and
waits, reading the typewriter, for a reply. At the outset,
no one is logged in, so each process simply hangs.
Finally someone types his name or other identification.
The appropriate instance of init wakes up, receives the
log-in line, and reads a password file. If the user name
is found, and if he is able to supply the correct password, init changes to the user's default current directory,
sets the process's user ID to that of the person logging
in, and performs an execute of the Shell. At this point
the Shell is ready to receive commands and the logging-in
protocol is complete.
Meanwhile, the mainstream path of init (the parent
of all the subinstances of itself which will later become
Shells) does a wait. If one of the child processes terminates, either because a Shell found an end of file or
because a user typed an incorrect name or password,
this path of init simply recreates the defunct process,
which in turn reopens the appropriate input and output
files and types another login message. Thus a user may
log out simply by typing the end-of-file sequence in
place of a c o m m a n d to the Shell.
6.7 Other Programs as Shell
The Shell as described above is designed to allow
users full access to the facilities of the system since it
will invoke the execution of any program with appropriate protection mode. Sometimes, however, a different interface to the system is desirable, and this
feature is easily arranged.
Recall that after a user has successfully logged in by
supplying his name and password, init ordinarily invokes
the Shell to interpret c o m m a n d lines. The user's entry
in the password file may contain the name of a program
to be invoked after login instead of the Shell. This
program is free to interpret the user's messages in any
way it wishes.
For example, the password file entries for users of a
secretarial editing system specify that the editor ed is
to be used instead of the Shell. Thus when editing
system users log in, they are inside the editor and can
373

begin work immediately; also, they can be prevented
from invoking UN;X programs not intended for their
use. In practice, it has proved desirable to allow a
temporary escape from the editor to execute the formatting program and other utilities.
Several of the games (e.g. chess, blackjack, 3D tictac-toe) available on UNIX illustrate a much more severely
restricted environment. For each of these an entry
exists in the password file specifying that the appropriate game-playing program is to be invoked instead
of the Shell. People who log in as a player of one of the
games find themselves limited to the game and unable
to investigate the presumably more interesting offerings
of UNJX as a whole.

7. Traps

The PDP-1 l hardware detects a number of program
faults, such as references to nonexistent memory,
unimplemented instructiont-!boe!pee addresses used
where an even address is required. Such faults cause the
processor to trap to a system routine. When an illegal
action is caught, unless other arrangements have been
made, the system terminates the process and writes
the user's image on file core in the current directory. A
debugger can be used to determine the state of the
program at the time of the fault.
Programs which are looping, which produce unwanted output, or about which' the user has second
thoughts may be halted by the use of the interrupt
signal, which is generated by typing the "delete"
character. Unless special action has been taken, this
signal simply causes the program to cease execution
without producing a core image file.
There is also a quit signal which is used to force a
core image to be produced. Thus programs which loop
unexpectedly may be halted and the core image examined without prearrangement.
The hardware-generated faults and the interrupt and
quit signals can, by request, be either ignored or caught
by the process. For example, the Shell ignores quits to"
prevent a quit from logging the user out. The editor
catches interrupts and returns to its c o m m a n d level.
This is useful for stopping long printouts without losing
work in progress (the editor manipulates a copy of
the file it is editing). In systems without floating point
hardware, unimplemented instructions are caught, and
floating point instructions are interpreted.

8. Perspective
Perhaps paradoxically, the success of UNIX is largely
due to the fact that it was not designed to meet any
predefined objectives. The first version was written
when one of us (Thompson), dissatisfied with the
available computer facilities, discovered a little-used
Communications
of
the ACM

July 1974
Volume 17
Number 7

PDP-7 and set out to create a more hospitable environment. This essentially personal effort was sufficiently
successful to gain the interest of the remaining author
and others, and later to justify the acquisition of the
POP-11/20, specifically to support a text editing and
formatting system. When in turn the 11/20 was outgrown, UNIX had proved useful enough to persuade
management to invest in the PDP-11/45. Our goals
throughout the effort, when articulated at all, have
always concerned themselves with building a comfortable relationship with the machine and with exploring
ideas and inventions in operating systems. We have
not been faced with the need to satisfy someone else's
requirements, and for this freedom we are grateful.
Three considerations which influenced the design
of UNIX are visible in retrospect.
First, since we are programmers, we naturally
designed the system to make it easy to write, test, and
run programs. The most important expression of our
desire for programming convenience was that the
system was arranged for interactive use, even though
the original version only supported one user. We bebelieve that a properly-designed interactive system is
much more productive and satisfying to use than a
" b a t c h " system. Moreover such a system is rather
easily adaptable to noninteractive use, while the converse is not true.
Second, there have always been fairly severe size
constraints on the system and its software. Given the
partially antagonistic desires for reasonable efficiency
and expressive power, the size constraint has encouraged
not only economy but a certain elegance of design.
This may be a thinly disguised version of the "salvation through suffering" philosophy, but in our case it
worked.
Third, nearly from the start, the system was able to,
and did, maintain itself. This fact is more important
than it might seem. If designers of a system are forced
to use that system, they quickly become aware of its
functional and superficial deficiencies and are strongly
motivated to correct them before it is too late. Since
all source programs were always available and easily
modified on-line, we were willing to revise and rewrite
the system and its software when new ideas were
invented, discovered, or suggested by others.
The aspects of UNIX discussed in this paper exhibit
clearly at least the first two of these design considerations. The interface to the file system, for example, is
extremely convenient from a programming standpoint.
The lowest possible interface level is designed to
eliminate distinctions between the various devices and
files and between direct and sequential access. N o
large "access method" routines are required to insulate
the p r o g r a m m e r from the system calls; in fact, all
user programs either call the system directly or use a
small library program, only tens of instructions long,
which buffers a number of characters and reads or
writes them all at once.

Another important aspect of programming convenience is that there are no "control blocks" with a
complicated structure partially maintained by and depended on by the file system or other system calls.
Generally speaking, the contents of a program's address
space are the property of the program, and we have
tried to avoid placing restrictions on the data structures
within that address space.
Given the requirement that all programs should be
usable with any file or device as input or output, it is
also desirable from a space-efficiency standpoint to push
device-dependent considerations into the operating system itself. The only alternatives seem to be to load
routines for dealing with each device with all programs,
which is expensive in space, or to depend on some means
of dynamically linking to the routine appropriate to
each device when it is actually needed, which is expensive either in overhead or in hardware.
Likewise, the process control scheme and c o m m a n d
interface have proved both convenient and efficient.
Since the Shell operates as an ordinary, swappable user
program, it consumes no wired-down space in the
system proper, and it may be made as powerful as
desired at little cost. In particular, given the framework .
in which the Shell executes as a process which spawns
other processes to perform commands, the notions of
I/O redirection, background processes, c o m m a n d flies,
and user-selectable system interfaces all become essentially trivial to implement.

374

Communications
of
the ACM

8.1 Influences

The success of'UNIX lies not so much in new inventions but rather in the full exploitation of a carefully
selected set of fertile ideas, and especially in showing
that they can be keys to the implementation of a small
yet powerful operating system.
The fork operation, essentially as we implemented it,
was present in the Berkeley time-sharing system [8]. On
a number of points we were influenced by Multics, which
suggested the particular form of the I / o system calls
[9] and both the name of the Shell and its general functions. The notion that the Shell should create a process
for each c o m m a n d was also suggested to us by the
early design of Multics, although in that system it was
later dropped for efficiency reasons. A similar scheme
is used by TENEX [10].

9. Statistics

The following statistics from UNIX are presented to
show the scale of the system and to show how a system
of this scale is used. Those of our users not involved in
document preparation tend to use the system for program development, especially language work. There are
few important "applications" programs.

July 1974
Volume 17
Number 7

9.1 Overall
72 user population
14 maximum simultaneous users
300 directories
4400 files
34000 512-byte secondary storage blocks used
9.2 Per day (24-hour d a y , 7 - d a y week basis)
T h e r e is a " b a c k g r o u n d " process t h a t runs at the
lowest possible p r i o r i t y ; it is used to s o a k u p a n y idle
c P u time. It has been used to p r o d u c e a m i l l i o n - d i g i t
a p p r o x i m a t i o n to the c o n s t a n t e - 2, a n d is n o w
g e n e r a t i n g c o m p o s i t e p s e u d o p r i m e s (base 2).
1800 commands
4.3 CPU hours (aside from background)
70 connect hours
30 different users
75 logins

9.3 Command CPU Usage (cut off at 1%)
15.7% Ccompiler
15.2% users' programs
11.7% editor
5.8% Shell (used as a cornmand, including command times)
5.3% chess
3.3% list directory
3.1% document formatter
1.6% backup dumper
1.8% assembler

1.7%
1.6%
1.6%
1.6%
1.4%
1.3%
1.3%
1.1%
1.0%

Fortran compiler
remove file
tape archive
file system consistency
check
library maintainer
concatenate/printfiles
paginate and print file
print disk usage
copy file

9.4 Command Accesses (cut off at 1%)
15.3%
9.6%
6.3%
6.3%
6.0%
6.0%
3.3%
3.2%
3.1%
1.8%
1.8%
1.6%

editor
list directory
remove file
C compiler
concatenate/printfile
users' programs
list people logged on
system
rename/move file
file status
library maintainer
document formatter
execute another command conditionally

1.6%
1.6%
1.5%
1.4%
1.4%
1.4%
1.2%
1.1%
1.1%
1.1%

debugger
Shell (used as a command)
print disk availability
list processes executing
assembler
print arguments
copy file
paginate and print file
print current date/time
file system consistency
check
1.0% tape archive

ficulties such as p o w e r d i p s a n d i n e x p l i c a b l e p r o c e s s o r
i n t e r r u p t s to r a n d o m locations. T h e r e m a i n d e r are
s o f t w a r e failures. T h e longest u n i n t e r r u p t e d up time
was a b o u t two weeks. Service calls average one every
t h r e e weeks, b u t are h e a v i l y clustered. T o t a l up time
has been a b o u t 98 p e r c e n t o f o u r 24-hour, 365-day
schedule.
Acknowledgments. W e are grateful to R . H . C a n a d a y ,
L.L. Cherry, a n d L.E. M c M a h o n for their c o n t r i b u tions to uNIX. W e are p a r t i c u l a r l y a p p r e c i a t i v e o f the
inventiveness, t h o u g h t f u l criticism, a n d c o n s t a n t supp o r t o f R. M o r r i s , M . D . M c I l r o y , a n d J.F. O s s a n n a .

References
1. Digital Equipment Corporation. PDP-I1/40 Processor
Handbook, 1972, and PDP-I1/45 Processor Handbook, 1971.
2. Deutsch, L.P., and Lampson, B.W. An online editor. Comm.
ACM 10, 12 (Dec. 1967), 793-799, 803.
3. Richards, M. BCPL: A tool for compiler writing and system
programming. Proc. AFIPS 1969 SJCC, Vol. 34, AFIPS Press,
Montvale, N.J., pp. 557-566.
4. McClure, R.M. TMG--A syntax directed compiler. Proc.
ACM 20th Nat. Conf., ACM, 1965, New York, pp. 262-274.
5. Hall, A.D. The M6 macroprocessor. Computing Science Tech.
Rep.#2, Bell Telephone Laboratories, 1969.
6. Ritchie, D.M. C reference manual. Unpublished memorandum,
Bell Telephone Laboratories, 1973.
7. Aleph-null. Computer Recreations. So[?ware Practice and
Experience 1, 2 (Apr.-June 1971), 201-204.
8. Deutsch, L.P., and Lampson, B.W. SDS 930 time-sharing
system preliminary reference manual. Doc. 30.10.10, Project G ENI E,
U of California at Berkeley, Apr. 1965.
9. Feiertag, R.J., and Organick, E.I. The Multics input-output
system. Proc. Third Syrup. on Oper. Syst. Princ., Oct. 18-20, 1971,
ACM, New York, pp. 35-41.
10. Bobrow, D.G., Burchfiel, J.D., Murphy, D.L., and Tomlinson,
R.S. TENEX, a paged time sharing system tbr the PDP-10. Comm.
ACM15, 3 (Mar. 1972), 135-143.

9.5 Reliability
O u r statistics on reliability are m u c h m o r e subjective
t h a n the others. T h e f o l l o w i n g results are true to the
best o f o u r c o m b i n e d recollections. T h e t i m e s p a n is
over one y e a r with a very early vintage 11/45.
T h e r e has been one loss o f a file system (one d i s k
o u t o f five) caused b y s o f t w a r e i n a b i l i t y to c o p e with
a h a r d w a r e p r o b l e m c a u s i n g r e p e a t e d p o w e r fail traps.
Files on t h a t d i s k were b a c k e d up three days.
A " c r a s h " is an u n s c h e d u l e d system r e b o o t o r
halt. T h e r e is a b o u t one crash every o t h e r d a y ; a b o u t
t w o - t h i r d s o f t h e m are c a u s e d by h a r d w a r e - r e l a t e d dif-

375

Communications
of
the ACM

July 1974
Volume 17
Number 7

THE STRUCTURE OF THE "THE"-MULTIPROGRAMMING SYSTEM

EWDI 96

Edsger W.Dijkstra
Technological University
EINDHOVEN
The Netherlands

Summary
A multiprogramming system is described in
which all activities are divided over a number of
sequential processes. These sequential processes
are placed at various hierarchical levels, in each
of which one or more independent abstractions have
been implemented. The hierarchical structure proved
to be vital for the verification of the logical
soundness of the design and the correctness of its
implementation.
Introduction
Papers "reporting on timely research and
development efforts" being explicitly asked for, I
shall try to present a progress report on the multiprogramming effort at the Department of Mathematics
at the Technological University, Eindhoven, the
Netherlands.
Having very limited resources (viz. a group of
six people of, on the average, half time availability) and wishing to contribute to the art of system
design -including all the stages of conception,
construction and verification- we are faced with the
problem of how to get the necessary experience. To
solve this problem we have adopted the following
three guiding principles:
I) Select a project as advanced as you can
conceive, as ambitious as you can justify, in the
hope that routine work can be kept to a minimum;
hold out against all pressure to incorporate such
system expansions that would only result into a
purely quantitative increase of the total amount of
work to be done.
2) Select a machine with sound basic characteristics (e.g. an interrupt system to fall in love
with is certainly an inspiring feature); from then
onwards try to keep the specific properties of the
configuration for which you are preparing the system
out of your considerations as long as possible.
3) Be aware of the fact that experience does by
no means automatically lead to wisdom and understanding; in other words, make a conscious effort to
learn as much as possible from your precious
experiences.
Accordingly, I shall try to go beyond just
reporting what we have done and how, and I shall
try to formulate as well what we have learned.
I should like to end the introduction with two
short remarks on working conditions, remarks I make
for the sake of completeness. I shall not stress
these points any further.

The one remark is that production speed is
severely degraded if one works with half time people
who have other obligations as well. This is at least
a factor four, probably it is worse. The people
themselves lose time and energy in switching over,
the group as a whole loses decision speed as discussions~ when needed, have often to be postponed
until all people concerned are available.
The other remark is that the members of the
group (mostly mathematicians) have previously
enjoyed as good students a university training of
5 to 8 years and are of Master's or Ph.D. level.
I mention this explicitly because at least in my
country the intellectual level needed for system
design is in general grossly underestimated. I am
more than ever convinced that this type of work is
just difficult and that every effort to do it with
other than the best people is doomed to either
failure or moderate success at enormous expenses.
The Tool and the Goal
The system has been designed for a Dutch
machine, the EL X8 (N.V.Electrologica, Rijswijk
(ZH)). Characteristics of our configuration are:
I) core memory cycle time 2.5 mms., 27 bits; at
present 32K.
2) drum of 512K words, 1024 words per track, rev.
time 40 ms.
3) an indirect addressing mechanism very well
suited for stack implementation
4) a sound system for commanding peripherals and
controlling of interrupts
5) a potentially great number of low capacity
channels; ten of them are used (3 paper tape readers
at 1000 char/sac; 3 paper tape punches at 150 char/
sec; 2 teleprinters; a plotter; a line printer)
6) absence of a number of not unusual awkward
features.
The primary goal of the system is to process
smoothly a continuous flow of user programs as a
service to the University. A multiprogramming
system has been chosen with the following objectives
in mind:
I) a reduction of turn around time for programs of
short duration
2) economic use of peripheral devices
3)automatic control of backing store to be combined
with economic use of the central processor
4) the economic feasibility to use the machine for
those applications for which only the flexibility of

a general purpose computer is needed but (as a r u l e )
not the capacity nor the processing power.

The system is not intended as a multi-access
system. There is no common data base via which
independent users can communicate with each other:
they only share the configuration and a procedure
library (that includes a translator for ALGOL 60
extended with complex numbers). The system does not
cater for user programs written in machine language.
Compared with larger efforts one can state that
quantitatively speaking the goals have been set as
modest as the equipment and our other resources.
Qualitatively speaking, I am afraid, we got more and
more immodest as the work progressed.

minutes (classical) inspection at the machine and
each of them correspondingly easy to remedy. At the
moment of writing the testing is not yet completed,
but the resulting system will be guaranteed to be
flawless. When the system has been delivered we shall
not live in the perpetual fear that a system derailment may still occur in an unlikely situation such as
might result from an unhappy "coincidence" of two or
more critical occurrences, for we shall have proved
the correctness of the system with a rigour and
explicitness that is unusual for the great majority
of mathematical proofs.
A Survey of the System Structure

A Progress Report
Storage Allocation.
We have made some minor mistakes of the usual
type (such as paying too much attention to speeding
up what was not the real bottle neck) and two major
ones.
Our f i r s t
m a j o r m i s t a k e has been t h a t f o r t o o
long a time we confined our attention to "a perfect
installation": by the time we considered how to make
the best of it when, say, one of the peripherals
broke down, we were faced with nasty problems.
Taking care of the "pathology" took more energy than
we had expected and part of our troubles were a
direct consequence of our earlier ingenuity, i.e.
the complexity of the situation into which the system
could have manoeuvred itself. Had we paid attention
to the pathology at an earlier stage of the design,
our management rules would certainly have been less
refined.

The second major mistake has been that we
conceived and programmed the major part of the system
without giving more than scanty thought to the problem of debugging it. For the fact that this mistake
had no serious consequences -on the contrary~ one
might argue as an afterthought- I must decline all
credit. I feel more like having passed through the
eye of the needle...
As captain of the crew I had had extensive
experience (dating back to 1958) in making basic
software dealing with real time interrupts and I
knew by bitter experience that as a result of the
irreproducibility of the interrupt moments, a program
error could present itself misleadingly like an
occasional machine malfunctioning. As a result I was
terribly afraid. Having fears regarding the possibility of debugging we decided to be as careful as
possible and -prevention is better than cure~- to
try to prevent nasty bugs from entering the construction.
This decision, inspired by fear, is at the
bottom of what I regard as the group's main contribution to the art of system design. We have found
that it is possible to design a refined multiprogramming system in such a way that its logical soundness
can be proved a priori and that its implementation
admits exhaustive testing. The only errors that
showed up during testing were trivial coding errors
(occurring with a density of one error per 500
instructions), each of them located within 10

In the classical yon Neumann machine information
is identified by the address of the memory location
containing the information. When we started to think
about the automatic control of secondary storage
we were familiar with a system (viz. GIER ALGOL)
in which all information was identified by its
drum address (as in the classical yon Neumann
machine) and in which the function of the core
memory was nothing more than to make the information
"page wise" accessible.
We have followed another approach and as it
turned out, to great advantage. In our terminology
we made a strict distinction between memory units
(we called them "pages" and had "core pages" and
"drum pages") and corresponding information units
(for lack of a better word we called them "segments")
a segment just fitting in a page. For segments we
created a completely independent identification
mechanism in which the number of possible segment
identifiers is much larger than the total number of
pages in primary and secondary store. The segment
identifier gives fast access to a so-called "segment
variable" in core whose value denotes whether the
segment is still empty or not and if not empty, in
which page (or pages) it can be found.
As a consequence of this approach: if a segment
of information, residing in a core page, has to be
dumped onto the drum in order to make the core page
available for other use, there is no need to return
the segment to the same drum page as it originally
came from. In fact, this freedom is exploited: among
the free drum pages the one with minimum latency
time is selected.
A next consequence is the total absence of a
drum allocation problem: there is not the slightest
reason why, say, a program should occupy consecutive
drum pages. In a multiprogramming environment this
is very convenient.
Processor Allocation.
We have given full recognition to the fact that
in a single sequential process (such as performed by
a sequential automaton) only the time succession of
the various states has a logical meaning, but not
the actual speed with which the sequential process
is performed. Therefore we have arranged the whole

system as a society of sequential processes, progressing with undefined speed ratios. To each user
program, accepted by the system, corresponds a
sequential process, to each input peripheral
corresponds a sequential process (buffering input
streams in synchronism with the execution of the
input commands), to each output peripheral corresponds a sequential process (unbuffering output
streams in synchronism with the execution of the
output commands); furthermore we have the "segment
controller" associated with the drum and the
"message interpreter" associated with the console
keyboard.
This enabled us to design the whole system in
terms of these abstract "sequential processes".
Their harmonious co-operation is regulated by means
of explicit mutual synchronization statements. On
the one hand, this explicit mutual synchronization
is necessary, as we do not make any assumption about
speed ratios, on the other hand this mutual synchronization is possible because "delaying the progress
of a process temporarily" can never be harmful to
the interior logic of the process delayed. The
fundamental consequence of this approach -viz. the
explicit mutual synchronization- is that the
harmonious co-operation of a set of such sequential
processes can be established by discrete reasoning;
as a further consequence the whole harmonious
society of co-operating sequential processes is
independent of the actual number of processors
available to carry out these processes, provided
the processors available can switch from process
to process.
System Hierarchy.
The total system admits a strict hierarchical
structure.
At level 0 we find the responsibility for
processor allocation to one of the processes whose
dynamic progress is logically permissible (i.e. in
view of the explicit mutual synchronization). At
this level the interrupt of the real time clock is
processed, introduced to prevent any process to
monopolize processing power. At this level a priority
rule is incorporated to achieve quick response of the
system where this is needed. Our first abstraction
has been achieved, above level 0 the number of
processors actually shared is no longer relevant. At
the higher levels we find the activity of the
different sequential processes, the actual processor
having lost its identity, having disappeared from
the picture.
At level I we have the so-called "segment
controller", a sequential process synchronized with
respect to the drum interrupt and the sequential
processes on higher levels. At level I we find the
responsibility to cater for the bookkeeping resulting
from the automatic backing store. At this level our
next abstraction has been achieved: at all higher
levels identification of information takes place in
terms of segments, the actual storage pages having
lost their identity, having disappeared from the

picture.
At level 2 we find the "message interpreter",
taking care of the allocation of the console keyboard
via which conversations between te operator and any
of the higher level processes can be carried out. The
message interpreter works in close synchronism with
the operator: when the operator presses a key, a
character is sent to the machine together with an
interrupt signal to announce this next keyboard
character, while the actual printing is then done
on account of an output command generated by the
machine under control of the message interpreter.
(As far as the hardware is concerned the console
teleprinter is regarded as two independent peripherals: an input keyboard and an output printer.) If
one of the processes opens a conversation it identifies itself for the benefit of the operator in the
opening sentence of this conversation. If, however,
the operator opens a conversation he must identify
the process he is addressing, in the opening sentence
of the conversation, i.e. this opening sentence must
be interpreted before it is known to which of the
processes the conversation is addressed~ There lies
the logical reason to introduce a separate sequential
process for the console teleprinter, a reason that
is reflected in its name "message interpreter". Above
level 2 it is as if each process had its private
conversational console. The fact that they share the
same physical console is translated into a resource
restriction of the form "only one conversation at a
time", a restriction that is satisfied via mutual
synchronization. At this level the next abstraction
has been implemented: at the higher levels the actual
console teleprinter has lost its identity. (If the
message interpreter had not been on a higher level
than the segment controller, then the only way to
implement it would have been to make a permanent
reservation in core for it; as the conversational
vocabulary might get large (as soon as our operators
wish to be addressed in fancy messages) this would
result in too heavy a permanent demand upon core
storage. Therefore the vocabulary in which the
messages are expressed is stored on segments, i.e.
as information units that can reside on the drum as
well. For this reason the message interpreter is of
a level one higher than the segment controller.)
At level 3 we find the sequential processes
associated with buffering of input streams and
unbuffering of output streams. At this level the next
abstraction is effected, viz. the abstraction of the
actual peripherals used, that are allocated at this
level to the "logical communication units" in terms
of which is worked in the still higher levels. The
sequential processes associated with the peripherals
are of a level above the message interpreter, because
they must be able to converse with the operator (e.g.
in the case of detected malfunctioning). The limited
number of peripherals again acts as a resource
restriction for the processes at higher levels, to be
satisfied by mutual synchronization between them.
At level 4 we find the independent user programs,
at level 5 the operator (not implemented by us).

The system structure has been described at
length in order to make the next section intelligible,
Design Experience
The conception stage took a long time. During
that period of time the concepts have been born in
terms of which we sketched the system in the previous
section. Furthermore we learnt the art of reasoning
by which we could deduce from our requirements the
way in which the processes should influence each
other as regards mutual synchronization so that
these requirements would be met. (The requirements
being that no information can be used before it has
been produced, that no peripheral can be set to two
tasks simultaneously, etc.) Finally we learnt the
art of reasoning by which we could prove that the
society composed of processes thus mutually synchronized by each other, would indeed in its time
behaviour satisfy all requirements.
The construction stage has been rather traditional, perhaps even old-fashioned: plain machine
code. Reprogramming on account of a change of
specifications has been rare, a circumstance that
must have contributed greatly to the feasibility of
the "steam method". The fact that the first two
stages took more time than planned was somewhat
compensated by a delay in the delivery of the machine.
In the verification stage we had, during short
shots, the machine completely at our disposal, shots
during which we worked with a virgin machine without
any software aids for debugging. Starting at level 0
the system has been tested, each time adding (a
portion of) the next level only after the previous
level had been thoroughly tested. Each test shot
itself contained on top of the (partial) system to
be tested a number of testing processes with a double
function. Firstly they had to force the system into
all different relevant states, secondly they had to
verify that the system continued to react according
to specification.
I shall not deny that the construction of these
testing programmes has been a major intellectual
effort: to convince oneself that one has not overlooked "e relevant state" and to convince oneself
that the testing programmes generate them all is no
simple matter. The encourageing thing is that (as
far as we are aware~) it could be done.
This fact was one of the happy consequences of
the hierarchical structure.
Testing level 0 (the real time clock and processor allocation) implied a number of testing
sequential processes on top of it, inspecting together that under all circumstances processor time
was divided among them according to the rules. This
being established, sequential processes as such had
been implemented.
Testing the segment controller at level I
meant that all "relevant states" could be formulated
in terms of sequential processes making (in various
combinations) demands on core pages, situations that

could be provoked by explicit synchronizing among
the testing programs. At that stage the existence
of the real time clock -although interrupting all
the time- was so immaterial that one of the testers
indeed forgot its existence~
By that time we had implemented the correct
reaction upon the (mutually unsynchronized) interrupts from the real time clock and the drum. If we
had not introduced the separate levels 0 and I and
if we had not created a terminology (viz. that of
the rather abstract sequential processes) in which
the existence of the clock interrupt could be discarded, but had tried instead to make in a nonhierarchical construction the central processor
directly react upon any weird time succession of
these two interrupts, the number of "relevant states"
would have exploded to such a height that exhaustive
testing would have been an illusion. (Apart from that
it is doubtful wether we would have had the means to
generate them all, drum and clock speed being outside
our control.)
For the sake of completeness I must mention
a further happy consequence. As stated before, above
level I core and drum pages have lost their identity
and buffering of input and output streams (at level
3) therefore occurs in terms of segments. While
testing at level 2 or 3 the drum channel hardware
broke down for quite some time, but testing could
proceed by restricting the number of segments so
that they all could be held in core. If building
up the line printer output streams had been implemented as "dumping onto the drum" and the actual printing as "printing from the drum" this advantage would
have been denied to us.
Conclusion
As far as program verification is concerned I
present nothing essentially new. In testing a
general purpose object (be it a piece of hardware,
a program, a machine or a system) one cannot subject
it to all possible cases: for a computer this would
imply that one feeds it with ell possible programs!
Therefore one must test it with a set of relevant
test cases. What is relevant or not, cannot be
decided as long as one regards the mechanism as a
black box, in other words it has to follow from the
internal structure of the mechanism to be tested. It
seems the designer's responsibility to construct his
mechanism in such a way -i.e. so effectively structured- that at each stage of the testing procedure
the number of relevant test cases is so small that
he can try them all and that what is being tested is
so perspicuous that it is clear that he has not
overlooked a situation. I have presented a survey of
our system because I think it a nice example of the
form that such a structure might take.
In my experience, I am sorry to say, industrial
software makers tend to react to it with mixed
feelings. On the one hand they are inclined to judge
that we have done a kind of model job, on the other
hand they express doubts whether the techniques used
are applicable outside the sheltered atmosphere of a

University Department and express the opinion that
we could only do it this way thanks to the modest
scope of the whole project. It is not my intention
to underestimate the organizing ability needed for
a much bigger job with ten or more times as many
people, but I should like to venture the opinion
that the larger the project, the more essential the
structuring~ A hierarchy of five logical levels
might then very well turn out to be of modest depth,
in particular when one designs the system more
consciously than we have done with the aim that the
software can be smoothly adapted to (perhaps drastic)
configuration expansions.
Acknowledqements
I should not like to publish this progress
report without expressing my great indebtedness to
my five collaborators C.Bron, A.N.Habermann, F.J.A.
Hendriks, C.Ligtmans and P.A.Voorhoeve. They have
contributed to ell the stages of the design, together
we learnt the art of reasoning needed. Construction
and verification is entirely their effort: if my
dreams have become true, this is due to their faith,
their talents and their persistent loyalty to the
whole project.
Finally I should like to thank the members of
the program committee who asked for more information
on the synchronizing primitives and some justification of my claim to be able to prove logical soundness a priori. In answer to this request the appendix
has been added, of which I hope that it gives the
desired information and justification.

"V(sem)" increases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is positive, the V-operation in question
has no further effect; if, however, the resulting
value of the semaphore concerned is non-positive,
one of the processes booked on its waiting list is
removed from this waiting list, i.e. its dynamic
progress is again logically permissible and in due
time a processor will be allocated to it (again, see
above "System Hierarchy", at level 0).
Corollary I:
If a semaphore value is nonpositive its absolute value equals the number of
processes booked on its waiting list.
Corollary 2:
The P-operation represents the
potential delay, the complementary V-operation
represents the removal of a barrier.
Note I:
P- and V-operations are "indivisible
actions", i.e. if they occur "simultaneously" in
parallel processes, they are non-interfering in the
sense that they can be regarded as being performed
the one after the other.
Note 2:
If the semaphore value resulting
from a V-operation is negative, its waiting list
did originally contain more than one process. It is
undefined -i.e. logically immaterial- which of the
waiting processes is then removed from the waiting
list.
Note 3:
A consequence of the mechanisms
described above is that a process whose dynamic
progress is permissible can only loose this status
by actually progressing, i.e. by performance of a
P-operation on a semaphore with a value that is
initially non-positive.

Appendix
The S~nchronizinq Primitives.
Explicit mutual synchronization of parallel
sequential processes is implemented via so-called
"semaphores". They are special purpose integer
variables allocated in the universe in which the
processes are embedded, they are initialized (with
the value 0 or I) before the parallel processes
themselves are started. After this initialization
the parallel processes will access the semaphores
only via two very specific operations, the so-called
synchronizing primitives. For historical reasons
they are called the P-operation and the V-operation.
A process, "Q" say, that performs the operation
"P(sem)" decreases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is non-negative, process Q can continue
with the execution of its next statement; if,
however, the resulting value is negative, process
Q is stopped and booked on a waiting list associated
with the semaphore concerned. Until further notice
(i.e. a V-operation on this very same semaphore)
dynamic progress of process Q is not logically
permissible and no processor will be allocated to
it (see above "System Hierarchy", at level 0).
A process, "R" say, that performs the operation

During system conception it transpired that we
used the semaphores in two completely different
ways. The difference is so marked that, looking
back, one wonders whether it was really fair to
present the two ways as a usage of the very same
primitives. On the one hand we have the semaphores
used for mutual exclusion, on the other hand the
private semaphores.
The Mutual Exclusion.
In the following program we indicate two
parallel, cyclic processes (between the brackets
"parbeRin" and '~arend") that come into action
after the surrounding universe has been introduced
and initialized.
begin semaphore mutex; mutex := I;
parbegin
beqin L I : P(mutex); critical section I; V(mutsx);
remainder of cycle I; ~oto LI
end;
beqin L2: P(mutex); critical section 2; V(mutex);
remainder of cycle 2; qoto L2
end
parend
end
As a result of the P- and V-operations on

"mutex" the actions, marked as "critical sections"
exclude each other mutually in time; the scheme
given allows straightforward extension to more than
two parallel processes, the maximum value of mutex
= I, the minimum value = - (n - I) if we have n
parallel processes.
Critical sections are used always and only for
the purpose of unambiguous inspection and modification of the state variables (allocated in the
surrounding universe) that describe the current
state of the system (as far as needed for the
regulation of the harmonious co-operation between
the various processes).
The Private Semaphores.
Each sequential process has associated with it
a number of private semaphores and no other process
will ever perform a P-operation on them. The universe
initializes them with the value = O, their maximum
value = I, their minimum value = - I.
Whenever a process reaches a stage where the
permission for dynamic progress depends on current
values of state variables, it follows the pattern:

P(mutex);
"inspection and modification of state variables
including a conditional V(private semaphore)";

V(mutex);
P(private semaphore)
If the inspection learns that the process in
question should continue, it performs the operation
"V(private semaphore)" -the semaphore value then
changes from 0 to I-, otherwise this V-operation
is skipped, leaving to the other processes the
obligation to perform this V-operation at a suitable
moment. The absence or presence of this obligation
is reflected in the final values of the state
variables upon leaving the critical section.
Whenever a process reaches a stage where as a
result of its progress possibly one (or more)
blocked processes should now get permission to
continue, it follows the pattern

P(mutex);
"modification and inspection of state variables
including zero or more V-operations on private
semaphores of other processes";

V(mutex)
By the introduction of suitable state variables
and appropriate programming of the critical sections
any strategy assigning peripherals, buffer areas etc.
can be implemented.
The amount of coding and reasoning can be
greatly reduced by the observation that in the two
complementary critical sections sketched above, the
same inspection can be performed by the introduction
of the notion of "an unstable situation", such as
a free reader and a process needing a reader.
Whenever an unstable situation emerges it is
removed (including ome or more V-operations on

private semaphores) in the very same critical
section in which it has been created.
Provinq the Harmonious Co-operation.
The sequential processes in the system can all
be regarded as cyclic processes in which a certain
neutral point can be marked, the so-called "homing
position", in which all processes are when the
system is at rest.
When a cyclic process leaves its homing position
"it accepts a task", when the task has been performed
end not earlier, the process returns to its homing
position. Each cyclic process has a specific task
processing power (e.g. the execution of a user
program or unbuffering a portion of printer output,
etc.)
The harmonious co-operation is mainly proved
in roughly three stages.
I)
It is proved that although a process
performing a task may generate in doing so a finite
number of tasks for other processes, a single
initial task cannot give rise to an infinite number
of task generations. The proof is simple as
processes can only generate tasks for processes
at lower levels of the hierarchy so that circularity
is excluded. (If a process needing a segment from
the drum has generated a task for the segment
controller, special precautions have been taken to
ensure that the segment asked for remains in core
at least until the requesting process has effectively
accessed the segment concerned. Without this precaution finite tasks could be forced to generate an
infinite number of tasks for the segment controller
and the system could get stuck in an unproductive
page flutter.)
2)
It is proved that it is impossible
that all processes have returned to their homing
position while somewhere in the system is still
pending a generated but unaccepted task. (This is
proved via instability of the situation just
described.)
3)
It is proved that after the acceptance
of an initial task all processes eventually will be
(again) in their homing position. Each process
blocked in the course of task execution relies on the
other processes for removal of the barrier. Essentially, the proof in question is a demonstratmon of the
absence of "circular waits": process P waiting for
process Q waiting for process R waiting for process
P. (Our usual term for the circular wait is "the
Deadly Embrace".) In a more general society than
our system this proof turned out to be e proof by
induction (on the level of hierarchy, starting at
the lowest level) as A.N.Habermann has shown in his
doctoral thesis.

THE WORKING SET MODEL FOR PROGRAM BEHAVIOR
Peter J. Denning
Massachusetts Institute of Technology
Cambridge, Massachusetts

SUMMARY

We claim neither is adequate.

Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate
model for program behavior.
In this paper a new
model is developed, the "working set model", which
enables us to decide which information is in use
by a running program and which is not.
Such knowledge is vital for dynamic management of paged
memories.
The working set of pages associated
with a process, defined to be the collection of its
most recently used pages, is a useful allocation
concept. A proposal for an easy-to-implement
allocation policy is set forth; this policy is
unique, inasmuch as it blends into one decision
function the heretofore independent activities of
process-scheduling and memory-management.

Because resources are multiplexed, each user
is given the illusion that he has a complete computing system at his sole disposal: a virtual
computer.
For our purposes, the basic elements of
a virtual computer are its virtual processor and
an "infinite" one-level virtual memory.
Dynamic
"advice" regarding resource requirements cannot be
obtained successfully from users for several
reasons:
i. A user may build his program on the work
of others, frequently sharing procedures
whose time and storage requirements may be
either unknown or, because of data dependence, indeterminate.
Therefore he cannot
be expected to estimate processor-memory
needs.
2. It is not clear what sort of "advice" might
be solicited.
Nor is it clear how the
operating system should use it, for overhead incurred by using advice could well
negate any advantages attained.
3. Any advice acquired from a user would be
intended (by him) to optimize the environment for his own program.
Configuring
resources to suit individuals may interfere
with overall good service to the community
of users.
Thus it seems inadvisable at the present time to
permit users, at their discretion, to advise the
operating system of their needs.

INTRODUCTION
Resource allocation is tricky business.
In
recent years there has been much dialogue on the
topics of process scheduling and core memory management, yet development of techniques has progressed independently along both these lines. No
one will deny that a unified approach is needed.
Probably the most basic reason behind the absence
of a general treatment is the lack of an adequate
model for program behavior.
In this paper we develop a new model, the working set model, which
embodies certain important behavioral properties
of computations operating in a multiprogrammed environment, enabling us to decide which information
is in use by a running program and which is not.
We do not intend that the proposed model be considered "final"; rather, we hope to stimulate a
new kind of thinking, thinking that may be of considerable help in solving many operating system
design problems.
The working set is intended to model the behavior of programs in the general purpose computer
system, or computer utility.
For this reason we
assume that the operating system must on its own
determine the behavior of programs it runs; it
cannot count on outside help.
Two commonly proposed sources of externally-supplied dynamic allocation information are the user and the compiler.

Work reported herein was supported in part by
Project MAC, an M.I.T. research project sponsored
by the Advanced Projects Research Agency, Dept.
of Defense, under Office of Naval Research Contract Number Nonr-4102(Ol).

Likewise, compilers cannot be expected to
supply information, extracted from the structure
of the program , regarding resource requirements:
i. Programs will be modular in construction;
information about other modules may be unavailable at compilation time.
Because of
dependence on data there may be no way to
decide (until run time) just which modules
will be included in a computation.
2. Compilers cluttered with extra machinery
to predict memory needs will be slower in
operation.
Many users are less interested
in whether their programs operate efficiently than whether they operate at all, and
so are concerned with rapid compilation.
Furthermore, the compiler is an often-used
component of the operating system; if slow
and bulky, it can be a serious drain on
system resources.

**Ramamoorthy I has put forth a proposal for automatic segmentation of programs during compilation.

Therefore in this paper we are advocating mechanisms that monitor the behavior of a computation,
making allocation decisions on the basis of currently observed characteristics.
Only a mechanism
that oversees the behavior of a program in operation can cope with arbitrary interconnections of
arbitrary modules having arbitrary characteristics.
Our treatment proceeds as follows. First we
define the type of computer system in which our
ideas are developed. After a brief discussion of
previous work with the problems of dynamic memory
management, we define the working set model. We
discuss a method of implementing memory management
based on this model, and indicate how working set
notions can be used to blend process scheduling
and memory management into one decision function,
accounting simultaneously for both types of demand.
Finally we discuss how data sharing fits into the
working-set scheme.

THE FRAMEWORK
We assume that the reader is already familiar
with the concepts of a computer utility 2'3'4, of
segmentation and paging 5'6, of program and addressing structure 6'8, so we will only mention these
topics here. Briefly, each process has access to
its own private, segmented name space; each segment known to the process is sliced into equalsize units, called pages, to facilitate mapping it
into the paged main memory.
Associated with each
segment is a page table, whose entries point to

the segment's pages. An "in-core" bit in each
page table entry is turned ON whenever the designated page is present in main memory ; an attempt
to reference a page whose "in-core" bit is OFF
causes a page fault, initiating proceedings to
secure the missing page. Finally, a process has
three states of existence: running, when a processor is assigned to it; ready, when it would be
running if only a processor were available; or
blocked, when it has no need of a processor (for
example, during a page fault or during a console
interaction). When talking about processes in execution, we will have to distinguish between "process time" and "real time". Process time is time
as seen by a process unaware it is suspended; that
is, as if it executed without interruptions.
We restrict attention to a two-level memory
system, indicated by Figure I. Only data residing
in main memory is accessible to a processor; all
other data reside in auxiliary memory, which we
regard as having i nflnite capacity. There is a
time T, the traverse time, involved in transferring
a page between memories. T is measured from the
moment a page fault occurs until the moment the
missing page is in main memory ready for use. T
is actually the expectation of a random variable
composed of waits in queues and mechanical positioning delays.
Though it usually takes less time to
store into auxiliary memory than to read from it,
we shall regard the traverse time T to be the same
regardless of which direction a page is moved.
,
Consistent with current usage, we will use the
terms "core memory" and "main memory" interchangeably.

PROCESSORS

~

~ ~

~ n

data flow from main memory
(controlled b y ~, c o r emanager)
I

main mem°rY(contrD~edfl~ ~ ~ ~ n m ~ [ c Y i e s )

FIGURE i.

Two-level memory system.

A basic allocation problem, "core memory management", is that of deciding just which pages are to
occupy main memory.
The basic strategy advocated
here -- a compromise against a lot of expensive
,
main memory -- is to minimize page traffic . There
are two reasons for this:
i. The more the data traffic between the two
levels of memory, the more the computational overhead in deciding just what to move
and where to move it.
2. Because the traverse time T is long compared
to a memory cycle, too much data movement
can result in congestion and serious interference with processor efficiency.
Roughly speaking, a working set of pages is
the minimum collection of pages that must be loaded
in main memory for a process to operate efficiently,
without "unnecessary" page faults. According to
our definitions, a "process" and its "working set"
are but two manifestations of the same ongoing
computational activity.

PREVIOUS WORK
In this section we outline strategies that have
been set forth in the past for memory management;
the interested reader will be referred to the literature for detail.
We regard management of paged memories to operate in two stages:
I. Pagin_g in." locate the required page in
auxiliary memory, load it into main memory,
turn the "in-core" bit of the appropriate
page table entry ON.
2. Paging out: remove some page from main memory, turn the "in-core" bit of the appropriate page table entry OFF.
Management algorithms can be classified according
to their methods of paging in and paging out.
It
is a common characteristic of nearly every strategy
that paging in is done on demand; that is, no action
is taken to load a page into memory until some
process attempts to reference it. To date there
have been no proposals recommending look-ahead, or
anticipatory page-loading, because (as we have
stressed) there is no reliable advance source of
allocation information, be it the programmer or
the compiler.
Although the working set is the
desired information, it might still be futile to
pre-load pages: there is no guarantee that a process will not block shortly after resumption,
having referenced only a fraction of its working
set.
The operating system could devote its already precious time to activities more rewarding
than loading pages which may not be used.
Thus we
will assume that paging in is done on demand only,
via the page fault mechansim.

Since data is stored and transmitted in units of
pages, we can (without ambiguity) refer to data
movement as "page traffic".

The chief problem in memory management is not
deciding which pages to load; it is deciding which
pages ought to be removed.
For if the page with
the least likelihood of being used in the immediate
future is retired to auxiliary memory, the best
choice has been made.
Nearly every worker in the
field has recognized this.
Debate has arisen over
which strategy to employ for retiring pages; that
is, which page-turning, or replacement, algorithm
to use.
A good measure of performance for a paging
policy is page traffic (the number of pages per
unit time being moved between memories), since
erroneously removed pages add to the traffic of
returning pages.
In the following we will use this
as a basis of comparison for several strategies.
Random selection.
Whenever a fresh page of memory
is needed, a page is selected at random to be replaced.
Although utterly simple to implement, this
method frequently removes useful pages (which must
therefore be recalled) and so results in high page
traffic.
~!%~
selection. The pages of main memory are ordered in a cyclic list.
Suppose the M pages of
main memory are numbered 0,1,...,(M-I) and a
pointer k indicates that the k-th page was most
recently paged in. Whenever a fresh page of memory
is needed, [(k+l) mod M] 4 k, page k is retired,
and another page brought in to fill the now vacant
slot.
This method -- also utterly simple to realize -- is based on the principle that programs tend
to follow sequences of instructions, so that references in the immediate future will most likely
be close to present references.
So, assuming there
is this tendency for page references to cluster,
and assuming some kind of uniformity in process
scheduling techniques, the page which has been in
memory longest is least likely to be reused: hence
the cyclic list. We see two ways in which this
algorithm can fail. First we question its basic
assumption.
It is not at all clear that modular
programs, which execute numerous inter-module calls,
will indeed exhibit sequential instruction fetch
patterns.
The thread of control will not string
pages together; rather, it will entwine them intricately.
[Fine, Mclssac, and Jackson 9 have some
experimental evidence in support of this reasoning.]
Second, this algorithm is subject to overloading when used in multiprogrammed memories.
When core demand is too heavy, one cycle of the
list completes rapidly and the pages deleted are
still needed by their processes.
This can create
a self-intensifying crisis.
Programs, deprived of
still-needed pages, generate a plethora of page
faults; the resulting traffic of returning pages
displaces still other useful pages, leading to more
page faults, and so on.
Oldest-unused selection.
Each page table entry
contains a ~us~ ~ bit, set ON each time the page is
referenced.
At periodic intervals all the page
table entries are searched and usage records updated.
When a fresh page of memory is needed, the page unreferenced for the longest time is removed.
One can

see that this method is intrinsically reasonable by
considering the simple case of a computer where
there is exactly one process whose pages cannot all
fit into main memory.
In this case the most reasonable choice for a page to replace is the oldest
unused page.
Unfortunately this method too is susceptible to overloading when many processes compete
for main memory.
ATLAS ioo~ detection method.

The Ferranti ATLAS

computer I0 had proposed a page-turning policy that
attempted to detect loop behavior in page reference
patterns, then minimize page traffic by maximizing
the time between page transfers, that is, by removing pages not expected to be needed for the longest time.
It was successful -- only for looping
programs.
Performance was unimpressive for programs exhibiting random reference patterns.
Implementation was costly.
Various studies concerning behavior of paging
algorithms have appeared.
Fine, Mclssac and
Jackson 9 have investigated the effects of demand
paging and have questioned whether paging is beneficial at all. We do not feel that their conclusion applies to the kind of multiprogrammed environment we have described.
They studied fixed-size
programs, that quickly acquired and retained a
large fraction of their pages.
Highly interactive,
modular programs are likely to behave differently.
Not only may program size vary dynamically (according to data dependencies), but also such programs
should be using a small fraction of their pages at
any one time, and the membership in this set of
working pages should be changing constantly.

We define the working set of information W(t,T)
of a process at time t to be the collection of data
items referenced by the process during the proces
time interval (t-T~,t).
Thus, the data items a process has referenced
during the last T seconds of its execution comprise
its working set. q will be called the working set
parameter.
We will regard the data items in W(t,T)
as being pages ,although they could just as well
be any other named data objects.
The working set
size ~(t,~) is
(I)

~(t,T)

=

Number of pages in W(t,T)

A working set W(t,~) has two important, general
properties.
Both are properties of typical programs,
and need not hold in special cases.
PI. Size. It should be clear immediately that
~(t,0) = 0 since no page reference can occur
in zero time.
It should also be clear that
~(t,T) as a function of T is monotonically
increasing, since more pages can be referenced
in longer time intervals.
Because a process
will refer to its most-needed pages frequently
and its least-needed pages infrequently, we
expect ~(t,~) as a function of T to have a
steep initial rise which diminishes to a more
gradual rise.
The general character of ~(t,~)
is suggested by the smoothed curve of Figure 2.
00(t ,~-)

Belady II has compared some of the algorithms
mathematically.
His most important conclusion is
that the "ideal" algorithm should possess much of
the simplicity of Random or Cyclic selection (for
efficiency) and some, though not much, accumulation
of data on past reference patterns.
He has shown
that too much "historical" data can have adverse
effects (witness ATLAS).
In the next section we begin investigation of
the working set concept.
Even though the ideas
are not entirely new 12'13'14, there has been no
detailed documentation publicly available.

THE WORKING SET MODEL
From the programmer's standpoint, the working
set of information is the smallest collection of
procedure and data items that must be present in
main memory to assure efficient execution of his
program.
We have already stressed that there will
be no advance notice from either the programmer or
the compiler regarding what information "ought" to
be in main memory.
It is up to the operating
system to determine on the basis of page reference
patterns whether pages are in use.
Therefore the
working set of information associated with a process is, from the system standpoint, the set of
most recently referenced pages.

FIGURE 2.

Behavior of ~(t,T).

Program modularity enables us to
P2. Correlation.
say something about correlation between the
size of a working set at two times, t and (t+c~).
Correlation is useful in devising storage
allocators, for the higher the correlation
between ~(t,T) and w(t+c~,T), the better is
~(t,T) a prediction of ~(t+c~,T).
In modular
programs, control passes randomly from one
module to another; if T is chosen properly
(as discussed in the next section), it is more
likely that a working set will change size
smoothly, less likely that it will change size
abruptly.
Thus for small time separations
(say, 6 < < T), ~(t,~) and ~(t+c~,T) are highly

correlated, meaning that a measurement of ~(t,T)
will serve as a good estimate of the memory requirement during the process time interval (t,t+c0.
For large time separations ~ (say, ~ >> .[), control
will have passed through a great many modules
during the interval (t,t+c~); thus ~(t,T ) gives
little information about ~(t+C~,7), and so ~(t,7)
and ~(t+c~,7) have much less correlation than for
small 5. This behavior is suggested in Figure 3.
Correlation between
~(t,7) and ~(t+Cz,7)

Detecting W(t~T )
According to our definition, W(t,T) is the set
of its pages a process has referenced within the
last T seconds of its execution.
This suggests
that memory management can be controlled by hardware mechanisms, by associating with each page of
main memory a timer.
Each time a page is referenced, its timer is set to T and begins to run
down; if the timer succeeds in running down, a flag
is set to mark the page for removal whenever the
space is needed.
In the appendix we describe such
a hardware memory management mechanism, hardware
that can be housed within the memory boxes.
The
mechanism has two interesting features:
i. It operates asynchronously and independently
of the supervisor, whose only respsonsibility
in memory management is handling page faults.
Quite literally, memory manages itself.
2. Analog devices such as capacitative timers
could be used to measure intervals.

~D

c~
0
FIGURE 3.

Correlation between working set sizes,

Choice of T

Unfortunately it is not practical to add on
hardware to existing systems. We seek a method of
handling memory management within the software.
The procedure we propose here samples the page
table entries of pages in core memory at process
time intervals of ~ seconds (~ is called the
sampling interval) where ~ = y/K , K an integer
constant chosen to make the sampling intervals as
"fine grain" as desired.
On the basis of page
references during each of the last K sampling intervals, the working set W(t,l~) can be determined,
as follows.

The value ultimately selected for ~ will reflect
efficiency requirements and will be influenced by
"in- core"
system parameters such as core memory size and memI
ory traverse time.
For example, if T is too small,
pages may be removed from main memory while they
are still useful, and high page traffic may result
from returning pages.
If T is too large, pages
may remain in main memory long after last being
used, and wasted main memory may result.
Thus the
value of T will have to represent a compromise between too much page traffic and too much wasted
memory space.
0

l

pointer
to page

use bits

luol-,l

---

TYPICAL PAGE TABLE ENTRY

SHIFT AT END OF SAMPLING INTERVAL
The following consideration leads us to recommend for T a value comparable to the memory traverse time T (Figure I). Consider a process that
is running continuously, being interrupted only
for page faults.
Assuming that memory allocation
procedures balk at removing from main memory any
page in a working set, once a page has entered
W(t,T) it will remain in main memory for at least
T seconds.
Under the very worst of page-shuffling
conditions, a page could be dispatched to auxiliary memory and be recalled immediately; the time
for this round trip is two traverse times, 2T.
Therefore a highly-shuffled page would spend
roughly T/2T of its time in main memory.
So,
for example, if we wished to insure that a page is
available in main memory (when needed) for not less
than 50 per cent of the time, we would have to
choose • ~ 2T.

FIGURE 4.

Page table entries for detecting W(t,Kcy).

As indicated by Figure 4, each page table entry
contains an "in-core" bit M, where M=i if and only
if the page is present in main memory.
It also
contains a string of use bits u0,ul,...,u K. Each
time a page reference occurs,

I ~ u 0.

At the end

of each sampling interval ~, the bit pattern contained in u0,ul,...,u K is shifted one position,
a 0 enters u 0, and u K is discarded:

UK_ 1

4

uK

u0

4

u1

0

4

u0

likely to change radically -- sometimes only A may
be in W(t,T), at other times one of the B-procedures
and D may be in W(t,.r). The pages of W(t,T) are
likely to be different after blocking for an inter-I
action from before blocking.
Thus, the fact that
a process blocks for an interaction (not page faults)
can be a strong indication of a change in W(t,T).
Therefore the look-ahead, most often used just
after a process unblocks, would probably load pages
not likely to be used.

(2)

Then the logical sum U of the use bits is computed:

Memory Allocation

Knowledge of only m(t,T) with demand paging
suffices to manage memory well.
Before running a
process we insure that there are enough pages of
memory free to contain its working set W(t,T),
whose pages fill free slots upon demand.
By
implication, enough free storage has been reserved
so that no page of a working set of another running process is displaced by a page of W(t,~)
(as can be the case with the Random, Cyclic, or
Oldest-unused paging policies).
Accordingly we
will use the working set size m(t,~) as a measure
of memory demand for storage allocation.

In our discussion so far we have seen two alternative quantities of possible use in storage allocation: the working set W(t,T) and the working set
size m(t,~).
We advocate use of m(t,T).

SOFTWARE IMPLEMENTATION

(3)

U

= u 0 + u I + ... + u K

so that U=I if and only if the page has been referenced during the last K sampling intervals;
of all the pages associated with a process, those
with U=I comprise its working set W(t,K~).
If
U=O when M=I the page is no longer in a working set
and may be removed from main memory.

The previous discussion has indicated a skeleton for implementing memory management using working sets.
No~ we will fill in the flesh.

Complete knowledge of W(t,~), page for page,
would be needed if look-ahead were contemplated.
We have already discussed why past paging policies
have shunned look-ahead, due to the strong possibility that pre-loading could be futile.
A program
organization likely to be typical of interactive,
modular programs, shown in Figure 5, fortifies our
previous argument against look-ahead.
The user
sends requests to the interface procedure A; having
interpreted the request, A calls on one of the
procedures Bi,...,B n to perform an operation on the

If the working set ideas are to contribute to
good service, an implementation should have these
properties:
i. Since there is such an intimate relation
between a process and its working set,
memory management and process scheduling
must be closely related activities. One cannot take place independently of the other.
2. Efficiency should be of prime importance.
When sampling of page tables is done, it

data D. The called B-procedure then returns to A
for the next user request.
Each time the process
of this program blocks, the working set W(t,T) is

BI

B2
USER

i;

/

A

B
n

FIGURE 5.

Organization

of a program.

3.

should be only on pages in currently changing working sets, and it should be done as
infrequently as possible.
The mechanism ought to be capable of providing measurements of current working set
Sizes and processor time consumptions for
each process.

Figure 6 displays an implementation having the
desired properties. Each solid box represents
a delay. The solid arrows indicate the paths that
may be followed by a process identifier while it
traverses the network of queues. The dashed boxes

and arrows show when operations are to be performed
on the time-used variable ti associated with process i; processor time used by process i since it
was last blocked (page faults excluded) is recorded
in t.. We shall follow a single process through
i
this system to see what transpires:
i. When process i is created, an identifier for
it is placed in the ready list, which is
a list of all processes in the ready state,
demanding service. Processes are selected
from the ready list to enter service according to the prevailing priority rule.

ti < qi

ti >- qi
[quantum runout ]

urst o v e r ]
[page fault ]

Run on a
processor
for burst O

I

[blocked]

0 4 ti |
i_+__

RUNNING
I )

LIST

acquire page I
I(T seconds)

!1111111,

Checker

I
BLOCKED
LIST

I

[unblocked]

L---J
[run process i]

READY
LIST

//
r_____--(
I assign quantum q |

L ......

r---~ ~---~ B
104ti
~
L.----~-J

FIGURE 6.

Implementation of Scheduling.

egin

2.

Once selected from the ready list, process
i is assigned a qunatum qi' which upperbounds its time in the running list. This
list is a cyclic queue; process i cycles
through repeatedly, receiving bursts ff of
processor time until it blocks or exceeds
its quantum qi" Note that the processor

3.

burst ff is also the sampling interval.
If process i blocks, its identifier is
placed in the blocked list, where it remains until the process unblocks; it is
then re-entered in the ready list.

Perennially present in the running list is a
special process, the checker.
The checker performs
core management functions.
It samples the page
tables of each process that has received service
since the last time it (the checker) was run, removing pages according to the algorithm discussed
at equations 2 and 3. It should be clear that if
the length of the running list is ~, sampling of
page tables occurs every 2K~ seconds, not every
seconds.
Associated with process i is a counter w. givl
ing the current size ~i(t,T) of its working set.

to run two processes together in time whenever they
are sharing information (symptomized by overlap
of their working sets) in order to avoid unnecessar
reloading of the same information.
How processes !
should be charged for memory usage when their working sets overlap is still an open question, and is
under investigation.

O

USE OF WORKING SETS IN RESOURCE ALLOCATION
In this section we want to indicate how information about working set size and processor time
consumption can be used by allocation procedures to
select the next job :from the ready list.
In Figure
6 we have tried to indicate that the interactions
between memory and processor requirements cannot be
ignored.
Here, we s]aall propose an allocation
policy that incorporates both process scheduling and
memory management into one function.
We begin by
defining carefully the notion "demand" and showing
how it can be measured dynamically by the operating
system.
From "demand" we proceed to "balance".
The
objective of the allocation policy will be to keep
the computer system ~'balanced".
We have chosen
this as an objective function primarily because of
its mathematical simplicity.

Each time a page fault occurs a new page enters
W.(t,T) and so w. must be increased by one.
Each
l

i

time a page is removed from Wi(t,T) by the checker,
w. is decreased by one.
1

Having completed its management duties, the
checker replenishes vacancies in the running list
by selecting jobs from the ready list according to
the prevailing priority rule. More will be said
about this shortly.

Demand
Our purpose here is to define "memory demand"
and "processor demand", then combine these into
the single notion "demand".
We define the memory demand m. of process i to be
i

(4)

SHARING
Sharing finds its place naturally.
When pages are shared, working sets will over7
lap.
If Arden's
suggestion concerning program
structure
is followed, sharing of data can be accomplished without modification of the regime of
Figure 6. If a page is in at least one working set,
the "use bits" in the page table entry will be
turned on and the page will not be removed.
To
prevent anomalies, the checker must not be permitted
to examine the same page table more than once during
one of its samples.
Allocation policies should tend

If a segment is shared, there will be an entry for
it in the segment tables of each participating process: however, each entry points to the same page
table.
Each physical segment has exactly one page
table describing it, but a name for the segment may
appear in many segment tables.

m.

=

min i. ~ i

I

)

0 < m. < I

where M is the number of pages of main memory, and
w. = ~ (t,y) is the 'working set count, such as
1

l

maintained by the strategy of Figure 6. If a working set contains more than M pages (it is bigger
than main memory) we regard its demand to be m=l.
Presumably M is large enough so that the probability
(over the ensemble of all processes) Pr[m=l] is
very small.
"Processor demand" is difficult to define without some discussion.
Just as memory demand is in
some sense a prediction of memory requirements in
the immediate future, so too processor demand should
be a prediction of processor requirements for the
near future.
There are many ways in which processor
demand could be defi'ned. The method we have chosen,
described below, defines the processor demand of a
process to be its expected fractional processor requirement before the next time it blocks (exclusive
of page faults).

Let q be the random variable of processor time
used by a process between interactions. [A process
"interacts" when it communicates with something
outside its name space, e.g., with a user, or with
another process.] In general character, f (x),
q
the probability density function for q, is hyper-

The conditional expectation function Q(y) is shown
in Figure 8.
Q(Y)

Q(~)

exponential (for a complete discussion, see FifelS):

(5)

f (x)
q

=

c a e

-ax

+ (l-c) b e

-bx

O<a~b

O<c~l

f (x) is diagrammed in Figure 7; most of the probq
ability is concentrated toward small q (i.e.,
frequently interacting processes), but f (x) has
q
a long exponential tail.

Q(0)

f (x)

0

q

FIGURE 8.

Conditional expectation function for q.

It starts at Q(0)

l
I

0

~-

x

¥

FIGURE 7.

Probability density function for q.

= ~

a

+

l-c

-~-

and rises toward a

1
constant maximum of Q(~) = -a
Note that, for
large enough y, the conditional expectation becomes
independent of y. The conditional expectation Q(y)
is a useful prediction function -- if y seconds of
processor time have been consumed by a process since
its last interaction, we may expect Q(y) seconds of
process time to elapse before its next interaction.
It should be clear that the conditional expectation
function Q(y) can be determined and updated automatically by the operating system.
We define the ~rocessor demand Pi of process i
to be

Given that it has been y seconds (process time)
since the last interaction, the conditional density function for time until next interaction is

Q(ti)
(8)

Pi

=

N Q(~)

Q(O)
'

i

N Q(~) ~ Pi ~

fq(X+y)

fqly(x)

=

(6)

~y fq(Z) dz

where N is the number of processors and t.i is the
time-used quantity for process i, such as maintained
by the strategy of Figure 6.
The demand ~i of process i is a pair

c a e -a(x+¥) + (l-c) b e -b(x+Y)
-ay
c e
+ (l-c) e -bY
(9)
which is just that portion of f (x) for q ~ y
q
with its area normalized to unity. The conditional
expectation of q, given y, is:

Q(y)
(7)

=

D.=l =

(Pi'mi)

where Pi is its processor demand (equation 8) and
m. is its memory demand (equation 4). That the proi
cessor demand is Pi tells us to expect process i

~ x fq[y(X) dx

to use Pi of the processors for the next Q(~) sec-

o

onds, before its next interaction .

e-aY + I - ~
e-bY
b
-ay
c e
+ (l-c) e -bY
a

That the

A reasonable choice for the quantum qi (Figure 6)
granted to process i might be qi = k Q(ti) for some
suitable constant k > I.

memory demand is m i tells us process i is most
probably going to use (miM) pages of memory during
the next few time units.

Balance
The computer system is said to be balanced
if simultaneously
(I0)

~

p

=

a

0 < ~ K i

=

~

0 <

processes in
running list

v~
(n)

m

~

i

processes in
running list
where p is a processor demand, m a memory demand,
and ~,~ are constants chosen to cause any desired
fraction of resource to constitute balance.
If
the system is balanced, the total demand presented
by the running list processes just consumes the
available fractions of processor and memory resources.
We can write equations I0 and II in the
more compact form
(12)

S

r~
)

=

D

=

(~°~)

D = (p,m)

processes in
running list
so that balance exists whenever equation 12 holds,
that is, whenever
S = (~,~).
Dynamic maintenance of S = ~ D is straightforward.
Whenever a process=of demand (p,m) is
admitted to the running list, S + (p,m) 4 S .
Whenever a process of demand (p?m) exits th~ running
list (except for page faults),
S - (p,m) 4 S .
Therefore ~ always measures the current tota~ running list demand.

Balance Policies
A "balance policy" is a resource allocation
policy whose objective is to keep the computer
system in balance.
Expressed as a minimization
problem:
(13)

{minimize

I~ - (~'~)I

]

Instead of a priority in the ready list, a process
has associated its demand ~. In the event of imbalance, the next job (or set of jobs) to leave the
ready list should be that whose demand comes closest
to restoring balance. [Means of formulating this
type of policy are currently under investigation
as part of doctoral research into the whole problem of resource allocation.]

We do not wish to venture further here into
the alluring problems of allocation policies; our
aim is primarily to stimulate new thinking by presenting seeds of ideas.
There are, however, two
points we want to stress about policy (13):
i. The criterion is basically an equipment
utilization criterion.
It is well known
that equipment utilization and good response to users are not mutually-aiding
criteria.
As it stands, policy (13) will
favor jobs of small demand, discriminate
against jobs of large demand; but with
modifications and the methods suggested by
Figure 6, it is possible to maintain
"almost-balance" along with good service
to users.
2. Even with intuitively simple strategies
such as balance, the allocation problem is
far from trivial -- interactions such as
those between process and working set, and
between balance and good service, are not
yet fully understood.

CONCLUS I0N
In a synopsis of previous work on core memory
management culled from varied sources we saw that
memory management operates in two basic stages:
page-in and page-out.
Page-in should be done on
demand, without look-ahead; page-out can be done
in a variety of ways.
Page-out is the heart of the
problem, for if pages least likely to be reused in
the near future are removed from main memory, the
traffic of returning pages is minimized.
The
working-set strategy, which attempts to have present in main memory every page "needed" by each
running process, which balks at the idea of displacing any page in a working set from main memory,
is offered as a viable solution to the memory
management problem.
Choice of the working set
parameter T will depend on compromises among page
traffic, wasted memory space, and required availability of pages,
the working set size, a convenient quantity to measure, provides a measure of
memory demand.
Processor demand can be defined to
be a prediction of processor time required before
an interaction.
A balance policy strives to balance demands against: equipment by judiciously
selecting jobs to rim.
The notions "demand" and
"balance" can play important roles in understanding
the complex interactions among computer system components.
Looking at this paper from a slightly different
point of view, we have seen four contenders for
paging policies: Random, Cyclic, 01dest-unused, and
Working-set.
For modular programs, the type ultimately expected to predominate in a multiprogrammed
environment, Random brings on the highest page traffic, Working-set the lowest.
Although Random and
Cyclic are inexpensively implemented, the added co
of Working-set is more than offset by its accuracy
and compatibility with generalized allocation
functions.

14. J.B.Dennis.
"Program Structure in a MultiAccess Computer." M.I.T. Project MAC technical report MAC-TR-iI.

ACKNOWLEDGEMENT

15. D.W.Fife. "An Optimization Model for TimeSharing." AFIPS Conf. Proc. 28 (April 1966),
97-104.

The author wishes to thank Jack B. Dennis for
many helpful criticisms.

REFERENCES
i.

C.V.Ramamoorthy.
"The Analytic Design of a
Dynamic Look Ahead and Program Segmenting
System for Multiprogrammed Computers." Proc.
21 Nat'l Conf. ACM (1966).

2.

R.M.Fano and E.E.David.
"On the Social Implications of Accessible Computing." AFIPS
Conf. Proc. 27 (Nov 1965), 243-247.

3.

L.L. Selwyn. "The Information Utility."
Industrial Management Review ~, 2, Spring 1966.

.

D.Parkhill. The Challenge of the Computer
Utility. Addison-Wesley, 1966.

5.

J.B.Dennis.
"Segmentation and the Design of
Multiprogrammed Computer Systems." JACM 1__2,4
(Oct 1965), 589-602.

6.

J.B.Dennis and E.C.Van Horn. "Programming
Semantics for Multiprogrammed Computations."
Comm ACM 9 (March 1966), 143-155.

7.

B.W.Arden, et al. "Program and Address Structure
in a Time-Sharing Environment." JACM 13, I
(Jan 1966), 1-16.

8.

J.H. Saltzer. "Traffic Control in a Multiplexed
Computer System." M.I.T. Project MAC technical
report MAC-TR-30, July 1966.

9.

G.H.Fine, P.V.Mclssac, C.W.Jackson.
"Dynamic
Program Behavior under Paging." Proc. 21
Nat'l Conf. ACM (1966).

I0.

T.Kilburn, et al. "One-level Storage System."
IRE Trans. on Elec. Comp. EC-II, 2 (April 1962).

II.

L.A.Belady.
"A Study of Replacement Algorithms
for a Virtual Storage Computer." IB___MSystems
Journal ~, 2 (1966), 78-101.

12.

Progress Report III, M.I.T. Project MAC
(1965-1966), 63-66.

13.

P.J.Denning. "Memory Allocation in Multiprogrammed Computers." M.I.T. Project MAC
Computation Structures Group Memo 24, March 1966.

A=P=P=E=N=D=I=X=- -

HARDWARE
IMPLEMENTATION OF MEMORY
MANAGEMENT

Just as hardware is used to streamline the
address-mapping mechanism, so too hardware can be
used to streamline memory management.
The hardware described here associates a timer with each
physical page of main memory a timer to measure
multiples of the working set parameter 7.
Each process, upon creation, is assigned an
identification number, i, which is used to index
the process table. The i-th entry in the process
table contains information about the i-th process,
including its current demand (Pi,mi). Because
this demand information is stored in a common place,
the memory hardware can update the memory demand
m. without calling the supervosor. Whenever a page
1

fault occurs, the new page is located in auxiliary
memory and transferred to main memory; then a signal is sent to the management hardware to free a
page of main memory in readiness for the next page
fault. The hardware selects a page not in any
working set and dispatches it directly to auxiliary
memory, without bothering the supervisor.
This
hardware modifies the page table entry pointing to
the newly deleted page, turning the "in-core" bit
OFF and leaving a pointer to help locate the page
in auxiliary memory.
Figure A indicates that with each page of memory
there is associated a pane re~ister, having three
fields:
I. c-field. ~ is a pointer to the memory location of the page table entry pointing to
this page. A page table cannot be moved or
removed without modifying ~.
2. t-field, t is a timer to measure off the
interval 7. The value of 7 to be used is
found in the t-register.
The supervisor
modifies the contents of the t-reglster
as discussed below.
3. A-field. A is an "alarm" bit, set to i if
the timer t runs out.

Operation proceeds as follows:
I. When a page is loaded into main memory,
is set to point to the correct page table
entry.
The "in-core" bit of that entry
is turned ON.
2. Each time a reference to some location
within a page occurs, its page register is
modified: T 4 t and 0 ~ A.
The timer
t begins to run down (in real time), taking
T seconds to do so.
3. If t runs down, 1 4 A. Whenever a fresh
page of memory is needed, the supervisor
sends a signal to additional memory hardware (not shown) which scans pages looking
for a page w i t h A = l .
Such a page is
dispatched directly to auxiliary memory.
is used to find the page table entry,
turn the "in-core" bit OFF, and leave
information there to permit future retrieval
of the page from auxiliary memory.
Note
that a page need not be removed when A=I;
it is only sub iect to removal.
This means
a page may leave and later re-enter a working set without actually leaving main memory.

The timers t are running down in real time.
The value in the t-register must be modifiable by
the supervisor for t]he following reason.
As in
Figure 6, the running list is cyclic, except now
suppose that each process is given a burst ~ of
processor time (~ need not be related to the sampiing interval ~), and continues to receive bursts
until its running-list quantum is exhausted.
If,
on a particular cycle there are n entries in the
running list and N processors in service, a given
process will be unable to reference any of its pages
for about n~/N seconds, the time to complete a cycle
through the queue.
So the supervisor should be
able to set T to some multiple of n~/N, for otherwise management hardware will begin removing pages
of working sets of running processes.
However T
should never be less than some multiple of the
traverse time T (Figure I) for otherwise when a
process interrupts for a page fault its working set
may disappear from core memery.

t-register

I L

11

I 1

11
I

I I

t

ii

PAGES
page table
entry pointer
TYPICAL PAGE REGISTER

I I

il

PAGE REGISTERS

MAIN MEMORY
FIGURE A.

Memory Management Hardware.

alarm


