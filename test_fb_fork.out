Pager started with 4 physical memory pages
vm_create	(107069, 632842)
vm_create returned 0
vm_switch	(632842)
					returning to (632842) with r|w pages:
vm_create	(632842, 632843)
vm_create returned 0
					returning to (632842) with r|w pages:
vm_map		(0x0, 0)
vm_map returned 0x600000000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x0
vm_map		(0x0, 0)
vm_map returned 0x600010000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x0
					r	vpage 0x60001	ppage 0x0
vm_map		(0x0, 0)
vm_map returned 0x600020000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x0
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
vm_fault	(0x600000000, write)
core map: 
[0 -> [0x6322a15a63a0, 0x6322a15a63a4, 0x6322a15a63a8]]
cow

clock_q: 
(1, 2, 3)
a free page!: 
clock_q: 
(2, 3, 1)
alloc: 1
core map: 
[1 -> [0x6322a15a63a0]]
[0 -> [0x6322a15a63a4, 0x6322a15a63a8]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
vm_fault	(0x600010000, write)
core map: 
[1 -> [0x6322a15a63a0]]
[0 -> [0x6322a15a63a4, 0x6322a15a63a8]]
cow

clock_q: 
(2, 3, 1)
a free page!: 
clock_q: 
(3, 1, 2)
alloc: 2
core map: 
[2 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63a0]]
[0 -> [0x6322a15a63a8]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60000	ppage 0x1
					rw	vpage 0x60001	ppage 0x2
					r	vpage 0x60002	ppage 0x0
vm_fault	(0x600020000, write)
core map: 
[2 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63a0]]
[0 -> [0x6322a15a63a8]]
cow

clock_q: 
(3, 1, 2)
a free page!: 
clock_q: 
(1, 2, 3)
alloc: 3
core map: 
[3 -> [0x6322a15a63a8]]
[2 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63a0]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60000	ppage 0x1
					rw	vpage 0x60001	ppage 0x2
					rw	vpage 0x60002	ppage 0x3
vm_create	(632842, 632844)
vm_create returned 0
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					r	vpage 0x60002	ppage 0x3
vm_map		(0x600000000, 0)
file_str = papers.txt
vm_map returned 0x600030000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					r	vpage 0x60002	ppage 0x3
vm_map		(0x600010000, 0)
file_str = papers.txt
matched! 
vm_map returned 0x600040000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					r	vpage 0x60002	ppage 0x3
vm_map		(0x600010000, 0)
file_str = papers.txt
matched! 
vm_map returned 0x600050000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					r	vpage 0x60002	ppage 0x3
vm_map		(0x600010000, 1)
file_str = papers.txt
vm_map returned 0x600060000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					r	vpage 0x60002	ppage 0x3
vm_map		(0x600010000, 2)
file_str = papers.txt
vm_map returned 0x600070000
					returning to (632842) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					r	vpage 0x60002	ppage 0x3
vm_fault	(0x600030000, read)
core map: 
[3 -> [0x6322a15a63a8, 0x6322a15a7478]]
[2 -> [0x6322a15a63a4, 0x6322a15a7474]]
[1 -> [0x6322a15a63a0, 0x6322a15a7470]]
clock_q: 
(1, 2, 3)
pm_evict
core: 
ppage (evict): 1
[3 -> [0x6322a15a63a8, 0x6322a15a7478]]
[2 -> [0x6322a15a63a4, 0x6322a15a7474]]
[1 -> [0x6322a15a63a0, 0x6322a15a7470]]
ppage = 1
pte = 0x6322a15a63a0
file_write	(<swap>, 0)
filemap:
[papers.txt -> [(0 -> (0, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4])), (1 -> (0, [0x6322a15a63b8])), (2 -> (0, [0x6322a15a63bc]))]]
clock_q: 
(2, 3, 1)
alloc: 1
epage: 1
file_read	(papers.txt, 0)
filemap: 
[papers.txt -> [(0 -> (0, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4])), (1 -> (0, [0x6322a15a63b8])), (2 -> (0, [0x6322a15a63bc]))]]
core map: 
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[3 -> [0x6322a15a63a8, 0x6322a15a7478]]
[2 -> [0x6322a15a63a4, 0x6322a15a7474]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					r	vpage 0x60003	ppage 0x1
					r	vpage 0x60004	ppage 0x1
					r	vpage 0x60005	ppage 0x1
vm_fault	(0x600030000, write)
core map: 
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[3 -> [0x6322a15a63a8, 0x6322a15a7478]]
[2 -> [0x6322a15a63a4, 0x6322a15a7474]]
core map: 
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[3 -> [0x6322a15a63a8, 0x6322a15a7478]]
[2 -> [0x6322a15a63a4, 0x6322a15a7474]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60003	ppage 0x1
					rw	vpage 0x60004	ppage 0x1
					rw	vpage 0x60005	ppage 0x1
vm_fault	(0x600010000, write)
core map: 
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[3 -> [0x6322a15a63a8, 0x6322a15a7478]]
[2 -> [0x6322a15a63a4, 0x6322a15a7474]]
cow

clock_q: 
(2, 3, 1)
pm_evict
core: 
ppage (evict): 3
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[3 -> [0x6322a15a63a8, 0x6322a15a7478]]
[2 -> [0x6322a15a63a4, 0x6322a15a7474]]
ppage = 3
pte = 0x6322a15a63a8
file_write	(<swap>, 2)
filemap:
[papers.txt -> [(0 -> (1, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4])), (1 -> (0, [0x6322a15a63b8])), (2 -> (0, [0x6322a15a63bc]))]]
clock_q: 
(1, 2, 3)
alloc: 3
core map: 
[3 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[2 -> [0x6322a15a7474]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60001	ppage 0x3
					rw	vpage 0x60003	ppage 0x1
					rw	vpage 0x60004	ppage 0x1
					rw	vpage 0x60005	ppage 0x1
vm_fault	(0x600020000, write)
core map: 
[3 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[2 -> [0x6322a15a7474]]
clock_q: 
(1, 2, 3)
pm_evict
core: 
ppage (evict): 2
[3 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
[2 -> [0x6322a15a7474]]
ppage = 2
pte = 0x6322a15a7474
file_write	(<swap>, 4)
filemap:
[papers.txt -> [(0 -> (1, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4])), (1 -> (0, [0x6322a15a63b8])), (2 -> (0, [0x6322a15a63bc]))]]
clock_q: 
(3, 1, 2)
alloc: 2
epage: 2
file_read	(<swap>, 2)
filemap: 
[papers.txt -> [(0 -> (1, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4])), (1 -> (0, [0x6322a15a63b8])), (2 -> (0, [0x6322a15a63bc]))]]
core map: 
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60001	ppage 0x3
					rw	vpage 0x60002	ppage 0x2
vm_map		(0x600010000, 1)
file_str = papers.txt
matched! 
vm_map returned 0x600080000
					returning to (632842) with r|w pages:
					rw	vpage 0x60001	ppage 0x3
					rw	vpage 0x60002	ppage 0x2
vm_fault	(0x600070000, read)
core map: 
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
clock_q: 
(3, 1, 2)
pm_evict
core: 
ppage (evict): 1
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
[1 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4]]
ppage = 1
pte = 0x6322a15a63ac
file_write	(papers.txt, 0)
filemap:
[papers.txt -> [(0 -> (1, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4])), (1 -> (0, [0x6322a15a63b8, 0x6322a15a63c0])), (2 -> (0, [0x6322a15a63bc]))]]
clock_q: 
(2, 3, 1)
alloc: 1
epage: 1
file_read	(papers.txt, 2)
filemap: 
[papers.txt -> [(0 -> (1, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4])), (1 -> (0, [0x6322a15a63b8, 0x6322a15a63c0])), (2 -> (0, [0x6322a15a63bc]))]]
core map: 
[1 -> [0x6322a15a63bc]]
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60002	ppage 0x2
					r	vpage 0x60007	ppage 0x1
vm_map		(0x600010000, 0)
core map: 
[1 -> [0x6322a15a63bc]]
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
core map: 
[1 -> [0x6322a15a63bc]]
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
file_str = papers.txt
matched! 
vm_map returned 0x600090000
					returning to (632842) with r|w pages:
					rw	vpage 0x60001	ppage 0x3
					rw	vpage 0x60002	ppage 0x2
					r	vpage 0x60007	ppage 0x1
vm_fault	(0x600090000, read)
core map: 
[1 -> [0x6322a15a63bc]]
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
clock_q: 
(2, 3, 1)
pm_evict
core: 
ppage (evict): 2
[1 -> [0x6322a15a63bc]]
[2 -> [0x6322a15a63a8]]
[3 -> [0x6322a15a63a4]]
ppage = 2
pte = 0x6322a15a63a8
file_write	(<swap>, 2)
filemap:
[papers.txt -> [(0 -> (1, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4])), (1 -> (0, [0x6322a15a63b8, 0x6322a15a63c0])), (2 -> (1, [0x6322a15a63bc]))]]
clock_q: 
(3, 1, 2)
alloc: 2
epage: 2
file_read	(papers.txt, 0)
filemap: 
[papers.txt -> [(0 -> (1, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4])), (1 -> (0, [0x6322a15a63b8, 0x6322a15a63c0])), (2 -> (1, [0x6322a15a63bc]))]]
core map: 
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4]]
[1 -> [0x6322a15a63bc]]
[3 -> [0x6322a15a63a4]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					r	vpage 0x60003	ppage 0x2
					r	vpage 0x60004	ppage 0x2
					r	vpage 0x60005	ppage 0x2
					r	vpage 0x60009	ppage 0x2
vm_fault	(0x600090000, write)
core map: 
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4]]
[1 -> [0x6322a15a63bc]]
[3 -> [0x6322a15a63a4]]
core map: 
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4]]
[1 -> [0x6322a15a63bc]]
[3 -> [0x6322a15a63a4]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60003	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60009	ppage 0x2
vm_map		(0x600010000, 0)
core map: 
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4]]
[1 -> [0x6322a15a63bc]]
[3 -> [0x6322a15a63a4]]
core map: 
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4]]
[1 -> [0x6322a15a63bc]]
[3 -> [0x6322a15a63a4]]
file_str = papers.txt
matched! 
vm_map returned 0x6000a0000
					returning to (632842) with r|w pages:
					rw	vpage 0x60001	ppage 0x3
					rw	vpage 0x60003	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60009	ppage 0x2
					rw	vpage 0x6000a	ppage 0x2
vm_fault	(0x600080000, read)
core map: 
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8]]
[1 -> [0x6322a15a63bc]]
[3 -> [0x6322a15a63a4]]
clock_q: 
(3, 1, 2)
pm_evict
core: 
ppage (evict): 1
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8]]
[1 -> [0x6322a15a63bc]]
[3 -> [0x6322a15a63a4]]
ppage = 1
pte = 0x6322a15a63bc
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8])), (1 -> (0, [0x6322a15a63b8, 0x6322a15a63c0])), (2 -> (1, [0x6322a15a63bc]))]]
clock_q: 
(2, 3, 1)
alloc: 1
epage: 1
file_read	(papers.txt, 1)
filemap: 
[papers.txt -> [(0 -> (2, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8])), (1 -> (0, [0x6322a15a63b8, 0x6322a15a63c0])), (2 -> (1, [0x6322a15a63bc]))]]
core map: 
[1 -> [0x6322a15a63b8, 0x6322a15a63c0]]
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8]]
[3 -> [0x6322a15a63a4]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60003	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					r	vpage 0x60006	ppage 0x1
					r	vpage 0x60008	ppage 0x1
					rw	vpage 0x60009	ppage 0x2
					rw	vpage 0x6000a	ppage 0x2
vm_fault	(0x600080000, write)
core map: 
[1 -> [0x6322a15a63b8, 0x6322a15a63c0]]
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8]]
[3 -> [0x6322a15a63a4]]
core map: 
[1 -> [0x6322a15a63b8, 0x6322a15a63c0]]
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8]]
[3 -> [0x6322a15a63a4]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					rw	vpage 0x60003	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x1
					rw	vpage 0x60008	ppage 0x1
					rw	vpage 0x60009	ppage 0x2
					rw	vpage 0x6000a	ppage 0x2
vm_create	(632842, 632845)
vm_create returned 0
					returning to (632842) with r|w pages:
					r	vpage 0x60003	ppage 0x2
					r	vpage 0x60004	ppage 0x2
					r	vpage 0x60005	ppage 0x2
					r	vpage 0x60006	ppage 0x1
					r	vpage 0x60008	ppage 0x1
					r	vpage 0x60009	ppage 0x2
					r	vpage 0x6000a	ppage 0x2
vm_fault	(0x600070000, read)
core map: 
[1 -> [0x6322a15a63b8, 0x6322a15a63c0, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8, 0x6322a15a84e8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508]]
[3 -> [0x6322a15a63a4, 0x6322a15a84e4]]
clock_q: 
(2, 3, 1)
pm_evict
core: 
ppage (evict): 3
[1 -> [0x6322a15a63b8, 0x6322a15a63c0, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8, 0x6322a15a84e8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508]]
[3 -> [0x6322a15a63a4, 0x6322a15a84e4]]
ppage = 3
pte = 0x6322a15a63a4
file_write	(<swap>, 1)
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63b8, 0x6322a15a63c0, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (1, [0x6322a15a63bc, 0x6322a15a84fc]))]]
clock_q: 
(1, 2, 3)
alloc: 3
epage: 3
file_read	(papers.txt, 2)
filemap: 
[papers.txt -> [(0 -> (2, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63b8, 0x6322a15a63c0, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (1, [0x6322a15a63bc, 0x6322a15a84fc]))]]
core map: 
[3 -> [0x6322a15a63bc, 0x6322a15a84fc]]
[1 -> [0x6322a15a63b8, 0x6322a15a63c0, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8, 0x6322a15a84e8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508]]
vm_fault returned 0
					returning to (632842) with r|w pages:
					r	vpage 0x60006	ppage 0x1
					r	vpage 0x60007	ppage 0x3
					r	vpage 0x60008	ppage 0x1
vm_destroy
infile size: 19
[0x6322a15a8508 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a8504 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a8500 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a84fc -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a84f8 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a84f4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f0 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84ec -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84e8 -> ftype: SWAP
infile: true
block: 8
filename: @SWAP
]
[0x6322a15a84e4 -> ftype: SWAP
infile: false
block: 7
filename: @SWAP
]
[0x6322a15a84e0 -> ftype: SWAP
infile: true
block: 6
filename: @SWAP
]
[0x6322a15a63c8 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a63a0 -> ftype: SWAP
infile: true
block: 0
filename: @SWAP
]
[0x6322a15a63a4 -> ftype: SWAP
infile: true
block: 1
filename: @SWAP
]
[0x6322a15a63a8 -> ftype: SWAP
infile: true
block: 2
filename: @SWAP
]
[0x6322a15a7470 -> ftype: SWAP
infile: false
block: 3
filename: @SWAP
]
[0x6322a15a63ac -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a7474 -> ftype: SWAP
infile: true
block: 4
filename: @SWAP
]
[0x6322a15a63b0 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a7478 -> ftype: SWAP
infile: false
block: 5
filename: @SWAP
]
[0x6322a15a63b4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a63b8 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a63bc -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a63c0 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a63c4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
filemap: 
[papers.txt -> [(0 -> (2, [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63b8, 0x6322a15a63c0, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a63bc, 0x6322a15a84fc]))]]
core: 
[3 -> [0x6322a15a63bc, 0x6322a15a84fc]]
[1 -> [0x6322a15a63b8, 0x6322a15a63c0, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
[2 -> [0x6322a15a63ac, 0x6322a15a63b0, 0x6322a15a63b4, 0x6322a15a63c4, 0x6322a15a63c8, 0x6322a15a84e8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508]]
swfile: 
[8 -> [0x6322a15a84e8]]
[7 -> [0x6322a15a84e4]]
[6 -> [0x6322a15a84e0]]
[5 -> [0x6322a15a7478]]
[4 -> [0x6322a15a7474]]
[3 -> [0x6322a15a7470]]
[2 -> [0x6322a15a63a8]]
[1 -> [0x6322a15a63a4]]
[0 -> [0x6322a15a63a0]]
core after discard: 
[3 -> [0x6322a15a84fc]]
[1 -> [0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
[2 -> [0x6322a15a84e8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508]]
filemap: 
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a84fc]))]]
ghost: 
free_block: 
(9, a, b, c, d, e, f, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1a, 1b, 1c, 1d, 1e, 1f, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 2a, 2b, 2c, 2d, 2e, 2f, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 3a, 3b, 3c, 3d, 3e, 3f, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 4a, 4b, 4c, 4d, 4e, 4f, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 5a, 5b, 5c, 5d, 5e, 5f, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 6a, 6b, 6c, 6d, 6e, 6f, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 7a, 7b, 7c, 7d, 7e, 7f, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 8a, 8b, 8c, 8d, 8e, 8f, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 9a, 9b, 9c, 9d, 9e, 9f, a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, aa, ab, ac, ad, ae, af, b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, ca, cb, cc, cd, ce, cf, d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, da, db, dc, dd, de, df, e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, ea, eb, ec, ed, ee, ef, f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, fa, fb, fc, fd, fe, ff, 0, 1, 2)
eblcnt: f4
vm_switch	(632843)
					returning to (632843) with r|w pages:
vm_map		(0x0, 0)
vm_map returned 0x600000000
					returning to (632843) with r|w pages:
					r	vpage 0x60000	ppage 0x0
vm_map		(0x0, 0)
vm_map returned 0x600010000
					returning to (632843) with r|w pages:
					r	vpage 0x60000	ppage 0x0
					r	vpage 0x60001	ppage 0x0
vm_map		(0x0, 0)
vm_map returned 0x600020000
					returning to (632843) with r|w pages:
					r	vpage 0x60000	ppage 0x0
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
vm_fault	(0x600000000, write)
core map: 
[0 -> [0x6322a15a6820, 0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a84fc]]
[1 -> [0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
[2 -> [0x6322a15a84e8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508]]
cow

clock_q: 
(1, 2, 3)
pm_evict
core: 
ppage (evict): 2
[0 -> [0x6322a15a6820, 0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a84fc]]
[1 -> [0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
[2 -> [0x6322a15a84e8, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508]]
ppage = 2
pte = 0x6322a15a84e8
file_write	(<swap>, 8)
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a84fc]))]]
clock_q: 
(3, 1, 2)
alloc: 2
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a84fc]]
[1 -> [0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
vm_map		(0x600000000, 1)
file_str = papers.txt
matched! 
vm_map returned 0x600030000
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
vm_fault	(0x600030000, read)
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
					rw	vpage 0x60003	ppage 0x1
vm_map		(0x600000000, 2)
file_str = papers.txt
matched! 
vm_map returned 0x600040000
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
					rw	vpage 0x60003	ppage 0x1
vm_fault	(0x600040000, read)
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a6830, 0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a6830, 0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
					rw	vpage 0x60003	ppage 0x1
					r	vpage 0x60004	ppage 0x3
vm_fault	(0x600040000, write)
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a6830, 0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a6830, 0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x0
					r	vpage 0x60002	ppage 0x0
					rw	vpage 0x60003	ppage 0x1
					rw	vpage 0x60004	ppage 0x3
vm_fault	(0x600010000, write)
core map: 
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a6830, 0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
cow

clock_q: 
(3, 1, 2)
pm_evict
core: 
ppage (evict): 3
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6824, 0x6322a15a6828]]
[3 -> [0x6322a15a6830, 0x6322a15a84fc]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
ppage = 3
pte = 0x6322a15a6830
file_write	(papers.txt, 2)
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a682c, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a6830, 0x6322a15a84fc]))]]
clock_q: 
(1, 2, 3)
alloc: 3
core map: 
[3 -> [0x6322a15a6824]]
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6828]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x0
vm_fault	(0x600020000, write)
core map: 
[3 -> [0x6322a15a6824]]
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6828]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
cow

clock_q: 
(1, 2, 3)
pm_evict
core: 
ppage (evict): 1
[3 -> [0x6322a15a6824]]
[2 -> [0x6322a15a6820]]
[0 -> [0x6322a15a6828]]
[1 -> [0x6322a15a682c, 0x6322a15a84e0, 0x6322a15a84f8, 0x6322a15a84fc, 0x6322a15a8500]]
ppage = 1
pte = 0x6322a15a682c
file_write	(papers.txt, 1)
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a682c, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a6830, 0x6322a15a84fc]))]]
clock_q: 
(2, 3, 1)
alloc: 1
core map: 
[1 -> [0x6322a15a6828]]
[3 -> [0x6322a15a6824]]
[2 -> [0x6322a15a6820]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x3
					rw	vpage 0x60002	ppage 0x1
vm_create	(632843, 632849)
vm_create returned 0
					returning to (632843) with r|w pages:
					r	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x1
vm_map		(0x600000000, 0)
core map: 
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0, 0x6322a15a6824]]
[2 -> [0x6322a15a63a0, 0x6322a15a6820]]
core map: 
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0, 0x6322a15a6824]]
[2 -> [0x6322a15a63a0, 0x6322a15a6820]]
file_str = papers.txt
matched! 
vm_map returned 0x600050000
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x1
vm_map		(0x600010000, 0)
file_str = papers.txt
matched! 
vm_map returned 0x600060000
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x1
vm_map		(0x600010000, 0)
file_str = papers.txt
matched! 
vm_map returned 0x600070000
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x1
vm_map		(0x600010000, 1)
file_str = papers.txt
matched! 
vm_map returned 0x600080000
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x1
vm_map		(0x600010000, 2)
file_str = papers.txt
matched! 
vm_map returned 0x600090000
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x1
vm_fault	(0x600050000, read)
core map: 
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0, 0x6322a15a6824]]
[2 -> [0x6322a15a63a0, 0x6322a15a6820, 0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c]]
core map: 
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0, 0x6322a15a6824]]
[2 -> [0x6322a15a63a0, 0x6322a15a6820, 0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60000	ppage 0x2
					r	vpage 0x60001	ppage 0x3
					r	vpage 0x60002	ppage 0x1
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x2
					rw	vpage 0x60007	ppage 0x2
vm_fault	(0x600010000, write)
core map: 
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0, 0x6322a15a6824]]
[2 -> [0x6322a15a63a0, 0x6322a15a6820, 0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c]]
cow

clock_q: 
(2, 3, 1)
pm_evict
core: 
ppage (evict): 2
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0, 0x6322a15a6824]]
[2 -> [0x6322a15a63a0, 0x6322a15a6820, 0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c]]
ppage = 2
pte = 0x6322a15a63a0
file_write	(<swap>, c)
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]))]]
clock_q: 
(3, 1, 2)
alloc: 2
core map: 
[2 -> [0x6322a15a6824]]
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x2
vm_fault	(0x600020000, write)
core map: 
[2 -> [0x6322a15a6824]]
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0]]
cow

clock_q: 
(3, 1, 2)
pm_evict
core: 
ppage (evict): 3
[2 -> [0x6322a15a6824]]
[1 -> [0x6322a15a63a8, 0x6322a15a63ac, 0x6322a15a6828]]
[3 -> [0x6322a15a63a4, 0x6322a15a63b0]]
ppage = 3
pte = 0x6322a15a63a4
file_write	(<swap>, d)
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]))]]
clock_q: 
(1, 2, 3)
alloc: 3
core map: 
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824]]
[1 -> [0x6322a15a63a8, 0x6322a15a63ac]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x2
					rw	vpage 0x60002	ppage 0x3
vm_map		(0x600010000, 1)
file_str = papers.txt
matched! 
vm_map returned 0x6000a0000
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x2
					rw	vpage 0x60002	ppage 0x3
vm_fault	(0x600090000, read)
core map: 
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824]]
[1 -> [0x6322a15a63a8, 0x6322a15a63ac]]
clock_q: 
(1, 2, 3)
pm_evict
core: 
ppage (evict): 1
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824]]
[1 -> [0x6322a15a63a8, 0x6322a15a63ac]]
ppage = 1
pte = 0x6322a15a63a8
file_write	(<swap>, e)
filemap:
[papers.txt -> [(0 -> (2, [0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]))]]
clock_q: 
(2, 3, 1)
alloc: 1
epage: 1
file_read	(papers.txt, 2)
filemap: 
[papers.txt -> [(0 -> (2, [0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (3, [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]))]]
core map: 
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					r	vpage 0x60004	ppage 0x1
					r	vpage 0x60009	ppage 0x1
vm_map		(0x600010000, 0)
core map: 
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824]]
core map: 
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824]]
file_str = papers.txt
matched! 
vm_map returned 0x6000b0000
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x2
					r	vpage 0x60004	ppage 0x1
					r	vpage 0x60009	ppage 0x1
vm_fault	(0x6000b0000, read)
core map: 
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824, 0x6322a15a684c]]
core map: 
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824, 0x6322a15a684c]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x2
					r	vpage 0x60004	ppage 0x1
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x2
					rw	vpage 0x60007	ppage 0x2
					r	vpage 0x60009	ppage 0x1
					rw	vpage 0x6000b	ppage 0x2
vm_map		(0x600010000, 0)
file_str = qapers.txt
vm_map returned 0x6000c0000
					returning to (632843) with r|w pages:
					rw	vpage 0x60001	ppage 0x2
					r	vpage 0x60004	ppage 0x1
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x2
					rw	vpage 0x60007	ppage 0x2
					r	vpage 0x60009	ppage 0x1
					rw	vpage 0x6000b	ppage 0x2
vm_fault	(0x6000a0000, read)
core map: 
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824, 0x6322a15a684c]]
clock_q: 
(2, 3, 1)
pm_evict
core: 
ppage (evict): 3
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[3 -> [0x6322a15a6828]]
[2 -> [0x6322a15a6824, 0x6322a15a684c]]
ppage = 3
pte = 0x6322a15a6828
file_write	(<swap>, b)
filemap:
[qapers.txt -> [(0 -> (0, [0x6322a15a6850]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c, 0x6322a15a684c, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]))]]
clock_q: 
(1, 2, 3)
alloc: 3
epage: 3
file_read	(papers.txt, 1)
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a6850]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c, 0x6322a15a684c, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508])), (1 -> (1, [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]))]]
core map: 
[3 -> [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500]]
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[2 -> [0x6322a15a6824, 0x6322a15a684c]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					r	vpage 0x60003	ppage 0x3
					r	vpage 0x60004	ppage 0x1
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x2
					rw	vpage 0x60007	ppage 0x2
					r	vpage 0x60008	ppage 0x3
					r	vpage 0x60009	ppage 0x1
					r	vpage 0x6000a	ppage 0x3
vm_fault	(0x6000a0000, write)
core map: 
[3 -> [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500]]
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[2 -> [0x6322a15a6824, 0x6322a15a684c]]
core map: 
[3 -> [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500]]
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc]]
[2 -> [0x6322a15a6824, 0x6322a15a684c]]
vm_fault returned 0
					returning to (632843) with r|w pages:
					rw	vpage 0x60003	ppage 0x3
					r	vpage 0x60004	ppage 0x1
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x2
					rw	vpage 0x60007	ppage 0x2
					rw	vpage 0x60008	ppage 0x3
					r	vpage 0x60009	ppage 0x1
					rw	vpage 0x6000a	ppage 0x3
vm_create	(632843, 632856)
vm_create returned 0
					returning to (632843) with r|w pages:
					r	vpage 0x60003	ppage 0x3
					r	vpage 0x60004	ppage 0x1
					r	vpage 0x60005	ppage 0x2
					r	vpage 0x60006	ppage 0x2
					r	vpage 0x60007	ppage 0x2
					r	vpage 0x60008	ppage 0x3
					r	vpage 0x60009	ppage 0x1
					r	vpage 0x6000a	ppage 0x3
vm_destroy
infile size: 2d
[0x6322a15a9bbc -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a9bb8 -> ftype: SWAP
infile: true
block: 11
filename: @SWAP
]
[0x6322a15a9bb4 -> ftype: SWAP
infile: false
block: 10
filename: @SWAP
]
[0x6322a15a9bb0 -> ftype: SWAP
infile: false
block: f
filename: @SWAP
]
[0x6322a15a7478 -> ftype: SWAP
infile: false
block: 5
filename: @SWAP
]
[0x6322a15a7474 -> ftype: SWAP
infile: true
block: 4
filename: @SWAP
]
[0x6322a15a9bd8 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a6838 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a6848 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a63ac -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a9bd4 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a6834 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a684c -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a63b0 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a84e0 -> ftype: SWAP
infile: true
block: 6
filename: @SWAP
]
[0x6322a15a6850 -> ftype: FILE_B
infile: true
block: 0
filename: qapers.txt
]
[0x6322a15a84e4 -> ftype: SWAP
infile: false
block: 7
filename: @SWAP
]
[0x6322a15a84e8 -> ftype: SWAP
infile: true
block: 8
filename: @SWAP
]
[0x6322a15a84ec -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f0 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f8 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a84fc -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a8500 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a8504 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a7470 -> ftype: SWAP
infile: false
block: 3
filename: @SWAP
]
[0x6322a15a8508 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bc0 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a6820 -> ftype: SWAP
infile: false
block: 9
filename: @SWAP
]
[0x6322a15a9bc4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a6824 -> ftype: SWAP
infile: false
block: a
filename: @SWAP
]
[0x6322a15a9bc8 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a6828 -> ftype: SWAP
infile: true
block: b
filename: @SWAP
]
[0x6322a15a9bdc -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a683c -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a63a0 -> ftype: SWAP
infile: true
block: c
filename: @SWAP
]
[0x6322a15a9bcc -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a682c -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a9bd0 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a6830 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a9be0 -> ftype: FILE_B
infile: true
block: 0
filename: qapers.txt
]
[0x6322a15a63a4 -> ftype: SWAP
infile: true
block: d
filename: @SWAP
]
[0x6322a15a6840 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a63a8 -> ftype: SWAP
infile: true
block: e
filename: @SWAP
]
[0x6322a15a6844 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a6850, 0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a6834, 0x6322a15a6838, 0x6322a15a683c, 0x6322a15a684c, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
core: 
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a682c, 0x6322a15a6840, 0x6322a15a6848, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a6830, 0x6322a15a6844, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[2 -> [0x6322a15a6824, 0x6322a15a684c, 0x6322a15a9bb0, 0x6322a15a9bb4, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc]]
swfile: 
[11 -> [0x6322a15a9bb8]]
[10 -> [0x6322a15a9bb4]]
[3 -> [0x6322a15a7470]]
[4 -> [0x6322a15a7474]]
[5 -> [0x6322a15a7478]]
[6 -> [0x6322a15a84e0]]
[7 -> [0x6322a15a84e4]]
[8 -> [0x6322a15a84e8]]
[9 -> [0x6322a15a6820]]
[a -> [0x6322a15a6824]]
[b -> [0x6322a15a6828]]
[c -> [0x6322a15a63a0]]
[d -> [0x6322a15a63a4]]
[e -> [0x6322a15a63a8]]
[f -> [0x6322a15a9bb0]]
core after discard: 
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[2 -> [0x6322a15a9bb0, 0x6322a15a9bb4, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc]]
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
ghost: 
free_block: 
(12, 13, 14, 15, 16, 17, 18, 19, 1a, 1b, 1c, 1d, 1e, 1f, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 2a, 2b, 2c, 2d, 2e, 2f, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 3a, 3b, 3c, 3d, 3e, 3f, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 4a, 4b, 4c, 4d, 4e, 4f, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 5a, 5b, 5c, 5d, 5e, 5f, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 6a, 6b, 6c, 6d, 6e, 6f, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 7a, 7b, 7c, 7d, 7e, 7f, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 8a, 8b, 8c, 8d, 8e, 8f, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 9a, 9b, 9c, 9d, 9e, 9f, a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, aa, ab, ac, ad, ae, af, b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, ca, cb, cc, cd, ce, cf, d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, da, db, dc, dd, de, df, e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, ea, eb, ec, ed, ee, ef, f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, fa, fb, fc, fd, fe, ff, 0, 1, 2, 9, a, b)
eblcnt: e8
vm_switch	(632844)
					returning to (632844) with r|w pages:
vm_map		(0x600000000, 0)
core map: 
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[2 -> [0x6322a15a9bb0, 0x6322a15a9bb4, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc]]
core map: 
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[2 -> [0x6322a15a9bb0, 0x6322a15a9bb4, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc]]
core map: 
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[2 -> [0x6322a15a9bb0, 0x6322a15a9bb4, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc]]
clock_q: 
(1, 2, 3)
pm_evict
core: 
ppage (evict): 2
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[2 -> [0x6322a15a9bb0, 0x6322a15a9bb4, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc]]
ppage = 2
pte = 0x6322a15a9bb0
file_write	(<swap>, f)
filemap:
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
clock_q: 
(3, 1, 2)
alloc: 2
epage: 2
file_read	(<swap>, 4)
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
core map: 
[2 -> [0x6322a15a7474]]
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
file_str = ·-!boe!pee addresses used
where an even address is required. Such faults cause the
processor to trap to a system routine. When an illegal
action is caught, unless other arrangements have been
made, the system terminates the process and writes
the user's image on file core in the current directory. A
debugger can be used to determine the state of the
program at the time of the fault.
Programs which are looping, which produce unwanted output, or about which' the user has second
thoughts may be halted by the use of the interrupt
signal, which is generated by typing the "delete"
character. Unless special action has been taken, this
signal simply causes the program to cease execution
without producing a core image file.
There is also a quit signal which is used to force a
core image to be produced. Thus programs which loop
unexpectedly may be halted and the core image examined without prearrangement.
The hardware-generated faults and the interrupt and
quit signals can, by request, be either ignored or caught
by the process. For example, the Shell ignores quits to"
prevent a quit from logging the user out. The editor
catches interrupts and returns to its c o m m a n d level.
This is useful for stopping long printouts without losing
work in progress (the editor manipulates a copy of
the file it is editing). In systems without floating point
hardware, unimplemented instructions are caught, and
floating point instructions are interpreted.

8. Perspective
Perhaps paradoxically, the success of UNIX is largely
due to the fact that it was not designed to meet any
predefined objectives. The first version was written
when one of us (Thompson), dissatisfied with the
available computer facilities, discovered a little-used
Communications
of
the ACM

July 1974
Volume 17
Number 7

PDP-7 and set out to create a more hospitable environment. This essentially personal effort was sufficiently
successful to gain the interest of the remaining author
and others, and later to justify the acquisition of the
POP-11/20, specifically to support a text editing and
formatting system. When in turn the 11/20 was outgrown, UNIX had proved useful enough to persuade
management to invest in the PDP-11/45. Our goals
throughout the effort, when articulated at all, have
always concerned themselves with building a comfortable relationship with the machine and with exploring
ideas and inventions in operating systems. We have
not been faced with the need to satisfy someone else's
requirements, and for this freedom we are grateful.
Three considerations which influenced the design
of UNIX are visible in retrospect.
First, since we are programmers, we naturally
designed the system to make it easy to write, test, and
run programs. The most important expression of our
desire for programming convenience was that the
system was arranged for interactive use, even though
the original version only supported one user. We bebelieve that a properly-designed interactive system is
much more productive and satisfying to use than a
" b a t c h " system. Moreover such a system is rather
easily adaptable to noninteractive use, while the converse is not true.
Second, there have always been fairly severe size
constraints on the system and its software. Given the
partially antagonistic desires for reasonable efficiency
and expressive power, the size constraint has encouraged
not only economy but a certain elegance of design.
This may be a thinly disguised version of the "salvation through suffering" philosophy, but in our case it
worked.
Third, nearly from the start, the system was able to,
and did, maintain itself. This fact is more important
than it might seem. If designers of a system are forced
to use that system, they quickly become aware of its
functional and superficial deficiencies and are strongly
motivated to correct them before it is too late. Since
all source programs were always available and easily
modified on-line, we were willing to revise and rewrite
the system and its software when new ideas were
invented, discovered, or suggested by others.
The aspects of UNIX discussed in this paper exhibit
clearly at least the first two of these design considerations. The interface to the file system, for example, is
extremely convenient from a programming standpoint.
The lowest possible interface level is designed to
eliminate distinctions between the various devices and
files and between direct and sequential access. N o
large "access method" routines are required to insulate
the p r o g r a m m e r from the system calls; in fact, all
user programs either call the system directly or use a
small library program, only tens of instructions long,
which buffers a number of characters and reads or
writes them all at once.

Another important aspect of programming convenience is that there are no "control blocks" with a
complicated structure partially maintained by and depended on by the file system or other system calls.
Generally speaking, the contents of a program's address
space are the property of the program, and we have
tried to avoid placing restrictions on the data structures
within that address space.
Given the requirement that all programs should be
usable with any file or device as input or output, it is
also desirable from a space-efficiency standpoint to push
device-dependent considerations into the operating system itself. The only alternatives seem to be to load
routines for dealing with each device with all programs,
which is expensive in space, or to depend on some means
of dynamically linking to the routine appropriate to
each device when it is actually needed, which is expensive either in overhead or in hardware.
Likewise, the process control scheme and c o m m a n d
interface have proved both convenient and efficient.
Since the Shell operates as an ordinary, swappable user
program, it consumes no wired-down space in the
system proper, and it may be made as powerful as
desired at little cost. In particular, given the framework .
in which the Shell executes as a process which spawns
other processes to perform commands, the notions of
I/O redirection, background processes, c o m m a n d flies,
and user-selectable system interfaces all become essentially trivial to implement.

374

Communications
of
the ACM

8.1 Influences

The success of'UNIX lies not so much in new inventions but rather in the full exploitation of a carefully
selected set of fertile ideas, and especially in showing
that they can be keys to the implementation of a small
yet powerful operating system.
The fork operation, essentially as we implemented it,
was present in the Berkeley time-sharing system [8]. On
a number of points we were influenced by Multics, which
suggested the particular form of the I / o system calls
[9] and both the name of the Shell and its general functions. The notion that the Shell should create a process
for each c o m m a n d was also suggested to us by the
early design of Multics, although in that system it was
later dropped for efficiency reasons. A similar scheme
is used by TENEX [10].

9. Statistics

The following statistics from UNIX are presented to
show the scale of the system and to show how a system
of this scale is used. Those of our users not involved in
document preparation tend to use the system for program development, especially language work. There are
few important "applications" programs.

July 1974
Volume 17
Number 7

9.1 Overall
72 user population
14 maximum simultaneous users
300 directories
4400 files
34000 512-byte secondary storage blocks used
9.2 Per day (24-hour d a y , 7 - d a y week basis)
T h e r e is a " b a c k g r o u n d " process t h a t runs at the
lowest possible p r i o r i t y ; it is used to s o a k u p a n y idle
c P u time. It has been used to p r o d u c e a m i l l i o n - d i g i t
a p p r o x i m a t i o n to the c o n s t a n t e - 2, a n d is n o w
g e n e r a t i n g c o m p o s i t e p s e u d o p r i m e s (base 2).
1800 commands
4.3 CPU hours (aside from background)
70 connect hours
30 different users
75 logins

9.3 Command CPU Usage (cut off at 1%)
15.7% Ccompiler
15.2% users' programs
11.7% editor
5.8% Shell (used as a cornmand, including command times)
5.3% chess
3.3% list directory
3.1% document formatter
1.6% backup dumper
1.8% assembler

1.7%
1.6%
1.6%
1.6%
1.4%
1.3%
1.3%
1.1%
1.0%

Fortran compiler
remove file
tape archive
file system consistency
check
library maintainer
concatenate/printfiles
paginate and print file
print disk usage
copy file

9.4 Command Accesses (cut off at 1%)
15.3%
9.6%
6.3%
6.3%
6.0%
6.0%
3.3%
3.2%
3.1%
1.8%
1.8%
1.6%

editor
list directory
remove file
C compiler
concatenate/printfile
users' programs
list people logged on
system
rename/move file
file status
library maintainer
document formatter
execute another command conditionally

1.6%
1.6%
1.5%
1.4%
1.4%
1.4%
1.2%
1.1%
1.1%
1.1%

debugger
Shell (used as a command)
print disk availability
list processes executing
assembler
print arguments
copy file
paginate and print file
print current date/time
file system consistency
check
1.0% tape archive

ficulties such as p o w e r d i p s a n d i n e x p l i c a b l e p r o c e s s o r
i n t e r r u p t s to r a n d o m locations. T h e r e m a i n d e r are
s o f t w a r e failures. T h e longest u n i n t e r r u p t e d up time
was a b o u t two weeks. Service calls average one every
t h r e e weeks, b u t are h e a v i l y clustered. T o t a l up time
has been a b o u t 98 p e r c e n t o f o u r 24-hour, 365-day
schedule.
Acknowledgments. W e are grateful to R . H . C a n a d a y ,
L.L. Cherry, a n d L.E. M c M a h o n for their c o n t r i b u tions to uNIX. W e are p a r t i c u l a r l y a p p r e c i a t i v e o f the
inventiveness, t h o u g h t f u l criticism, a n d c o n s t a n t supp o r t o f R. M o r r i s , M . D . M c I l r o y , a n d J.F. O s s a n n a .

References
1. Digital Equipment Corporation. PDP-I1/40 Processor
Handbook, 1972, and PDP-I1/45 Processor Handbook, 1971.
2. Deutsch, L.P., and Lampson, B.W. An online editor. Comm.
ACM 10, 12 (Dec. 1967), 793-799, 803.
3. Richards, M. BCPL: A tool for compiler writing and system
programming. Proc. AFIPS 1969 SJCC, Vol. 34, AFIPS Press,
Montvale, N.J., pp. 557-566.
4. McClure, R.M. TMG--A syntax directed compiler. Proc.
ACM 20th Nat. Conf., ACM, 1965, New York, pp. 262-274.
5. Hall, A.D. The M6 macroprocessor. Computing Science Tech.
Rep.#2, Bell Telephone Laboratories, 1969.
6. Ritchie, D.M. C reference manual. Unpublished memorandum,
Bell Telephone Laboratories, 1973.
7. Aleph-null. Computer Recreations. So[?ware Practice and
Experience 1, 2 (Apr.-June 1971), 201-204.
8. Deutsch, L.P., and Lampson, B.W. SDS 930 time-sharing
system preliminary reference manual. Doc. 30.10.10, Project G ENI E,
U of California at Berkeley, Apr. 1965.
9. Feiertag, R.J., and Organick, E.I. The Multics input-output
system. Proc. Third Syrup. on Oper. Syst. Princ., Oct. 18-20, 1971,
ACM, New York, pp. 35-41.
10. Bobrow, D.G., Burchfiel, J.D., Murphy, D.L., and Tomlinson,
R.S. TENEX, a paged time sharing system tbr the PDP-10. Comm.
ACM15, 3 (Mar. 1972), 135-143.

9.5 Reliability
O u r statistics on reliability are m u c h m o r e subjective
t h a n the others. T h e f o l l o w i n g results are true to the
best o f o u r c o m b i n e d recollections. T h e t i m e s p a n is
over one y e a r with a very early vintage 11/45.
T h e r e has been one loss o f a file system (one d i s k
o u t o f five) caused b y s o f t w a r e i n a b i l i t y to c o p e with
a h a r d w a r e p r o b l e m c a u s i n g r e p e a t e d p o w e r fail traps.
Files on t h a t d i s k were b a c k e d up three days.
A " c r a s h " is an u n s c h e d u l e d system r e b o o t o r
halt. T h e r e is a b o u t one crash every o t h e r d a y ; a b o u t
t w o - t h i r d s o f t h e m are c a u s e d by h a r d w a r e - r e l a t e d dif-

375

Communications
of
the ACM

July 1974
Volume 17
Number 7

THE STRUCTURE OF THE "THE"-MULTIPROGRAMMING SYSTEM

EWDI 96

Edsger W.Dijkstra
Technological University
EINDHOVEN
The Netherlands

Summary
A multiprogramming system is described in
which all activities are divided over a number of
sequential processes. These sequential processes
are placed at various hierarchical levels, in each
of which one or more independent abstractions have
been implemented. The hierarchical structure proved
to be vital for the verification of the logical
soundness of the design and the correctness of its
implementation.
Introduction
Papers "reporting on timely research and
development efforts" being explicitly asked for, I
shall try to present a progress report on the multiprogramming effort at the Department of Mathematics
at the Technological University, Eindhoven, the
Netherlands.
Having very limited resources (viz. a group of
six people of, on the average, half time availability) and wishing to contribute to the art of system
design -including all the stages of conception,
construction and verification- we are faced with the
problem of how to get the necessary experience. To
solve this problem we have adopted the following
three guiding principles:
I) Select a project as advanced as you can
conceive, as ambitious as you can justify, in the
hope that routine work can be kept to a minimum;
hold out against all pressure to incorporate such
system expansions that would only result into a
purely quantitative increase of the total amount of
work to be done.
2) Select a machine with sound basic characteristics (e.g. an interrupt system to fall in love
with is certainly an inspiring feature); from then
onwards try to keep the specific properties of the
configuration for which you are preparing the system
out of your considerations as long as possible.
3) Be aware of the fact that experience does by
no means automatically lead to wisdom and understanding; in other words, make a conscious effort to
learn as much as possible from your precious
experiences.
Accordingly, I shall try to go beyond just
reporting what we have done and how, and I shall
try to formulate as well what we have learned.
I should like to end the introduction with two
short remarks on working conditions, remarks I make
for the sake of completeness. I shall not stress
these points any further.

The one remark is that production speed is
severely degraded if one works with half time people
who have other obligations as well. This is at least
a factor four, probably it is worse. The people
themselves lose time and energy in switching over,
the group as a whole loses decision speed as discussions~ when needed, have often to be postponed
until all people concerned are available.
The other remark is that the members of the
group (mostly mathematicians) have previously
enjoyed as good students a university training of
5 to 8 years and are of Master's or Ph.D. level.
I mention this explicitly because at least in my
country the intellectual level needed for system
design is in general grossly underestimated. I am
more than ever convinced that this type of work is
just difficult and that every effort to do it with
other than the best people is doomed to either
failure or moderate success at enormous expenses.
The Tool and the Goal
The system has been designed for a Dutch
machine, the EL X8 (N.V.Electrologica, Rijswijk
(ZH)). Characteristics of our configuration are:
I) core memory cycle time 2.5 mms., 27 bits; at
present 32K.
2) drum of 512K words, 1024 words per track, rev.
time 40 ms.
3) an indirect addressing mechanism very well
suited for stack implementation
4) a sound system for commanding peripherals and
controlling of interrupts
5) a potentially great number of low capacity
channels; ten of them are used (3 paper tape readers
at 1000 char/sac; 3 paper tape punches at 150 char/
sec; 2 teleprinters; a plotter; a line printer)
6) absence of a number of not unusual awkward
features.
The primary goal of the system is to process
smoothly a continuous flow of user programs as a
service to the University. A multiprogramming
system has been chosen with the following objectives
in mind:
I) a reduction of turn around time for programs of
short duration
2) economic use of peripheral devices
3)automatic control of backing store to be combined
with economic use of the central processor
4) the economic feasibility to use the machine for
those applications for which only the flexibility of

a general purpose computer is needed but (as a r u l e )
not the capacity nor the processing power.

The system is not intended as a multi-access
system. There is no common data base via which
independent users can communicate with each other:
they only share the configuration and a procedure
library (that includes a translator for ALGOL 60
extended with complex numbers). The system does not
cater for user programs written in machine language.
Compared with larger efforts one can state that
quantitatively speaking the goals have been set as
modest as the equipment and our other resources.
Qualitatively speaking, I am afraid, we got more and
more immodest as the work progressed.

minutes (classical) inspection at the machine and
each of them correspondingly easy to remedy. At the
moment of writing the testing is not yet completed,
but the resulting system will be guaranteed to be
flawless. When the system has been delivered we shall
not live in the perpetual fear that a system derailment may still occur in an unlikely situation such as
might result from an unhappy "coincidence" of two or
more critical occurrences, for we shall have proved
the correctness of the system with a rigour and
explicitness that is unusual for the great majority
of mathematical proofs.
A Survey of the System Structure

A Progress Report
Storage Allocation.
We have made some minor mistakes of the usual
type (such as paying too much attention to speeding
up what was not the real bottle neck) and two major
ones.
Our f i r s t
m a j o r m i s t a k e has been t h a t f o r t o o
long a time we confined our attention to "a perfect
installation": by the time we considered how to make
the best of it when, say, one of the peripherals
broke down, we were faced with nasty problems.
Taking care of the "pathology" took more energy than
we had expected and part of our troubles were a
direct consequence of our earlier ingenuity, i.e.
the complexity of the situation into which the system
could have manoeuvred itself. Had we paid attention
to the pathology at an earlier stage of the design,
our management rules would certainly have been less
refined.

The second major mistake has been that we
conceived and programmed the major part of the system
without giving more than scanty thought to the problem of debugging it. For the fact that this mistake
had no serious consequences -on the contrary~ one
might argue as an afterthought- I must decline all
credit. I feel more like having passed through the
eye of the needle...
As captain of the crew I had had extensive
experience (dating back to 1958) in making basic
software dealing with real time interrupts and I
knew by bitter experience that as a result of the
irreproducibility of the interrupt moments, a program
error could present itself misleadingly like an
occasional machine malfunctioning. As a result I was
terribly afraid. Having fears regarding the possibility of debugging we decided to be as careful as
possible and -prevention is better than cure~- to
try to prevent nasty bugs from entering the construction.
This decision, inspired by fear, is at the
bottom of what I regard as the group's main contribution to the art of system design. We have found
that it is possible to design a refined multiprogramming system in such a way that its logical soundness
can be proved a priori and that its implementation
admits exhaustive testing. The only errors that
showed up during testing were trivial coding errors
(occurring with a density of one error per 500
instructions), each of them located within 10

In the classical yon Neumann machine information
is identified by the address of the memory location
containing the information. When we started to think
about the automatic control of secondary storage
we were familiar with a system (viz. GIER ALGOL)
in which all information was identified by its
drum address (as in the classical yon Neumann
machine) and in which the function of the core
memory was nothing more than to make the information
"page wise" accessible.
We have followed another approach and as it
turned out, to great advantage. In our terminology
we made a strict distinction between memory units
(we called them "pages" and had "core pages" and
"drum pages") and corresponding information units
(for lack of a better word we called them "segments")
a segment just fitting in a page. For segments we
created a completely independent identification
mechanism in which the number of possible segment
identifiers is much larger than the total number of
pages in primary and secondary store. The segment
identifier gives fast access to a so-called "segment
variable" in core whose value denotes whether the
segment is still empty or not and if not empty, in
which page (or pages) it can be found.
As a consequence of this approach: if a segment
of information, residing in a core page, has to be
dumped onto the drum in order to make the core page
available for other use, there is no need to return
the segment to the same drum page as it originally
came from. In fact, this freedom is exploited: among
the free drum pages the one with minimum latency
time is selected.
A next consequence is the total absence of a
drum allocation problem: there is not the slightest
reason why, say, a program should occupy consecutive
drum pages. In a multiprogramming environment this
is very convenient.
Processor Allocation.
We have given full recognition to the fact that
in a single sequential process (such as performed by
a sequential automaton) only the time succession of
the various states has a logical meaning, but not
the actual speed with which the sequential process
is performed. Therefore we have arranged the whole

system as a society of sequential processes, progressing with undefined speed ratios. To each user
program, accepted by the system, corresponds a
sequential process, to each input peripheral
corresponds a sequential process (buffering input
streams in synchronism with the execution of the
input commands), to each output peripheral corresponds a sequential process (unbuffering output
streams in synchronism with the execution of the
output commands); furthermore we have the "segment
controller" associated with the drum and the
"message interpreter" associated with the console
keyboard.
This enabled us to design the whole system in
terms of these abstract "sequential processes".
Their harmonious co-operation is regulated by means
of explicit mutual synchronization statements. On
the one hand, this explicit mutual synchronization
is necessary, as we do not make any assumption about
speed ratios, on the other hand this mutual synchronization is possible because "delaying the progress
of a process temporarily" can never be harmful to
the interior logic of the process delayed. The
fundamental consequence of this approach -viz. the
explicit mutual synchronization- is that the
harmonious co-operation of a set of such sequential
processes can be established by discrete reasoning;
as a further consequence the whole harmonious
society of co-operating sequential processes is
independent of the actual number of processors
available to carry out these processes, provided
the processors available can switch from process
to process.
System Hierarchy.
The total system admits a strict hierarchical
structure.
At level 0 we find the responsibility for
processor allocation to one of the processes whose
dynamic progress is logically permissible (i.e. in
view of the explicit mutual synchronization). At
this level the interrupt of the real time clock is
processed, introduced to prevent any process to
monopolize processing power. At this level a priority
rule is incorporated to achieve quick response of the
system where this is needed. Our first abstraction
has been achieved, above level 0 the number of
processors actually shared is no longer relevant. At
the higher levels we find the activity of the
different sequential processes, the actual processor
having lost its identity, having disappeared from
the picture.
At level I we have the so-called "segment
controller", a sequential process synchronized with
respect to the drum interrupt and the sequential
processes on higher levels. At level I we find the
responsibility to cater for the bookkeeping resulting
from the automatic backing store. At this level our
next abstraction has been achieved: at all higher
levels identification of information takes place in
terms of segments, the actual storage pages having
lost their identity, having disappeared from the

picture.
At level 2 we find the "message interpreter",
taking care of the allocation of the console keyboard
via which conversations between te operator and any
of the higher level processes can be carried out. The
message interpreter works in close synchronism with
the operator: when the operator presses a key, a
character is sent to the machine together with an
interrupt signal to announce this next keyboard
character, while the actual printing is then done
on account of an output command generated by the
machine under control of the message interpreter.
(As far as the hardware is concerned the console
teleprinter is regarded as two independent peripherals: an input keyboard and an output printer.) If
one of the processes opens a conversation it identifies itself for the benefit of the operator in the
opening sentence of this conversation. If, however,
the operator opens a conversation he must identify
the process he is addressing, in the opening sentence
of the conversation, i.e. this opening sentence must
be interpreted before it is known to which of the
processes the conversation is addressed~ There lies
the logical reason to introduce a separate sequential
process for the console teleprinter, a reason that
is reflected in its name "message interpreter". Above
level 2 it is as if each process had its private
conversational console. The fact that they share the
same physical console is translated into a resource
restriction of the form "only one conversation at a
time", a restriction that is satisfied via mutual
synchronization. At this level the next abstraction
has been implemented: at the higher levels the actual
console teleprinter has lost its identity. (If the
message interpreter had not been on a higher level
than the segment controller, then the only way to
implement it would have been to make a permanent
reservation in core for it; as the conversational
vocabulary might get large (as soon as our operators
wish to be addressed in fancy messages) this would
result in too heavy a permanent demand upon core
storage. Therefore the vocabulary in which the
messages are expressed is stored on segments, i.e.
as information units that can reside on the drum as
well. For this reason the message interpreter is of
a level one higher than the segment controller.)
At level 3 we find the sequential processes
associated with buffering of input streams and
unbuffering of output streams. At this level the next
abstraction is effected, viz. the abstraction of the
actual peripherals used, that are allocated at this
level to the "logical communication units" in terms
of which is worked in the still higher levels. The
sequential processes associated with the peripherals
are of a level above the message interpreter, because
they must be able to converse with the operator (e.g.
in the case of detected malfunctioning). The limited
number of peripherals again acts as a resource
restriction for the processes at higher levels, to be
satisfied by mutual synchronization between them.
At level 4 we find the independent user programs,
at level 5 the operator (not implemented by us).

The system structure has been described at
length in order to make the next section intelligible,
Design Experience
The conception stage took a long time. During
that period of time the concepts have been born in
terms of which we sketched the system in the previous
section. Furthermore we learnt the art of reasoning
by which we could deduce from our requirements the
way in which the processes should influence each
other as regards mutual synchronization so that
these requirements would be met. (The requirements
being that no information can be used before it has
been produced, that no peripheral can be set to two
tasks simultaneously, etc.) Finally we learnt the
art of reasoning by which we could prove that the
society composed of processes thus mutually synchronized by each other, would indeed in its time
behaviour satisfy all requirements.
The construction stage has been rather traditional, perhaps even old-fashioned: plain machine
code. Reprogramming on account of a change of
specifications has been rare, a circumstance that
must have contributed greatly to the feasibility of
the "steam method". The fact that the first two
stages took more time than planned was somewhat
compensated by a delay in the delivery of the machine.
In the verification stage we had, during short
shots, the machine completely at our disposal, shots
during which we worked with a virgin machine without
any software aids for debugging. Starting at level 0
the system has been tested, each time adding (a
portion of) the next level only after the previous
level had been thoroughly tested. Each test shot
itself contained on top of the (partial) system to
be tested a number of testing processes with a double
function. Firstly they had to force the system into
all different relevant states, secondly they had to
verify that the system continued to react according
to specification.
I shall not deny that the construction of these
testing programmes has been a major intellectual
effort: to convince oneself that one has not overlooked "e relevant state" and to convince oneself
that the testing programmes generate them all is no
simple matter. The encourageing thing is that (as
far as we are aware~) it could be done.
This fact was one of the happy consequences of
the hierarchical structure.
Testing level 0 (the real time clock and processor allocation) implied a number of testing
sequential processes on top of it, inspecting together that under all circumstances processor time
was divided among them according to the rules. This
being established, sequential processes as such had
been implemented.
Testing the segment controller at level I
meant that all "relevant states" could be formulated
in terms of sequential processes making (in various
combinations) demands on core pages, situations that

could be provoked by explicit synchronizing among
the testing programs. At that stage the existence
of the real time clock -although interrupting all
the time- was so immaterial that one of the testers
indeed forgot its existence~
By that time we had implemented the correct
reaction upon the (mutually unsynchronized) interrupts from the real time clock and the drum. If we
had not introduced the separate levels 0 and I and
if we had not created a terminology (viz. that of
the rather abstract sequential processes) in which
the existence of the clock interrupt could be discarded, but had tried instead to make in a nonhierarchical construction the central processor
directly react upon any weird time succession of
these two interrupts, the number of "relevant states"
would have exploded to such a height that exhaustive
testing would have been an illusion. (Apart from that
it is doubtful wether we would have had the means to
generate them all, drum and clock speed being outside
our control.)
For the sake of completeness I must mention
a further happy consequence. As stated before, above
level I core and drum pages have lost their identity
and buffering of input and output streams (at level
3) therefore occurs in terms of segments. While
testing at level 2 or 3 the drum channel hardware
broke down for quite some time, but testing could
proceed by restricting the number of segments so
that they all could be held in core. If building
up the line printer output streams had been implemented as "dumping onto the drum" and the actual printing as "printing from the drum" this advantage would
have been denied to us.
Conclusion
As far as program verification is concerned I
present nothing essentially new. In testing a
general purpose object (be it a piece of hardware,
a program, a machine or a system) one cannot subject
it to all possible cases: for a computer this would
imply that one feeds it with ell possible programs!
Therefore one must test it with a set of relevant
test cases. What is relevant or not, cannot be
decided as long as one regards the mechanism as a
black box, in other words it has to follow from the
internal structure of the mechanism to be tested. It
seems the designer's responsibility to construct his
mechanism in such a way -i.e. so effectively structured- that at each stage of the testing procedure
the number of relevant test cases is so small that
he can try them all and that what is being tested is
so perspicuous that it is clear that he has not
overlooked a situation. I have presented a survey of
our system because I think it a nice example of the
form that such a structure might take.
In my experience, I am sorry to say, industrial
software makers tend to react to it with mixed
feelings. On the one hand they are inclined to judge
that we have done a kind of model job, on the other
hand they express doubts whether the techniques used
are applicable outside the sheltered atmosphere of a

University Department and express the opinion that
we could only do it this way thanks to the modest
scope of the whole project. It is not my intention
to underestimate the organizing ability needed for
a much bigger job with ten or more times as many
people, but I should like to venture the opinion
that the larger the project, the more essential the
structuring~ A hierarchy of five logical levels
might then very well turn out to be of modest depth,
in particular when one designs the system more
consciously than we have done with the aim that the
software can be smoothly adapted to (perhaps drastic)
configuration expansions.
Acknowledqements
I should not like to publish this progress
report without expressing my great indebtedness to
my five collaborators C.Bron, A.N.Habermann, F.J.A.
Hendriks, C.Ligtmans and P.A.Voorhoeve. They have
contributed to ell the stages of the design, together
we learnt the art of reasoning needed. Construction
and verification is entirely their effort: if my
dreams have become true, this is due to their faith,
their talents and their persistent loyalty to the
whole project.
Finally I should like to thank the members of
the program committee who asked for more information
on the synchronizing primitives and some justification of my claim to be able to prove logical soundness a priori. In answer to this request the appendix
has been added, of which I hope that it gives the
desired information and justification.

"V(sem)" increases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is positive, the V-operation in question
has no further effect; if, however, the resulting
value of the semaphore concerned is non-positive,
one of the processes booked on its waiting list is
removed from this waiting list, i.e. its dynamic
progress is again logically permissible and in due
time a processor will be allocated to it (again, see
above "System Hierarchy", at level 0).
Corollary I:
If a semaphore value is nonpositive its absolute value equals the number of
processes booked on its waiting list.
Corollary 2:
The P-operation represents the
potential delay, the complementary V-operation
represents the removal of a barrier.
Note I:
P- and V-operations are "indivisible
actions", i.e. if they occur "simultaneously" in
parallel processes, they are non-interfering in the
sense that they can be regarded as being performed
the one after the other.
Note 2:
If the semaphore value resulting
from a V-operation is negative, its waiting list
did originally contain more than one process. It is
undefined -i.e. logically immaterial- which of the
waiting processes is then removed from the waiting
list.
Note 3:
A consequence of the mechanisms
described above is that a process whose dynamic
progress is permissible can only loose this status
by actually progressing, i.e. by performance of a
P-operation on a semaphore with a value that is
initially non-positive.

Appendix
The S~nchronizinq Primitives.
Explicit mutual synchronization of parallel
sequential processes is implemented via so-called
"semaphores". They are special purpose integer
variables allocated in the universe in which the
processes are embedded, they are initialized (with
the value 0 or I) before the parallel processes
themselves are started. After this initialization
the parallel processes will access the semaphores
only via two very specific operations, the so-called
synchronizing primitives. For historical reasons
they are called the P-operation and the V-operation.
A process, "Q" say, that performs the operation
"P(sem)" decreases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is non-negative, process Q can continue
with the execution of its next statement; if,
however, the resulting value is negative, process
Q is stopped and booked on a waiting list associated
with the semaphore concerned. Until further notice
(i.e. a V-operation on this very same semaphore)
dynamic progress of process Q is not logically
permissible and no processor will be allocated to
it (see above "System Hierarchy", at level 0).
A process, "R" say, that performs the operation

During system conception it transpired that we
used the semaphores in two completely different
ways. The difference is so marked that, looking
back, one wonders whether it was really fair to
present the two ways as a usage of the very same
primitives. On the one hand we have the semaphores
used for mutual exclusion, on the other hand the
private semaphores.
The Mutual Exclusion.
In the following program we indicate two
parallel, cyclic processes (between the brackets
"parbeRin" and '~arend") that come into action
after the surrounding universe has been introduced
and initialized.
begin semaphore mutex; mutex := I;
parbegin
beqin L I : P(mutex); critical section I; V(mutsx);
remainder of cycle I; ~oto LI
end;
beqin L2: P(mutex); critical section 2; V(mutex);
remainder of cycle 2; qoto L2
end
parend
end
As a result of the P- and V-operations on

"mutex" the actions, marked as "critical sections"
exclude each other mutually in time; the scheme
given allows straightforward extension to more than
two parallel processes, the maximum value of mutex
= I, the minimum value = - (n - I) if we have n
parallel processes.
Critical sections are used always and only for
the purpose of unambiguous inspection and modification of the state variables (allocated in the
surrounding universe) that describe the current
state of the system (as far as needed for the
regulation of the harmonious co-operation between
the various processes).
The Private Semaphores.
Each sequential process has associated with it
a number of private semaphores and no other process
will ever perform a P-operation on them. The universe
initializes them with the value = O, their maximum
value = I, their minimum value = - I.
Whenever a process reaches a stage where the
permission for dynamic progress depends on current
values of state variables, it follows the pattern:

P(mutex);
"inspection and modification of state variables
including a conditional V(private semaphore)";

V(mutex);
P(private semaphore)
If the inspection learns that the process in
question should continue, it performs the operation
"V(private semaphore)" -the semaphore value then
changes from 0 to I-, otherwise this V-operation
is skipped, leaving to the other processes the
obligation to perform this V-operation at a suitable
moment. The absence or presence of this obligation
is reflected in the final values of the state
variables upon leaving the critical section.
Whenever a process reaches a stage where as a
result of its progress possibly one (or more)
blocked processes should now get permission to
continue, it follows the pattern

P(mutex);
"modification and inspection of state variables
including zero or more V-operations on private
semaphores of other processes";

V(mutex)
By the introduction of suitable state variables
and appropriate programming of the critical sections
any strategy assigning peripherals, buffer areas etc.
can be implemented.
The amount of coding and reasoning can be
greatly reduced by the observation that in the two
complementary critical sections sketched above, the
same inspection can be performed by the introduction
of the notion of "an unstable situation", such as
a free reader and a process needing a reader.
Whenever an unstable situation emerges it is
removed (including ome or more V-operations on

private semaphores) in the very same critical
section in which it has been created.
Provinq the Harmonious Co-operation.
The sequential processes in the system can all
be regarded as cyclic processes in which a certain
neutral point can be marked, the so-called "homing
position", in which all processes are when the
system is at rest.
When a cyclic process leaves its homing position
"it accepts a task", when the task has been performed
end not earlier, the process returns to its homing
position. Each cyclic process has a specific task
processing power (e.g. the execution of a user
program or unbuffering a portion of printer output,
etc.)
The harmonious co-operation is mainly proved
in roughly three stages.
I)
It is proved that although a process
performing a task may generate in doing so a finite
number of tasks for other processes, a single
initial task cannot give rise to an infinite number
of task generations. The proof is simple as
processes can only generate tasks for processes
at lower levels of the hierarchy so that circularity
is excluded. (If a process needing a segment from
the drum has generated a task for the segment
controller, special precautions have been taken to
ensure that the segment asked for remains in core
at least until the requesting process has effectively
accessed the segment concerned. Without this precaution finite tasks could be forced to generate an
infinite number of tasks for the segment controller
and the system could get stuck in an unproductive
page flutter.)
2)
It is proved that it is impossible
that all processes have returned to their homing
position while somewhere in the system is still
pending a generated but unaccepted task. (This is
proved via instability of the situation just
described.)
3)
It is proved that after the acceptance
of an initial task all processes eventually will be
(again) in their homing position. Each process
blocked in the course of task execution relies on the
other processes for removal of the barrier. Essentially, the proof in question is a demonstratmon of the
absence of "circular waits": process P waiting for
process Q waiting for process R waiting for process
P. (Our usual term for the circular wait is "the
Deadly Embrace".) In a more general society than
our system this proof turned out to be e proof by
induction (on the level of hierarchy, starting at
the lowest level) as A.N.Habermann has shown in his
doctoral thesis.

THE WORKING SET MODEL FOR PROGRAM BEHAVIOR
Peter J. Denning
Massachusetts Institute of Technology
Cambridge, Massachusetts

SUMMARY

We claim neither is adequate.

Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate
model for program behavior.
In this paper a new
model is developed, the "working set model", which
enables us to decide which information is in use
by a running program and which is not.
Such knowledge is vital for dynamic management of paged
memories.
The working set of pages associated
with a process, defined to be the collection of its
most recently used pages, is a useful allocation
concept. A proposal for an easy-to-implement
allocation policy is set forth; this policy is
unique, inasmuch as it blends into one decision
function the heretofore independent activities of
process-scheduling and memory-management.

Because resources are multiplexed, each user
is given the illusion that he has a complete computing system at his sole disposal: a virtual
computer.
For our purposes, the basic elements of
a virtual computer are its virtual processor and
an "infinite" one-level virtual memory.
Dynamic
"advice" regarding resource requirements cannot be
obtained successfully from users for several
reasons:
i. A user may build his program on the work
of others, frequently sharing procedures
whose time and storage requirements may be
either unknown or, because of data dependence, indeterminate.
Therefore he cannot
be expected to estimate processor-memory
needs.
2. It is not clear what sort of "advice" might
be solicited.
Nor is it clear how the
operating system should use it, for overhead incurred by using advice could well
negate any advantages attained.
3. Any advice acquired from a user would be
intended (by him) to optimize the environment for his own program.
Configuring
resources to suit individuals may interfere
with overall good service to the community
of users.
Thus it seems inadvisable at the present time to
permit users, at their discretion, to advise the
operating system of their needs.

INTRODUCTION
Resource allocation is tricky business.
In
recent years there has been much dialogue on the
topics of process scheduling and core memory management, yet development of techniques has progressed independently along both these lines. No
one will deny that a unified approach is needed.
Probably the most basic reason behind the absence
of a general treatment is the lack of an adequate
model for program behavior.
In this paper we develop a new model, the working set model, which
embodies certain important behavioral properties
of computations operating in a multiprogrammed environment, enabling us to decide which information
is in use by a running program and which is not.
We do not intend that the proposed model be considered "final"; rather, we hope to stimulate a
new kind of thinking, thinking that may be of considerable help in solving many operating system
design problems.
The working set is intended to model the behavior of programs in the general purpose computer
system, or computer utility.
For this reason we
assume that the operating system must on its own
determine the behavior of programs it runs; it
cannot count on outside help.
Two commonly proposed sources of externally-supplied dynamic allocation information are the user and the compiler.

Work reported herein was supported in part by
Project MAC, an M.I.T. research project sponsored
by the Advanced Projects Research Agency, Dept.
of Defense, under Office of Naval Research Contract Number Nonr-4102(Ol).

Likewise, compilers cannot be expected to
supply information, extracted from the structure
of the program , regarding resource requirements:
i. Programs will be modular in construction;
information about other modules may be unavailable at compilation time.
Because of
dependence on data there may be no way to
decide (until run time) just which modules
will be included in a computation.
2. Compilers cluttered with extra machinery
to predict memory needs will be slower in
operation.
Many users are less interested
in whether their programs operate efficiently than whether they operate at all, and
so are concerned with rapid compilation.
Furthermore, the compiler is an often-used
component of the operating system; if slow
and bulky, it can be a serious drain on
system resources.

**Ramamoorthy I has put forth a proposal for automatic segmentation of programs during compilation.

Therefore in this paper we are advocating mechanisms that monitor the behavior of a computation,
making allocation decisions on the basis of currently observed characteristics.
Only a mechanism
that oversees the behavior of a program in operation can cope with arbitrary interconnections of
arbitrary modules having arbitrary characteristics.
Our treatment proceeds as follows. First we
define the type of computer system in which our
ideas are developed. After a brief discussion of
previous work with the problems of dynamic memory
management, we define the working set model. We
discuss a method of implementing memory management
based on this model, and indicate how working set
notions can be used to blend process scheduling
and memory management into one decision function,
accounting simultaneously for both types of demand.
Finally we discuss how data sharing fits into the
working-set scheme.

THE FRAMEWORK
We assume that the reader is already familiar
with the concepts of a computer utility 2'3'4, of
segmentation and paging 5'6, of program and addressing structure 6'8, so we will only mention these
topics here. Briefly, each process has access to
its own private, segmented name space; each segment known to the process is sliced into equalsize units, called pages, to facilitate mapping it
into the paged main memory.
Associated with each
segment is a page table, whose entries point to

the segment's pages. An "in-core" bit in each
page table entry is turned ON whenever the designated page is present in main memory ; an attempt
to reference a page whose "in-core" bit is OFF
causes a page fault, initiating proceedings to
secure the missing page. Finally, a process has
three states of existence: running, when a processor is assigned to it; ready, when it would be
running if only a processor were available; or
blocked, when it has no need of a processor (for
example, during a page fault or during a console
interaction). When talking about processes in execution, we will have to distinguish between "process time" and "real time". Process time is time
as seen by a process unaware it is suspended; that
is, as if it executed without interruptions.
We restrict attention to a two-level memory
system, indicated by Figure I. Only data residing
in main memory is accessible to a processor; all
other data reside in auxiliary memory, which we
regard as having i nflnite capacity. There is a
time T, the traverse time, involved in transferring
a page between memories. T is measured from the
moment a page fault occurs until the moment the
missing page is in main memory ready for use. T
is actually the expectation of a random variable
composed of waits in queues and mechanical positioning delays.
Though it usually takes less time to
store into auxiliary memory than to read from it,
we shall regard the traverse time T to be the same
regardless of which direction a page is moved.
,
Consistent with current usage, we will use the
terms "core memory" and "main memory" interchangeably.

PROCESSORS

~

~ ~

~ n

data flow from main memory
(controlled b y ~, c o r emanager)
I

main memÂ°rY(contrD~edfl~ ~ ~ ~ n m ~ [ c Y i e s )

FIGURE i.

Two-level memory system.

A basic allocation problem, "core memory management", is that of deciding just which pages are to
occupy main memory.
The basic strategy advocated
here -- a compromise against a lot of expensive
,
main memory -- is to minimize page traffic . There
are two reasons for this:
i. The more the data traffic between the two
levels of memory, the more the computational overhead in deciding just what to move
and where to move it.
2. Because the traverse time T is long compared
to a memory cycle, too much data movement
can result in congestion and serious interference with processor efficiency.
Roughly speaking, a working set of pages is
the minimum collection of pages that must be loaded
in main memory for a process to operate efficiently,
without "unnecessary" page faults. According to
our definitions, a "process" and its "working set"
are but two manifestations of the same ongoing
computational activity.

PREVIOUS WORK
In this section we outline strategies that have
been set forth in the past for memory management;
the interested reader will be referred to the literature for detail.
We regard management of paged memories to operate in two stages:
I. Pagin_g in." locate the required page in
auxiliary memory, load it into main memory,
turn the "in-core" bit of the appropriate
page table entry ON.
2. Paging out: remove some page from main memory, turn the "in-core" bit of the appropriate page table entry OFF.
Management algorithms can be classified according
to their methods of paging in and paging out.
It
is a common characteristic of nearly every strategy
that paging in is done on demand; that is, no action
is taken to load a page into memory until some
process attempts to reference it. To date there
have been no proposals recommending look-ahead, or
anticipatory page-loading, because (as we have
stressed) there is no reliable advance source of
allocation information, be it the programmer or
the compiler.
Although the working set is the
desired information, it might still be futile to
pre-load pages: there is no guarantee that a process will not block shortly after resumption,
having referenced only a fraction of its working
set.
The operating system could devote its already precious time to activities more rewarding
than loading pages which may not be used.
Thus we
will assume that paging in is done on demand only,
via the page fault mechansim.

Since data is stored and transmitted in units of
pages, we can (without ambiguity) refer to data
movement as "page traffic".

The chief problem in memory management is not
deciding which pages to load; it is deciding which
pages ought to be removed.
For if the page with
the least likelihood of being used in the immediate
future is retired to auxiliary memory, the best
choice has been made.
Nearly every worker in the
field has recognized this.
Debate has arisen over
which strategy to employ for retiring pages; that
is, which page-turning, or replacement, algorithm
to use.
A good measure of performance for a paging
policy is page traffic (the number of pages per
unit time being moved between memories), since
erroneously removed pages add to the traffic of
returning pages.
In the following we will use this
as a basis of comparison for several strategies.
Random selection.
Whenever a fresh page of memory
is needed, a page is selected at random to be replaced.
Although utterly simple to implement, this
method frequently removes useful pages (which must
therefore be recalled) and so results in high page
traffic.
~!%~
selection. The pages of main memory are ordered in a cyclic list.
Suppose the M pages of
main memory are numbered 0,1,...,(M-I) and a
pointer k indicates that the k-th page was most
recently paged in. Whenever a fresh page of memory
is needed, [(k+l) mod M] 4 k, page k is retired,
and another page brought in to fill the now vacant
slot.
This method -- also utterly simple to realize -- is based on the principle that programs tend
to follow sequences of instructions, so that references in the immediate future will most likely
be close to present references.
So, assuming there
is this tendency for page references to cluster,
and assuming some kind of uniformity in process
scheduling techniques, the page which has been in
memory longest is least likely to be reused: hence
the cyclic list. We see two ways in which this
algorithm can fail. First we question its basic
assumption.
It is not at all clear that modular
programs, which execute numerous inter-module calls,
will indeed exhibit sequential instruction fetch
patterns.
The thread of control will not string
pages together; rather, it will entwine them intricately.
[Fine, Mclssac, and Jackson 9 have some
experimental evidence in support of this reasoning.]
Second, this algorithm is subject to overloading when used in multiprogrammed memories.
When core demand is too heavy, one cycle of the
list completes rapidly and the pages deleted are
still needed by their processes.
This can create
a self-intensifying crisis.
Programs, deprived of
still-needed pages, generate a plethora of page
faults; the resulting traffic of returning pages
displaces still other useful pages, leading to more
page faults, and so on.
Oldest-unused selection.
Each page table entry
contains a ~us~ ~ bit, set ON each time the page is
referenced.
At periodic intervals all the page
table entries are searched and usage records updated.
When a fresh page of memory is needed, the page unreferenced for the longest time is removed.
One can

see that this method is intrinsically reasonable by
considering the simple case of a computer where
there is exactly one process whose pages cannot all
fit into main memory.
In this case the most reasonable choice for a page to replace is the oldest
unused page.
Unfortunately this method too is susceptible to overloading when many processes compete
for main memory.
ATLAS ioo~ detection method.

The Ferranti ATLAS

computer I0 had proposed a page-turning policy that
attempted to detect loop behavior in page reference
patterns, then minimize page traffic by maximizing
the time between page transfers, that is, by removing pages not expected to be needed for the longest time.
It was successful -- only for looping
programs.
Performance was unimpressive for programs exhibiting random reference patterns.
Implementation was costly.
Various studies concerning behavior of paging
algorithms have appeared.
Fine, Mclssac and
Jackson 9 have investigated the effects of demand
paging and have questioned whether paging is beneficial at all. We do not feel that their conclusion applies to the kind of multiprogrammed environment we have described.
They studied fixed-size
programs, that quickly acquired and retained a
large fraction of their pages.
Highly interactive,
modular programs are likely to behave differently.
Not only may program size vary dynamically (according to data dependencies), but also such programs
should be using a small fraction of their pages at
any one time, and the membership in this set of
working pages should be changing constantly.

We define the working set of information W(t,T)
of a process at time t to be the collection of data
items referenced by the process during the proces
time interval (t-T~,t).
Thus, the data items a process has referenced
during the last T seconds of its execution comprise
its working set. q will be called the working set
parameter.
We will regard the data items in W(t,T)
as being pages ,although they could just as well
be any other named data objects.
The working set
size ~(t,~) is
(I)

~(t,T)

=

Number of pages in W(t,T)

A working set W(t,~) has two important, general
properties.
Both are properties of typical programs,
and need not hold in special cases.
PI. Size. It should be clear immediately that
~(t,0) = 0 since no page reference can occur
in zero time.
It should also be clear that
~(t,T) as a function of T is monotonically
increasing, since more pages can be referenced
in longer time intervals.
Because a process
will refer to its most-needed pages frequently
and its least-needed pages infrequently, we
expect ~(t,~) as a function of T to have a
steep initial rise which diminishes to a more
gradual rise.
The general character of ~(t,~)
is suggested by the smoothed curve of Figure 2.
00(t ,~-)

Belady II has compared some of the algorithms
mathematically.
His most important conclusion is
that the "ideal" algorithm should possess much of
the simplicity of Random or Cyclic selection (for
efficiency) and some, though not much, accumulation
of data on past reference patterns.
He has shown
that too much "historical" data can have adverse
effects (witness ATLAS).
In the next section we begin investigation of
the working set concept.
Even though the ideas
are not entirely new 12'13'14, there has been no
detailed documentation publicly available.

THE WORKING SET MODEL
From the programmer's standpoint, the working
set of information is the smallest collection of
procedure and data items that must be present in
main memory to assure efficient execution of his
program.
We have already stressed that there will
be no advance notice from either the programmer or
the compiler regarding what information "ought" to
be in main memory.
It is up to the operating
system to determine on the basis of page reference
patterns whether pages are in use.
Therefore the
working set of information associated with a process is, from the system standpoint, the set of
most recently referenced pages.

FIGURE 2.

Behavior of ~(t,T).

Program modularity enables us to
P2. Correlation.
say something about correlation between the
size of a working set at two times, t and (t+c~).
Correlation is useful in devising storage
allocators, for the higher the correlation
between ~(t,T) and w(t+c~,T), the better is
~(t,T) a prediction of ~(t+c~,T).
In modular
programs, control passes randomly from one
module to another; if T is chosen properly
(as discussed in the next section), it is more
likely that a working set will change size
smoothly, less likely that it will change size
abruptly.
Thus for small time separations
(say, 6 < < T), ~(t,~) and ~(t+c~,T) are highly

correlated, meaning that a measurement of ~(t,T)
will serve as a good estimate of the memory requirement during the process time interval (t,t+c0.
For large time separations ~ (say, ~ >> .[), control
will have passed through a great many modules
during the interval (t,t+c~); thus ~(t,T ) gives
little information about ~(t+C~,7), and so ~(t,7)
and ~(t+c~,7) have much less correlation than for
small 5. This behavior is suggested in Figure 3.
Correlation between
~(t,7) and ~(t+Cz,7)

Detecting W(t~T )
According to our definition, W(t,T) is the set
of its pages a process has referenced within the
last T seconds of its execution.
This suggests
that memory management can be controlled by hardware mechanisms, by associating with each page of
main memory a timer.
Each time a page is referenced, its timer is set to T and begins to run
down; if the timer succeeds in running down, a flag
is set to mark the page for removal whenever the
space is needed.
In the appendix we describe such
a hardware memory management mechanism, hardware
that can be housed within the memory boxes.
The
mechanism has two interesting features:
i. It operates asynchronously and independently
of the supervisor, whose only respsonsibility
in memory management is handling page faults.
Quite literally, memory manages itself.
2. Analog devices such as capacitative timers
could be used to measure intervals.

~D

c~
0
FIGURE 3.

Correlation between working set sizes,

Choice of T

Unfortunately it is not practical to add on
hardware to existing systems. We seek a method of
handling memory management within the software.
The procedure we propose here samples the page
table entries of pages in core memory at process
time intervals of ~ seconds (~ is called the
sampling interval) where ~ = y/K , K an integer
constant chosen to make the sampling intervals as
"fine grain" as desired.
On the basis of page
references during each of the last K sampling intervals, the working set W(t,l~) can be determined,
as follows.

The value ultimately selected for ~ will reflect
efficiency requirements and will be influenced by
"in- core"
system parameters such as core memory size and memI
ory traverse time.
For example, if T is too small,
pages may be removed from main memory while they
are still useful, and high page traffic may result
from returning pages.
If T is too large, pages
may remain in main memory long after last being
used, and wasted main memory may result.
Thus the
value of T will have to represent a compromise between too much page traffic and too much wasted
memory space.
0

l

pointer
to page

use bits

luol-,l

---

TYPICAL PAGE TABLE ENTRY

SHIFT AT END OF SAMPLING INTERVAL
The following consideration leads us to recommend for T a value comparable to the memory traverse time T (Figure I). Consider a process that
is running continuously, being interrupted only
for page faults.
Assuming that memory allocation
procedures balk at removing from main memory any
page in a working set, once a page has entered
W(t,T) it will remain in main memory for at least
T seconds.
Under the very worst of page-shuffling
conditions, a page could be dispatched to auxiliary memory and be recalled immediately; the time
for this round trip is two traverse times, 2T.
Therefore a highly-shuffled page would spend
roughly T/2T of its time in main memory.
So,
for example, if we wished to insure that a page is
available in main memory (when needed) for not less
than 50 per cent of the time, we would have to
choose â¢ ~ 2T.

FIGURE 4.

Page table entries for detecting W(t,Kcy).

As indicated by Figure 4, each page table entry
contains an "in-core" bit M, where M=i if and only
if the page is present in main memory.
It also
contains a string of use bits u0,ul,...,u K. Each
time a page reference occurs,

I ~ u 0.

At the end

of each sampling interval ~, the bit pattern contained in u0,ul,...,u K is shifted one position,
a 0 enters u 0, and u K ipapers.txt
vm_map returned 0x600030000
					returning to (632844) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
vm_map		(0x600010000, 0)
file_str = papers.txt
matched! 
vm_map returned 0x600040000
					returning to (632844) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
vm_map		(0x600010000, 0)
file_str = papers.txt
matched! 
vm_map returned 0x600050000
					returning to (632844) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
vm_map		(0x600010000, 1)
file_str = papers.txt
matched! 
vm_map returned 0x600060000
					returning to (632844) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x3
vm_map		(0x600010000, 2)
file_str = papers.txt
matched! 
vm_map returned 0x600070000
					returning to (632844) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60006	ppage 0x3
vm_fault	(0x600030000, read)
core map: 
[2 -> [0x6322a15a7474, 0x6322a15a7480, 0x6322a15a7484]]
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a7488, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a748c, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
clock_q: 
(3, 1, 2)
pm_evict
core: 
ppage (evict): 1
[2 -> [0x6322a15a7474, 0x6322a15a7480, 0x6322a15a7484]]
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a7488, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
[1 -> [0x6322a15a63b0, 0x6322a15a748c, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
ppage = 1
pte = 0x6322a15a63b0
filemap:
[·-!boe!pee addresses used
where an even address is required. Such faults cause the
processor to trap to a system routine. When an illegal
action is caught, unless other arrangements have been
made, the system terminates the process and writes
the user's image on file core in the current directory. A
debugger can be used to determine the state of the
program at the time of the fault.
Programs which are looping, which produce unwanted output, or about which' the user has second
thoughts may be halted by the use of the interrupt
signal, which is generated by typing the "delete"
character. Unless special action has been taken, this
signal simply causes the program to cease execution
without producing a core image file.
There is also a quit signal which is used to force a
core image to be produced. Thus programs which loop
unexpectedly may be halted and the core image examined without prearrangement.
The hardware-generated faults and the interrupt and
quit signals can, by request, be either ignored or caught
by the process. For example, the Shell ignores quits to"
prevent a quit from logging the user out. The editor
catches interrupts and returns to its c o m m a n d level.
This is useful for stopping long printouts without losing
work in progress (the editor manipulates a copy of
the file it is editing). In systems without floating point
hardware, unimplemented instructions are caught, and
floating point instructions are interpreted.

8. Perspective
Perhaps paradoxically, the success of UNIX is largely
due to the fact that it was not designed to meet any
predefined objectives. The first version was written
when one of us (Thompson), dissatisfied with the
available computer facilities, discovered a little-used
Communications
of
the ACM

July 1974
Volume 17
Number 7

PDP-7 and set out to create a more hospitable environment. This essentially personal effort was sufficiently
successful to gain the interest of the remaining author
and others, and later to justify the acquisition of the
POP-11/20, specifically to support a text editing and
formatting system. When in turn the 11/20 was outgrown, UNIX had proved useful enough to persuade
management to invest in the PDP-11/45. Our goals
throughout the effort, when articulated at all, have
always concerned themselves with building a comfortable relationship with the machine and with exploring
ideas and inventions in operating systems. We have
not been faced with the need to satisfy someone else's
requirements, and for this freedom we are grateful.
Three considerations which influenced the design
of UNIX are visible in retrospect.
First, since we are programmers, we naturally
designed the system to make it easy to write, test, and
run programs. The most important expression of our
desire for programming convenience was that the
system was arranged for interactive use, even though
the original version only supported one user. We bebelieve that a properly-designed interactive system is
much more productive and satisfying to use than a
" b a t c h " system. Moreover such a system is rather
easily adaptable to noninteractive use, while the converse is not true.
Second, there have always been fairly severe size
constraints on the system and its software. Given the
partially antagonistic desires for reasonable efficiency
and expressive power, the size constraint has encouraged
not only economy but a certain elegance of design.
This may be a thinly disguised version of the "salvation through suffering" philosophy, but in our case it
worked.
Third, nearly from the start, the system was able to,
and did, maintain itself. This fact is more important
than it might seem. If designers of a system are forced
to use that system, they quickly become aware of its
functional and superficial deficiencies and are strongly
motivated to correct them before it is too late. Since
all source programs were always available and easily
modified on-line, we were willing to revise and rewrite
the system and its software when new ideas were
invented, discovered, or suggested by others.
The aspects of UNIX discussed in this paper exhibit
clearly at least the first two of these design considerations. The interface to the file system, for example, is
extremely convenient from a programming standpoint.
The lowest possible interface level is designed to
eliminate distinctions between the various devices and
files and between direct and sequential access. N o
large "access method" routines are required to insulate
the p r o g r a m m e r from the system calls; in fact, all
user programs either call the system directly or use a
small library program, only tens of instructions long,
which buffers a number of characters and reads or
writes them all at once.

Another important aspect of programming convenience is that there are no "control blocks" with a
complicated structure partially maintained by and depended on by the file system or other system calls.
Generally speaking, the contents of a program's address
space are the property of the program, and we have
tried to avoid placing restrictions on the data structures
within that address space.
Given the requirement that all programs should be
usable with any file or device as input or output, it is
also desirable from a space-efficiency standpoint to push
device-dependent considerations into the operating system itself. The only alternatives seem to be to load
routines for dealing with each device with all programs,
which is expensive in space, or to depend on some means
of dynamically linking to the routine appropriate to
each device when it is actually needed, which is expensive either in overhead or in hardware.
Likewise, the process control scheme and c o m m a n d
interface have proved both convenient and efficient.
Since the Shell operates as an ordinary, swappable user
program, it consumes no wired-down space in the
system proper, and it may be made as powerful as
desired at little cost. In particular, given the framework .
in which the Shell executes as a process which spawns
other processes to perform commands, the notions of
I/O redirection, background processes, c o m m a n d flies,
and user-selectable system interfaces all become essentially trivial to implement.

374

Communications
of
the ACM

8.1 Influences

The success of'UNIX lies not so much in new inventions but rather in the full exploitation of a carefully
selected set of fertile ideas, and especially in showing
that they can be keys to the implementation of a small
yet powerful operating system.
The fork operation, essentially as we implemented it,
was present in the Berkeley time-sharing system [8]. On
a number of points we were influenced by Multics, which
suggested the particular form of the I / o system calls
[9] and both the name of the Shell and its general functions. The notion that the Shell should create a process
for each c o m m a n d was also suggested to us by the
early design of Multics, although in that system it was
later dropped for efficiency reasons. A similar scheme
is used by TENEX [10].

9. Statistics

The following statistics from UNIX are presented to
show the scale of the system and to show how a system
of this scale is used. Those of our users not involved in
document preparation tend to use the system for program development, especially language work. There are
few important "applications" programs.

July 1974
Volume 17
Number 7

9.1 Overall
72 user population
14 maximum simultaneous users
300 directories
4400 files
34000 512-byte secondary storage blocks used
9.2 Per day (24-hour d a y , 7 - d a y week basis)
T h e r e is a " b a c k g r o u n d " process t h a t runs at the
lowest possible p r i o r i t y ; it is used to s o a k u p a n y idle
c P u time. It has been used to p r o d u c e a m i l l i o n - d i g i t
a p p r o x i m a t i o n to the c o n s t a n t e - 2, a n d is n o w
g e n e r a t i n g c o m p o s i t e p s e u d o p r i m e s (base 2).
1800 commands
4.3 CPU hours (aside from background)
70 connect hours
30 different users
75 logins

9.3 Command CPU Usage (cut off at 1%)
15.7% Ccompiler
15.2% users' programs
11.7% editor
5.8% Shell (used as a cornmand, including command times)
5.3% chess
3.3% list directory
3.1% document formatter
1.6% backup dumper
1.8% assembler

1.7%
1.6%
1.6%
1.6%
1.4%
1.3%
1.3%
1.1%
1.0%

Fortran compiler
remove file
tape archive
file system consistency
check
library maintainer
concatenate/printfiles
paginate and print file
print disk usage
copy file

9.4 Command Accesses (cut off at 1%)
15.3%
9.6%
6.3%
6.3%
6.0%
6.0%
3.3%
3.2%
3.1%
1.8%
1.8%
1.6%

editor
list directory
remove file
C compiler
concatenate/printfile
users' programs
list people logged on
system
rename/move file
file status
library maintainer
document formatter
execute another command conditionally

1.6%
1.6%
1.5%
1.4%
1.4%
1.4%
1.2%
1.1%
1.1%
1.1%

debugger
Shell (used as a command)
print disk availability
list processes executing
assembler
print arguments
copy file
paginate and print file
print current date/time
file system consistency
check
1.0% tape archive

ficulties such as p o w e r d i p s a n d i n e x p l i c a b l e p r o c e s s o r
i n t e r r u p t s to r a n d o m locations. T h e r e m a i n d e r are
s o f t w a r e failures. T h e longest u n i n t e r r u p t e d up time
was a b o u t two weeks. Service calls average one every
t h r e e weeks, b u t are h e a v i l y clustered. T o t a l up time
has been a b o u t 98 p e r c e n t o f o u r 24-hour, 365-day
schedule.
Acknowledgments. W e are grateful to R . H . C a n a d a y ,
L.L. Cherry, a n d L.E. M c M a h o n for their c o n t r i b u tions to uNIX. W e are p a r t i c u l a r l y a p p r e c i a t i v e o f the
inventiveness, t h o u g h t f u l criticism, a n d c o n s t a n t supp o r t o f R. M o r r i s , M . D . M c I l r o y , a n d J.F. O s s a n n a .

References
1. Digital Equipment Corporation. PDP-I1/40 Processor
Handbook, 1972, and PDP-I1/45 Processor Handbook, 1971.
2. Deutsch, L.P., and Lampson, B.W. An online editor. Comm.
ACM 10, 12 (Dec. 1967), 793-799, 803.
3. Richards, M. BCPL: A tool for compiler writing and system
programming. Proc. AFIPS 1969 SJCC, Vol. 34, AFIPS Press,
Montvale, N.J., pp. 557-566.
4. McClure, R.M. TMG--A syntax directed compiler. Proc.
ACM 20th Nat. Conf., ACM, 1965, New York, pp. 262-274.
5. Hall, A.D. The M6 macroprocessor. Computing Science Tech.
Rep.#2, Bell Telephone Laboratories, 1969.
6. Ritchie, D.M. C reference manual. Unpublished memorandum,
Bell Telephone Laboratories, 1973.
7. Aleph-null. Computer Recreations. So[?ware Practice and
Experience 1, 2 (Apr.-June 1971), 201-204.
8. Deutsch, L.P., and Lampson, B.W. SDS 930 time-sharing
system preliminary reference manual. Doc. 30.10.10, Project G ENI E,
U of California at Berkeley, Apr. 1965.
9. Feiertag, R.J., and Organick, E.I. The Multics input-output
system. Proc. Third Syrup. on Oper. Syst. Princ., Oct. 18-20, 1971,
ACM, New York, pp. 35-41.
10. Bobrow, D.G., Burchfiel, J.D., Murphy, D.L., and Tomlinson,
R.S. TENEX, a paged time sharing system tbr the PDP-10. Comm.
ACM15, 3 (Mar. 1972), 135-143.

9.5 Reliability
O u r statistics on reliability are m u c h m o r e subjective
t h a n the others. T h e f o l l o w i n g results are true to the
best o f o u r c o m b i n e d recollections. T h e t i m e s p a n is
over one y e a r with a very early vintage 11/45.
T h e r e has been one loss o f a file system (one d i s k
o u t o f five) caused b y s o f t w a r e i n a b i l i t y to c o p e with
a h a r d w a r e p r o b l e m c a u s i n g r e p e a t e d p o w e r fail traps.
Files on t h a t d i s k were b a c k e d up three days.
A " c r a s h " is an u n s c h e d u l e d system r e b o o t o r
halt. T h e r e is a b o u t one crash every o t h e r d a y ; a b o u t
t w o - t h i r d s o f t h e m are c a u s e d by h a r d w a r e - r e l a t e d dif-

375

Communications
of
the ACM

July 1974
Volume 17
Number 7

THE STRUCTURE OF THE "THE"-MULTIPROGRAMMING SYSTEM

EWDI 96

Edsger W.Dijkstra
Technological University
EINDHOVEN
The Netherlands

Summary
A multiprogramming system is described in
which all activities are divided over a number of
sequential processes. These sequential processes
are placed at various hierarchical levels, in each
of which one or more independent abstractions have
been implemented. The hierarchical structure proved
to be vital for the verification of the logical
soundness of the design and the correctness of its
implementation.
Introduction
Papers "reporting on timely research and
development efforts" being explicitly asked for, I
shall try to present a progress report on the multiprogramming effort at the Department of Mathematics
at the Technological University, Eindhoven, the
Netherlands.
Having very limited resources (viz. a group of
six people of, on the average, half time availability) and wishing to contribute to the art of system
design -including all the stages of conception,
construction and verification- we are faced with the
problem of how to get the necessary experience. To
solve this problem we have adopted the following
three guiding principles:
I) Select a project as advanced as you can
conceive, as ambitious as you can justify, in the
hope that routine work can be kept to a minimum;
hold out against all pressure to incorporate such
system expansions that would only result into a
purely quantitative increase of the total amount of
work to be done.
2) Select a machine with sound basic characteristics (e.g. an interrupt system to fall in love
with is certainly an inspiring feature); from then
onwards try to keep the specific properties of the
configuration for which you are preparing the system
out of your considerations as long as possible.
3) Be aware of the fact that experience does by
no means automatically lead to wisdom and understanding; in other words, make a conscious effort to
learn as much as possible from your precious
experiences.
Accordingly, I shall try to go beyond just
reporting what we have done and how, and I shall
try to formulate as well what we have learned.
I should like to end the introduction with two
short remarks on working conditions, remarks I make
for the sake of completeness. I shall not stress
these points any further.

The one remark is that production speed is
severely degraded if one works with half time people
who have other obligations as well. This is at least
a factor four, probably it is worse. The people
themselves lose time and energy in switching over,
the group as a whole loses decision speed as discussions~ when needed, have often to be postponed
until all people concerned are available.
The other remark is that the members of the
group (mostly mathematicians) have previously
enjoyed as good students a university training of
5 to 8 years and are of Master's or Ph.D. level.
I mention this explicitly because at least in my
country the intellectual level needed for system
design is in general grossly underestimated. I am
more than ever convinced that this type of work is
just difficult and that every effort to do it with
other than the best people is doomed to either
failure or moderate success at enormous expenses.
The Tool and the Goal
The system has been designed for a Dutch
machine, the EL X8 (N.V.Electrologica, Rijswijk
(ZH)). Characteristics of our configuration are:
I) core memory cycle time 2.5 mms., 27 bits; at
present 32K.
2) drum of 512K words, 1024 words per track, rev.
time 40 ms.
3) an indirect addressing mechanism very well
suited for stack implementation
4) a sound system for commanding peripherals and
controlling of interrupts
5) a potentially great number of low capacity
channels; ten of them are used (3 paper tape readers
at 1000 char/sac; 3 paper tape punches at 150 char/
sec; 2 teleprinters; a plotter; a line printer)
6) absence of a number of not unusual awkward
features.
The primary goal of the system is to process
smoothly a continuous flow of user programs as a
service to the University. A multiprogramming
system has been chosen with the following objectives
in mind:
I) a reduction of turn around time for programs of
short duration
2) economic use of peripheral devices
3)automatic control of backing store to be combined
with economic use of the central processor
4) the economic feasibility to use the machine for
those applications for which only the flexibility of

a general purpose computer is needed but (as a r u l e )
not the capacity nor the processing power.

The system is not intended as a multi-access
system. There is no common data base via which
independent users can communicate with each other:
they only share the configuration and a procedure
library (that includes a translator for ALGOL 60
extended with complex numbers). The system does not
cater for user programs written in machine language.
Compared with larger efforts one can state that
quantitatively speaking the goals have been set as
modest as the equipment and our other resources.
Qualitatively speaking, I am afraid, we got more and
more immodest as the work progressed.

minutes (classical) inspection at the machine and
each of them correspondingly easy to remedy. At the
moment of writing the testing is not yet completed,
but the resulting system will be guaranteed to be
flawless. When the system has been delivered we shall
not live in the perpetual fear that a system derailment may still occur in an unlikely situation such as
might result from an unhappy "coincidence" of two or
more critical occurrences, for we shall have proved
the correctness of the system with a rigour and
explicitness that is unusual for the great majority
of mathematical proofs.
A Survey of the System Structure

A Progress Report
Storage Allocation.
We have made some minor mistakes of the usual
type (such as paying too much attention to speeding
up what was not the real bottle neck) and two major
ones.
Our f i r s t
m a j o r m i s t a k e has been t h a t f o r t o o
long a time we confined our attention to "a perfect
installation": by the time we considered how to make
the best of it when, say, one of the peripherals
broke down, we were faced with nasty problems.
Taking care of the "pathology" took more energy than
we had expected and part of our troubles were a
direct consequence of our earlier ingenuity, i.e.
the complexity of the situation into which the system
could have manoeuvred itself. Had we paid attention
to the pathology at an earlier stage of the design,
our management rules would certainly have been less
refined.

The second major mistake has been that we
conceived and programmed the major part of the system
without giving more than scanty thought to the problem of debugging it. For the fact that this mistake
had no serious consequences -on the contrary~ one
might argue as an afterthought- I must decline all
credit. I feel more like having passed through the
eye of the needle...
As captain of the crew I had had extensive
experience (dating back to 1958) in making basic
software dealing with real time interrupts and I
knew by bitter experience that as a result of the
irreproducibility of the interrupt moments, a program
error could present itself misleadingly like an
occasional machine malfunctioning. As a result I was
terribly afraid. Having fears regarding the possibility of debugging we decided to be as careful as
possible and -prevention is better than cure~- to
try to prevent nasty bugs from entering the construction.
This decision, inspired by fear, is at the
bottom of what I regard as the group's main contribution to the art of system design. We have found
that it is possible to design a refined multiprogramming system in such a way that its logical soundness
can be proved a priori and that its implementation
admits exhaustive testing. The only errors that
showed up during testing were trivial coding errors
(occurring with a density of one error per 500
instructions), each of them located within 10

In the classical yon Neumann machine information
is identified by the address of the memory location
containing the information. When we started to think
about the automatic control of secondary storage
we were familiar with a system (viz. GIER ALGOL)
in which all information was identified by its
drum address (as in the classical yon Neumann
machine) and in which the function of the core
memory was nothing more than to make the information
"page wise" accessible.
We have followed another approach and as it
turned out, to great advantage. In our terminology
we made a strict distinction between memory units
(we called them "pages" and had "core pages" and
"drum pages") and corresponding information units
(for lack of a better word we called them "segments")
a segment just fitting in a page. For segments we
created a completely independent identification
mechanism in which the number of possible segment
identifiers is much larger than the total number of
pages in primary and secondary store. The segment
identifier gives fast access to a so-called "segment
variable" in core whose value denotes whether the
segment is still empty or not and if not empty, in
which page (or pages) it can be found.
As a consequence of this approach: if a segment
of information, residing in a core page, has to be
dumped onto the drum in order to make the core page
available for other use, there is no need to return
the segment to the same drum page as it originally
came from. In fact, this freedom is exploited: among
the free drum pages the one with minimum latency
time is selected.
A next consequence is the total absence of a
drum allocation problem: there is not the slightest
reason why, say, a program should occupy consecutive
drum pages. In a multiprogramming environment this
is very convenient.
Processor Allocation.
We have given full recognition to the fact that
in a single sequential process (such as performed by
a sequential automaton) only the time succession of
the various states has a logical meaning, but not
the actual speed with which the sequential process
is performed. Therefore we have arranged the whole

system as a society of sequential processes, progressing with undefined speed ratios. To each user
program, accepted by the system, corresponds a
sequential process, to each input peripheral
corresponds a sequential process (buffering input
streams in synchronism with the execution of the
input commands), to each output peripheral corresponds a sequential process (unbuffering output
streams in synchronism with the execution of the
output commands); furthermore we have the "segment
controller" associated with the drum and the
"message interpreter" associated with the console
keyboard.
This enabled us to design the whole system in
terms of these abstract "sequential processes".
Their harmonious co-operation is regulated by means
of explicit mutual synchronization statements. On
the one hand, this explicit mutual synchronization
is necessary, as we do not make any assumption about
speed ratios, on the other hand this mutual synchronization is possible because "delaying the progress
of a process temporarily" can never be harmful to
the interior logic of the process delayed. The
fundamental consequence of this approach -viz. the
explicit mutual synchronization- is that the
harmonious co-operation of a set of such sequential
processes can be established by discrete reasoning;
as a further consequence the whole harmonious
society of co-operating sequential processes is
independent of the actual number of processors
available to carry out these processes, provided
the processors available can switch from process
to process.
System Hierarchy.
The total system admits a strict hierarchical
structure.
At level 0 we find the responsibility for
processor allocation to one of the processes whose
dynamic progress is logically permissible (i.e. in
view of the explicit mutual synchronization). At
this level the interrupt of the real time clock is
processed, introduced to prevent any process to
monopolize processing power. At this level a priority
rule is incorporated to achieve quick response of the
system where this is needed. Our first abstraction
has been achieved, above level 0 the number of
processors actually shared is no longer relevant. At
the higher levels we find the activity of the
different sequential processes, the actual processor
having lost its identity, having disappeared from
the picture.
At level I we have the so-called "segment
controller", a sequential process synchronized with
respect to the drum interrupt and the sequential
processes on higher levels. At level I we find the
responsibility to cater for the bookkeeping resulting
from the automatic backing store. At this level our
next abstraction has been achieved: at all higher
levels identification of information takes place in
terms of segments, the actual storage pages having
lost their identity, having disappeared from the

picture.
At level 2 we find the "message interpreter",
taking care of the allocation of the console keyboard
via which conversations between te operator and any
of the higher level processes can be carried out. The
message interpreter works in close synchronism with
the operator: when the operator presses a key, a
character is sent to the machine together with an
interrupt signal to announce this next keyboard
character, while the actual printing is then done
on account of an output command generated by the
machine under control of the message interpreter.
(As far as the hardware is concerned the console
teleprinter is regarded as two independent peripherals: an input keyboard and an output printer.) If
one of the processes opens a conversation it identifies itself for the benefit of the operator in the
opening sentence of this conversation. If, however,
the operator opens a conversation he must identify
the process he is addressing, in the opening sentence
of the conversation, i.e. this opening sentence must
be interpreted before it is known to which of the
processes the conversation is addressed~ There lies
the logical reason to introduce a separate sequential
process for the console teleprinter, a reason that
is reflected in its name "message interpreter". Above
level 2 it is as if each process had its private
conversational console. The fact that they share the
same physical console is translated into a resource
restriction of the form "only one conversation at a
time", a restriction that is satisfied via mutual
synchronization. At this level the next abstraction
has been implemented: at the higher levels the actual
console teleprinter has lost its identity. (If the
message interpreter had not been on a higher level
than the segment controller, then the only way to
implement it would have been to make a permanent
reservation in core for it; as the conversational
vocabulary might get large (as soon as our operators
wish to be addressed in fancy messages) this would
result in too heavy a permanent demand upon core
storage. Therefore the vocabulary in which the
messages are expressed is stored on segments, i.e.
as information units that can reside on the drum as
well. For this reason the message interpreter is of
a level one higher than the segment controller.)
At level 3 we find the sequential processes
associated with buffering of input streams and
unbuffering of output streams. At this level the next
abstraction is effected, viz. the abstraction of the
actual peripherals used, that are allocated at this
level to the "logical communication units" in terms
of which is worked in the still higher levels. The
sequential processes associated with the peripherals
are of a level above the message interpreter, because
they must be able to converse with the operator (e.g.
in the case of detected malfunctioning). The limited
number of peripherals again acts as a resource
restriction for the processes at higher levels, to be
satisfied by mutual synchronization between them.
At level 4 we find the independent user programs,
at level 5 the operator (not implemented by us).

The system structure has been described at
length in order to make the next section intelligible,
Design Experience
The conception stage took a long time. During
that period of time the concepts have been born in
terms of which we sketched the system in the previous
section. Furthermore we learnt the art of reasoning
by which we could deduce from our requirements the
way in which the processes should influence each
other as regards mutual synchronization so that
these requirements would be met. (The requirements
being that no information can be used before it has
been produced, that no peripheral can be set to two
tasks simultaneously, etc.) Finally we learnt the
art of reasoning by which we could prove that the
society composed of processes thus mutually synchronized by each other, would indeed in its time
behaviour satisfy all requirements.
The construction stage has been rather traditional, perhaps even old-fashioned: plain machine
code. Reprogramming on account of a change of
specifications has been rare, a circumstance that
must have contributed greatly to the feasibility of
the "steam method". The fact that the first two
stages took more time than planned was somewhat
compensated by a delay in the delivery of the machine.
In the verification stage we had, during short
shots, the machine completely at our disposal, shots
during which we worked with a virgin machine without
any software aids for debugging. Starting at level 0
the system has been tested, each time adding (a
portion of) the next level only after the previous
level had been thoroughly tested. Each test shot
itself contained on top of the (partial) system to
be tested a number of testing processes with a double
function. Firstly they had to force the system into
all different relevant states, secondly they had to
verify that the system continued to react according
to specification.
I shall not deny that the construction of these
testing programmes has been a major intellectual
effort: to convince oneself that one has not overlooked "e relevant state" and to convince oneself
that the testing programmes generate them all is no
simple matter. The encourageing thing is that (as
far as we are aware~) it could be done.
This fact was one of the happy consequences of
the hierarchical structure.
Testing level 0 (the real time clock and processor allocation) implied a number of testing
sequential processes on top of it, inspecting together that under all circumstances processor time
was divided among them according to the rules. This
being established, sequential processes as such had
been implemented.
Testing the segment controller at level I
meant that all "relevant states" could be formulated
in terms of sequential processes making (in various
combinations) demands on core pages, situations that

could be provoked by explicit synchronizing among
the testing programs. At that stage the existence
of the real time clock -although interrupting all
the time- was so immaterial that one of the testers
indeed forgot its existence~
By that time we had implemented the correct
reaction upon the (mutually unsynchronized) interrupts from the real time clock and the drum. If we
had not introduced the separate levels 0 and I and
if we had not created a terminology (viz. that of
the rather abstract sequential processes) in which
the existence of the clock interrupt could be discarded, but had tried instead to make in a nonhierarchical construction the central processor
directly react upon any weird time succession of
these two interrupts, the number of "relevant states"
would have exploded to such a height that exhaustive
testing would have been an illusion. (Apart from that
it is doubtful wether we would have had the means to
generate them all, drum and clock speed being outside
our control.)
For the sake of completeness I must mention
a further happy consequence. As stated before, above
level I core and drum pages have lost their identity
and buffering of input and output streams (at level
3) therefore occurs in terms of segments. While
testing at level 2 or 3 the drum channel hardware
broke down for quite some time, but testing could
proceed by restricting the number of segments so
that they all could be held in core. If building
up the line printer output streams had been implemented as "dumping onto the drum" and the actual printing as "printing from the drum" this advantage would
have been denied to us.
Conclusion
As far as program verification is concerned I
present nothing essentially new. In testing a
general purpose object (be it a piece of hardware,
a program, a machine or a system) one cannot subject
it to all possible cases: for a computer this would
imply that one feeds it with ell possible programs!
Therefore one must test it with a set of relevant
test cases. What is relevant or not, cannot be
decided as long as one regards the mechanism as a
black box, in other words it has to follow from the
internal structure of the mechanism to be tested. It
seems the designer's responsibility to construct his
mechanism in such a way -i.e. so effectively structured- that at each stage of the testing procedure
the number of relevant test cases is so small that
he can try them all and that what is being tested is
so perspicuous that it is clear that he has not
overlooked a situation. I have presented a survey of
our system because I think it a nice example of the
form that such a structure might take.
In my experience, I am sorry to say, industrial
software makers tend to react to it with mixed
feelings. On the one hand they are inclined to judge
that we have done a kind of model job, on the other
hand they express doubts whether the techniques used
are applicable outside the sheltered atmosphere of a

University Department and express the opinion that
we could only do it this way thanks to the modest
scope of the whole project. It is not my intention
to underestimate the organizing ability needed for
a much bigger job with ten or more times as many
people, but I should like to venture the opinion
that the larger the project, the more essential the
structuring~ A hierarchy of five logical levels
might then very well turn out to be of modest depth,
in particular when one designs the system more
consciously than we have done with the aim that the
software can be smoothly adapted to (perhaps drastic)
configuration expansions.
Acknowledqements
I should not like to publish this progress
report without expressing my great indebtedness to
my five collaborators C.Bron, A.N.Habermann, F.J.A.
Hendriks, C.Ligtmans and P.A.Voorhoeve. They have
contributed to ell the stages of the design, together
we learnt the art of reasoning needed. Construction
and verification is entirely their effort: if my
dreams have become true, this is due to their faith,
their talents and their persistent loyalty to the
whole project.
Finally I should like to thank the members of
the program committee who asked for more information
on the synchronizing primitives and some justification of my claim to be able to prove logical soundness a priori. In answer to this request the appendix
has been added, of which I hope that it gives the
desired information and justification.

"V(sem)" increases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is positive, the V-operation in question
has no further effect; if, however, the resulting
value of the semaphore concerned is non-positive,
one of the processes booked on its waiting list is
removed from this waiting list, i.e. its dynamic
progress is again logically permissible and in due
time a processor will be allocated to it (again, see
above "System Hierarchy", at level 0).
Corollary I:
If a semaphore value is nonpositive its absolute value equals the number of
processes booked on its waiting list.
Corollary 2:
The P-operation represents the
potential delay, the complementary V-operation
represents the removal of a barrier.
Note I:
P- and V-operations are "indivisible
actions", i.e. if they occur "simultaneously" in
parallel processes, they are non-interfering in the
sense that they can be regarded as being performed
the one after the other.
Note 2:
If the semaphore value resulting
from a V-operation is negative, its waiting list
did originally contain more than one process. It is
undefined -i.e. logically immaterial- which of the
waiting processes is then removed from the waiting
list.
Note 3:
A consequence of the mechanisms
described above is that a process whose dynamic
progress is permissible can only loose this status
by actually progressing, i.e. by performance of a
P-operation on a semaphore with a value that is
initially non-positive.

Appendix
The S~nchronizinq Primitives.
Explicit mutual synchronization of parallel
sequential processes is implemented via so-called
"semaphores". They are special purpose integer
variables allocated in the universe in which the
processes are embedded, they are initialized (with
the value 0 or I) before the parallel processes
themselves are started. After this initialization
the parallel processes will access the semaphores
only via two very specific operations, the so-called
synchronizing primitives. For historical reasons
they are called the P-operation and the V-operation.
A process, "Q" say, that performs the operation
"P(sem)" decreases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is non-negative, process Q can continue
with the execution of its next statement; if,
however, the resulting value is negative, process
Q is stopped and booked on a waiting list associated
with the semaphore concerned. Until further notice
(i.e. a V-operation on this very same semaphore)
dynamic progress of process Q is not logically
permissible and no processor will be allocated to
it (see above "System Hierarchy", at level 0).
A process, "R" say, that performs the operation

During system conception it transpired that we
used the semaphores in two completely different
ways. The difference is so marked that, looking
back, one wonders whether it was really fair to
present the two ways as a usage of the very same
primitives. On the one hand we have the semaphores
used for mutual exclusion, on the other hand the
private semaphores.
The Mutual Exclusion.
In the following program we indicate two
parallel, cyclic processes (between the brackets
"parbeRin" and '~arend") that come into action
after the surrounding universe has been introduced
and initialized.
begin semaphore mutex; mutex := I;
parbegin
beqin L I : P(mutex); critical section I; V(mutsx);
remainder of cycle I; ~oto LI
end;
beqin L2: P(mutex); critical section 2; V(mutex);
remainder of cycle 2; qoto L2
end
parend
end
As a result of the P- and V-operations on

"mutex" the actions, marked as "critical sections"
exclude each other mutually in time; the scheme
given allows straightforward extension to more than
two parallel processes, the maximum value of mutex
= I, the minimum value = - (n - I) if we have n
parallel processes.
Critical sections are used always and only for
the purpose of unambiguous inspection and modification of the state variables (allocated in the
surrounding universe) that describe the current
state of the system (as far as needed for the
regulation of the harmonious co-operation between
the various processes).
The Private Semaphores.
Each sequential process has associated with it
a number of private semaphores and no other process
will ever perform a P-operation on them. The universe
initializes them with the value = O, their maximum
value = I, their minimum value = - I.
Whenever a process reaches a stage where the
permission for dynamic progress depends on current
values of state variables, it follows the pattern:

P(mutex);
"inspection and modification of state variables
including a conditional V(private semaphore)";

V(mutex);
P(private semaphore)
If the inspection learns that the process in
question should continue, it performs the operation
"V(private semaphore)" -the semaphore value then
changes from 0 to I-, otherwise this V-operation
is skipped, leaving to the other processes the
obligation to perform this V-operation at a suitable
moment. The absence or presence of this obligation
is reflected in the final values of the state
variables upon leaving the critical section.
Whenever a process reaches a stage where as a
result of its progress possibly one (or more)
blocked processes should now get permission to
continue, it follows the pattern

P(mutex);
"modification and inspection of state variables
including zero or more V-operations on private
semaphores of other processes";

V(mutex)
By the introduction of suitable state variables
and appropriate programming of the critical sections
any strategy assigning peripherals, buffer areas etc.
can be implemented.
The amount of coding and reasoning can be
greatly reduced by the observation that in the two
complementary critical sections sketched above, the
same inspection can be performed by the introduction
of the notion of "an unstable situation", such as
a free reader and a process needing a reader.
Whenever an unstable situation emerges it is
removed (including ome or more V-operations on

private semaphores) in the very same critical
section in which it has been created.
Provinq the Harmonious Co-operation.
The sequential processes in the system can all
be regarded as cyclic processes in which a certain
neutral point can be marked, the so-called "homing
position", in which all processes are when the
system is at rest.
When a cyclic process leaves its homing position
"it accepts a task", when the task has been performed
end not earlier, the process returns to its homing
position. Each cyclic process has a specific task
processing power (e.g. the execution of a user
program or unbuffering a portion of printer output,
etc.)
The harmonious co-operation is mainly proved
in roughly three stages.
I)
It is proved that although a process
performing a task may generate in doing so a finite
number of tasks for other processes, a single
initial task cannot give rise to an infinite number
of task generations. The proof is simple as
processes can only generate tasks for processes
at lower levels of the hierarchy so that circularity
is excluded. (If a process needing a segment from
the drum has generated a task for the segment
controller, special precautions have been taken to
ensure that the segment asked for remains in core
at least until the requesting process has effectively
accessed the segment concerned. Without this precaution finite tasks could be forced to generate an
infinite number of tasks for the segment controller
and the system could get stuck in an unproductive
page flutter.)
2)
It is proved that it is impossible
that all processes have returned to their homing
position while somewhere in the system is still
pending a generated but unaccepted task. (This is
proved via instability of the situation just
described.)
3)
It is proved that after the acceptance
of an initial task all processes eventually will be
(again) in their homing position. Each process
blocked in the course of task execution relies on the
other processes for removal of the barrier. Essentially, the proof in question is a demonstratmon of the
absence of "circular waits": process P waiting for
process Q waiting for process R waiting for process
P. (Our usual term for the circular wait is "the
Deadly Embrace".) In a more general society than
our system this proof turned out to be e proof by
induction (on the level of hierarchy, starting at
the lowest level) as A.N.Habermann has shown in his
doctoral thesis.

THE WORKING SET MODEL FOR PROGRAM BEHAVIOR
Peter J. Denning
Massachusetts Institute of Technology
Cambridge, Massachusetts

SUMMARY

We claim neither is adequate.

Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate
model for program behavior.
In this paper a new
model is developed, the "working set model", which
enables us to decide which information is in use
by a running program and which is not.
Such knowledge is vital for dynamic management of paged
memories.
The working set of pages associated
with a process, defined to be the collection of its
most recently used pages, is a useful allocation
concept. A proposal for an easy-to-implement
allocation policy is set forth; this policy is
unique, inasmuch as it blends into one decision
function the heretofore independent activities of
process-scheduling and memory-management.

Because resources are multiplexed, each user
is given the illusion that he has a complete computing system at his sole disposal: a virtual
computer.
For our purposes, the basic elements of
a virtual computer are its virtual processor and
an "infinite" one-level virtual memory.
Dynamic
"advice" regarding resource requirements cannot be
obtained successfully from users for several
reasons:
i. A user may build his program on the work
of others, frequently sharing procedures
whose time and storage requirements may be
either unknown or, because of data dependence, indeterminate.
Therefore he cannot
be expected to estimate processor-memory
needs.
2. It is not clear what sort of "advice" might
be solicited.
Nor is it clear how the
operating system should use it, for overhead incurred by using advice could well
negate any advantages attained.
3. Any advice acquired from a user would be
intended (by him) to optimize the environment for his own program.
Configuring
resources to suit individuals may interfere
with overall good service to the community
of users.
Thus it seems inadvisable at the present time to
permit users, at their discretion, to advise the
operating system of their needs.

INTRODUCTION
Resource allocation is tricky business.
In
recent years there has been much dialogue on the
topics of process scheduling and core memory management, yet development of techniques has progressed independently along both these lines. No
one will deny that a unified approach is needed.
Probably the most basic reason behind the absence
of a general treatment is the lack of an adequate
model for program behavior.
In this paper we develop a new model, the working set model, which
embodies certain important behavioral properties
of computations operating in a multiprogrammed environment, enabling us to decide which information
is in use by a running program and which is not.
We do not intend that the proposed model be considered "final"; rather, we hope to stimulate a
new kind of thinking, thinking that may be of considerable help in solving many operating system
design problems.
The working set is intended to model the behavior of programs in the general purpose computer
system, or computer utility.
For this reason we
assume that the operating system must on its own
determine the behavior of programs it runs; it
cannot count on outside help.
Two commonly proposed sources of externally-supplied dynamic allocation information are the user and the compiler.

Work reported herein was supported in part by
Project MAC, an M.I.T. research project sponsored
by the Advanced Projects Research Agency, Dept.
of Defense, under Office of Naval Research Contract Number Nonr-4102(Ol).

Likewise, compilers cannot be expected to
supply information, extracted from the structure
of the program , regarding resource requirements:
i. Programs will be modular in construction;
information about other modules may be unavailable at compilation time.
Because of
dependence on data there may be no way to
decide (until run time) just which modules
will be included in a computation.
2. Compilers cluttered with extra machinery
to predict memory needs will be slower in
operation.
Many users are less interested
in whether their programs operate efficiently than whether they operate at all, and
so are concerned with rapid compilation.
Furthermore, the compiler is an often-used
component of the operating system; if slow
and bulky, it can be a serious drain on
system resources.

**Ramamoorthy I has put forth a proposal for automatic segmentation of programs during compilation.

Therefore in this paper we are advocating mechanisms that monitor the behavior of a computation,
making allocation decisions on the basis of currently observed characteristics.
Only a mechanism
that oversees the behavior of a program in operation can cope with arbitrary interconnections of
arbitrary modules having arbitrary characteristics.
Our treatment proceeds as follows. First we
define the type of computer system in which our
ideas are developed. After a brief discussion of
previous work with the problems of dynamic memory
management, we define the working set model. We
discuss a method of implementing memory management
based on this model, and indicate how working set
notions can be used to blend process scheduling
and memory management into one decision function,
accounting simultaneously for both types of demand.
Finally we discuss how data sharing fits into the
working-set scheme.

THE FRAMEWORK
We assume that the reader is already familiar
with the concepts of a computer utility 2'3'4, of
segmentation and paging 5'6, of program and addressing structure 6'8, so we will only mention these
topics here. Briefly, each process has access to
its own private, segmented name space; each segment known to the process is sliced into equalsize units, called pages, to facilitate mapping it
into the paged main memory.
Associated with each
segment is a page table, whose entries point to

the segment's pages. An "in-core" bit in each
page table entry is turned ON whenever the designated page is present in main memory ; an attempt
to reference a page whose "in-core" bit is OFF
causes a page fault, initiating proceedings to
secure the missing page. Finally, a process has
three states of existence: running, when a processor is assigned to it; ready, when it would be
running if only a processor were available; or
blocked, when it has no need of a processor (for
example, during a page fault or during a console
interaction). When talking about processes in execution, we will have to distinguish between "process time" and "real time". Process time is time
as seen by a process unaware it is suspended; that
is, as if it executed without interruptions.
We restrict attention to a two-level memory
system, indicated by Figure I. Only data residing
in main memory is accessible to a processor; all
other data reside in auxiliary memory, which we
regard as having i nflnite capacity. There is a
time T, the traverse time, involved in transferring
a page between memories. T is measured from the
moment a page fault occurs until the moment the
missing page is in main memory ready for use. T
is actually the expectation of a random variable
composed of waits in queues and mechanical positioning delays.
Though it usually takes less time to
store into auxiliary memory than to read from it,
we shall regard the traverse time T to be the same
regardless of which direction a page is moved.
,
Consistent with current usage, we will use the
terms "core memory" and "main memory" interchangeably.

PROCESSORS

~

~ ~

~ n

data flow from main memory
(controlled b y ~, c o r emanager)
I

main memÂ°rY(contrD~edfl~ ~ ~ ~ n m ~ [ c Y i e s )

FIGURE i.

Two-level memory system.

A basic allocation problem, "core memory management", is that of deciding just which pages are to
occupy main memory.
The basic strategy advocated
here -- a compromise against a lot of expensive
,
main memory -- is to minimize page traffic . There
are two reasons for this:
i. The more the data traffic between the two
levels of memory, the more the computational overhead in deciding just what to move
and where to move it.
2. Because the traverse time T is long compared
to a memory cycle, too much data movement
can result in congestion and serious interference with processor efficiency.
Roughly speaking, a working set of pages is
the minimum collection of pages that must be loaded
in main memory for a process to operate efficiently,
without "unnecessary" page faults. According to
our definitions, a "process" and its "working set"
are but two manifestations of the same ongoing
computational activity.

PREVIOUS WORK
In this section we outline strategies that have
been set forth in the past for memory management;
the interested reader will be referred to the literature for detail.
We regard management of paged memories to operate in two stages:
I. Pagin_g in." locate the required page in
auxiliary memory, load it into main memory,
turn the "in-core" bit of the appropriate
page table entry ON.
2. Paging out: remove some page from main memory, turn the "in-core" bit of the appropriate page table entry OFF.
Management algorithms can be classified according
to their methods of paging in and paging out.
It
is a common characteristic of nearly every strategy
that paging in is done on demand; that is, no action
is taken to load a page into memory until some
process attempts to reference it. To date there
have been no proposals recommending look-ahead, or
anticipatory page-loading, because (as we have
stressed) there is no reliable advance source of
allocation information, be it the programmer or
the compiler.
Although the working set is the
desired information, it might still be futile to
pre-load pages: there is no guarantee that a process will not block shortly after resumption,
having referenced only a fraction of its working
set.
The operating system could devote its already precious time to activities more rewarding
than loading pages which may not be used.
Thus we
will assume that paging in is done on demand only,
via the page fault mechansim.

Since data is stored and transmitted in units of
pages, we can (without ambiguity) refer to data
movement as "page traffic".

The chief problem in memory management is not
deciding which pages to load; it is deciding which
pages ought to be removed.
For if the page with
the least likelihood of being used in the immediate
future is retired to auxiliary memory, the best
choice has been made.
Nearly every worker in the
field has recognized this.
Debate has arisen over
which strategy to employ for retiring pages; that
is, which page-turning, or replacement, algorithm
to use.
A good measure of performance for a paging
policy is page traffic (the number of pages per
unit time being moved between memories), since
erroneously removed pages add to the traffic of
returning pages.
In the following we will use this
as a basis of comparison for several strategies.
Random selection.
Whenever a fresh page of memory
is needed, a page is selected at random to be replaced.
Although utterly simple to implement, this
method frequently removes useful pages (which must
therefore be recalled) and so results in high page
traffic.
~!%~
selection. The pages of main memory are ordered in a cyclic list.
Suppose the M pages of
main memory are numbered 0,1,...,(M-I) and a
pointer k indicates that the k-th page was most
recently paged in. Whenever a fresh page of memory
is needed, [(k+l) mod M] 4 k, page k is retired,
and another page brought in to fill the now vacant
slot.
This method -- also utterly simple to realize -- is based on the principle that programs tend
to follow sequences of instructions, so that references in the immediate future will most likely
be close to present references.
So, assuming there
is this tendency for page references to cluster,
and assuming some kind of uniformity in process
scheduling techniques, the page which has been in
memory longest is least likely to be reused: hence
the cyclic list. We see two ways in which this
algorithm can fail. First we question its basic
assumption.
It is not at all clear that modular
programs, which execute numerous inter-module calls,
will indeed exhibit sequential instruction fetch
patterns.
The thread of control will not string
pages together; rather, it will entwine them intricately.
[Fine, Mclssac, and Jackson 9 have some
experimental evidence in support of this reasoning.]
Second, this algorithm is subject to overloading when used in multiprogrammed memories.
When core demand is too heavy, one cycle of the
list completes rapidly and the pages deleted are
still needed by their processes.
This can create
a self-intensifying crisis.
Programs, deprived of
still-needed pages, generate a plethora of page
faults; the resulting traffic of returning pages
displaces still other useful pages, leading to more
page faults, and so on.
Oldest-unused selection.
Each page table entry
contains a ~us~ ~ bit, set ON each time the page is
referenced.
At periodic intervals all the page
table entries are searched and usage records updated.
When a fresh page of memory is needed, the page unreferenced for the longest time is removed.
One can

see that this method is intrinsically reasonable by
considering the simple case of a computer where
there is exactly one process whose pages cannot all
fit into main memory.
In this case the most reasonable choice for a page to replace is the oldest
unused page.
Unfortunately this method too is susceptible to overloading when many processes compete
for main memory.
ATLAS ioo~ detection method.

The Ferranti ATLAS

computer I0 had proposed a page-turning policy that
attempted to detect loop behavior in page reference
patterns, then minimize page traffic by maximizing
the time between page transfers, that is, by removing pages not expected to be needed for the longest time.
It was successful -- only for looping
programs.
Performance was unimpressive for programs exhibiting random reference patterns.
Implementation was costly.
Various studies concerning behavior of paging
algorithms have appeared.
Fine, Mclssac and
Jackson 9 have investigated the effects of demand
paging and have questioned whether paging is beneficial at all. We do not feel that their conclusion applies to the kind of multiprogrammed environment we have described.
They studied fixed-size
programs, that quickly acquired and retained a
large fraction of their pages.
Highly interactive,
modular programs are likely to behave differently.
Not only may program size vary dynamically (according to data dependencies), but also such programs
should be using a small fraction of their pages at
any one time, and the membership in this set of
working pages should be changing constantly.

We define the working set of information W(t,T)
of a process at time t to be the collection of data
items referenced by the process during the proces
time interval (t-T~,t).
Thus, the data items a process has referenced
during the last T seconds of its execution comprise
its working set. q will be called the working set
parameter.
We will regard the data items in W(t,T)
as being pages ,although they could just as well
be any other named data objects.
The working set
size ~(t,~) is
(I)

~(t,T)

=

Number of pages in W(t,T)

A working set W(t,~) has two important, general
properties.
Both are properties of typical programs,
and need not hold in special cases.
PI. Size. It should be clear immediately that
~(t,0) = 0 since no page reference can occur
in zero time.
It should also be clear that
~(t,T) as a function of T is monotonically
increasing, since more pages can be referenced
in longer time intervals.
Because a process
will refer to its most-needed pages frequently
and its least-needed pages infrequently, we
expect ~(t,~) as a function of T to have a
steep initial rise which diminishes to a more
gradual rise.
The general character of ~(t,~)
is suggested by the smoothed curve of Figure 2.
00(t ,~-)

Belady II has compared some of the algorithms
mathematically.
His most important conclusion is
that the "ideal" algorithm should possess much of
the simplicity of Random or Cyclic selection (for
efficiency) and some, though not much, accumulation
of data on past reference patterns.
He has shown
that too much "historical" data can have adverse
effects (witness ATLAS).
In the next section we begin investigation of
the working set concept.
Even though the ideas
are not entirely new 12'13'14, there has been no
detailed documentation publicly available.

THE WORKING SET MODEL
From the programmer's standpoint, the working
set of information is the smallest collection of
procedure and data items that must be present in
main memory to assure efficient execution of his
program.
We have already stressed that there will
be no advance notice from either the programmer or
the compiler regarding what information "ought" to
be in main memory.
It is up to the operating
system to determine on the basis of page reference
patterns whether pages are in use.
Therefore the
working set of information associated with a process is, from the system standpoint, the set of
most recently referenced pages.

FIGURE 2.

Behavior of ~(t,T).

Program modularity enables us to
P2. Correlation.
say something about correlation between the
size of a working set at two times, t and (t+c~).
Correlation is useful in devising storage
allocators, for the higher the correlation
between ~(t,T) and w(t+c~,T), the better is
~(t,T) a prediction of ~(t+c~,T).
In modular
programs, control passes randomly from one
module to another; if T is chosen properly
(as discussed in the next section), it is more
likely that a working set will change size
smoothly, less likely that it will change size
abruptly.
Thus for small time separations
(say, 6 < < T), ~(t,~) and ~(t+c~,T) are highly

correlated, meaning that a measurement of ~(t,T)
will serve as a good estimate of the memory requirement during the process time interval (t,t+c0.
For large time separations ~ (say, ~ >> .[), control
will have passed through a great many modules
during the interval (t,t+c~); thus ~(t,T ) gives
little information about ~(t+C~,7), and so ~(t,7)
and ~(t+c~,7) have much less correlation than for
small 5. This behavior is suggested in Figure 3.
Correlation between
~(t,7) and ~(t+Cz,7)

Detecting W(t~T )
According to our definition, W(t,T) is the set
of its pages a process has referenced within the
last T seconds of its execution.
This suggests
that memory management can be controlled by hardware mechanisms, by associating with each page of
main memory a timer.
Each time a page is referenced, its timer is set to T and begins to run
down; if the timer succeeds in running down, a flag
is set to mark the page for removal whenever the
space is needed.
In the appendix we describe such
a hardware memory management mechanism, hardware
that can be housed within the memory boxes.
The
mechanism has two interesting features:
i. It operates asynchronously and independently
of the supervisor, whose only respsonsibility
in memory management is handling page faults.
Quite literally, memory manages itself.
2. Analog devices such as capacitative timers
could be used to measure intervals.

~D

c~
0
FIGURE 3.

Correlation between working set sizes,

Choice of T

Unfortunately it is not practical to add on
hardware to existing systems. We seek a method of
handling memory management within the software.
The procedure we propose here samples the page
table entries of pages in core memory at process
time intervals of ~ seconds (~ is called the
sampling interval) where ~ = y/K , K an integer
constant chosen to make the sampling intervals as
"fine grain" as desired.
On the basis of page
references during each of the last K sampling intervals, the working set W(t,l~) can be determined,
as follows.

The value ultimately selected for ~ will reflect
efficiency requirements and will be influenced by
"in- core"
system parameters such as core memory size and memI
ory traverse time.
For example, if T is too small,
pages may be removed from main memory while they
are still useful, and high page traffic may result
from returning pages.
If T is too large, pages
may remain in main memory long after last being
used, and wasted main memory may result.
Thus the
value of T will have to represent a compromise between too much page traffic and too much wasted
memory space.
0

l

pointer
to page

use bits

luol-,l

---

TYPICAL PAGE TABLE ENTRY

SHIFT AT END OF SAMPLING INTERVAL
The following consideration leads us to recommend for T a value comparable to the memory traverse time T (Figure I). Consider a process that
is running continuously, being interrupted only
for page faults.
Assuming that memory allocation
procedures balk at removing from main memory any
page in a working set, once a page has entered
W(t,T) it will remain in main memory for at least
T seconds.
Under the very worst of page-shuffling
conditions, a page could be dispatched to auxiliary memory and be recalled immediately; the time
for this round trip is two traverse times, 2T.
Therefore a highly-shuffled page would spend
roughly T/2T of its time in main memory.
So,
for example, if we wished to insure that a page is
available in main memory (when needed) for not less
than 50 per cent of the time, we would have to
choose â¢ ~ 2T.

FIGURE 4.

Page table entries for detecting W(t,Kcy).

As indicated by Figure 4, each page table entry
contains an "in-core" bit M, where M=i if and only
if the page is present in main memory.
It also
contains a string of use bits u0,ul,...,u K. Each
time a page reference occurs,

I ~ u 0.

At the end

of each sampling interval ~, the bit pattern contained in u0,ul,...,u K is shifted one position,
a 0 enters u 0, and u K ipapers.txt -> [(0 -> (0, [0x6322a15a747c]))]]
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a7480, 0x6322a15a7484, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a7488, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a748c, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
clock_q: 
(2, 3, 1)
alloc: 1
epage: 1
file_read	(·-!boe!pee addresses used
where an even address is required. Such faults cause the
processor to trap to a system routine. When an illegal
action is caught, unless other arrangements have been
made, the system terminates the process and writes
the user's image on file core in the current directory. A
debugger can be used to determine the state of the
program at the time of the fault.
Programs which are looping, which produce unwanted output, or about which' the user has second
thoughts may be halted by the use of the interrupt
signal, which is generated by typing the "delete"
character. Unless special action has been taken, this
signal simply causes the program to cease execution
without producing a core image file.
There is also a quit signal which is used to force a
core image to be produced. Thus programs which loop
unexpectedly may be halted and the core image examined without prearrangement.
The hardware-generated faults and the interrupt and
quit signals can, by request, be either ignored or caught
by the process. For example, the Shell ignores quits to"
prevent a quit from logging the user out. The editor
catches interrupts and returns to its c o m m a n d level.
This is useful for stopping long printouts without losing
work in progress (the editor manipulates a copy of
the file it is editing). In systems without floating point
hardware, unimplemented instructions are caught, and
floating point instructions are interpreted.

8. Perspective
Perhaps paradoxically, the success of UNIX is largely
due to the fact that it was not designed to meet any
predefined objectives. The first version was written
when one of us (Thompson), dissatisfied with the
available computer facilities, discovered a little-used
Communications
of
the ACM

July 1974
Volume 17
Number 7

PDP-7 and set out to create a more hospitable environment. This essentially personal effort was sufficiently
successful to gain the interest of the remaining author
and others, and later to justify the acquisition of the
POP-11/20, specifically to support a text editing and
formatting system. When in turn the 11/20 was outgrown, UNIX had proved useful enough to persuade
management to invest in the PDP-11/45. Our goals
throughout the effort, when articulated at all, have
always concerned themselves with building a comfortable relationship with the machine and with exploring
ideas and inventions in operating systems. We have
not been faced with the need to satisfy someone else's
requirements, and for this freedom we are grateful.
Three considerations which influenced the design
of UNIX are visible in retrospect.
First, since we are programmers, we naturally
designed the system to make it easy to write, test, and
run programs. The most important expression of our
desire for programming convenience was that the
system was arranged for interactive use, even though
the original version only supported one user. We bebelieve that a properly-designed interactive system is
much more productive and satisfying to use than a
" b a t c h " system. Moreover such a system is rather
easily adaptable to noninteractive use, while the converse is not true.
Second, there have always been fairly severe size
constraints on the system and its software. Given the
partially antagonistic desires for reasonable efficiency
and expressive power, the size constraint has encouraged
not only economy but a certain elegance of design.
This may be a thinly disguised version of the "salvation through suffering" philosophy, but in our case it
worked.
Third, nearly from the start, the system was able to,
and did, maintain itself. This fact is more important
than it might seem. If designers of a system are forced
to use that system, they quickly become aware of its
functional and superficial deficiencies and are strongly
motivated to correct them before it is too late. Since
all source programs were always available and easily
modified on-line, we were willing to revise and rewrite
the system and its software when new ideas were
invented, discovered, or suggested by others.
The aspects of UNIX discussed in this paper exhibit
clearly at least the first two of these design considerations. The interface to the file system, for example, is
extremely convenient from a programming standpoint.
The lowest possible interface level is designed to
eliminate distinctions between the various devices and
files and between direct and sequential access. N o
large "access method" routines are required to insulate
the p r o g r a m m e r from the system calls; in fact, all
user programs either call the system directly or use a
small library program, only tens of instructions long,
which buffers a number of characters and reads or
writes them all at once.

Another important aspect of programming convenience is that there are no "control blocks" with a
complicated structure partially maintained by and depended on by the file system or other system calls.
Generally speaking, the contents of a program's address
space are the property of the program, and we have
tried to avoid placing restrictions on the data structures
within that address space.
Given the requirement that all programs should be
usable with any file or device as input or output, it is
also desirable from a space-efficiency standpoint to push
device-dependent considerations into the operating system itself. The only alternatives seem to be to load
routines for dealing with each device with all programs,
which is expensive in space, or to depend on some means
of dynamically linking to the routine appropriate to
each device when it is actually needed, which is expensive either in overhead or in hardware.
Likewise, the process control scheme and c o m m a n d
interface have proved both convenient and efficient.
Since the Shell operates as an ordinary, swappable user
program, it consumes no wired-down space in the
system proper, and it may be made as powerful as
desired at little cost. In particular, given the framework .
in which the Shell executes as a process which spawns
other processes to perform commands, the notions of
I/O redirection, background processes, c o m m a n d flies,
and user-selectable system interfaces all become essentially trivial to implement.

374

Communications
of
the ACM

8.1 Influences

The success of'UNIX lies not so much in new inventions but rather in the full exploitation of a carefully
selected set of fertile ideas, and especially in showing
that they can be keys to the implementation of a small
yet powerful operating system.
The fork operation, essentially as we implemented it,
was present in the Berkeley time-sharing system [8]. On
a number of points we were influenced by Multics, which
suggested the particular form of the I / o system calls
[9] and both the name of the Shell and its general functions. The notion that the Shell should create a process
for each c o m m a n d was also suggested to us by the
early design of Multics, although in that system it was
later dropped for efficiency reasons. A similar scheme
is used by TENEX [10].

9. Statistics

The following statistics from UNIX are presented to
show the scale of the system and to show how a system
of this scale is used. Those of our users not involved in
document preparation tend to use the system for program development, especially language work. There are
few important "applications" programs.

July 1974
Volume 17
Number 7

9.1 Overall
72 user population
14 maximum simultaneous users
300 directories
4400 files
34000 512-byte secondary storage blocks used
9.2 Per day (24-hour d a y , 7 - d a y week basis)
T h e r e is a " b a c k g r o u n d " process t h a t runs at the
lowest possible p r i o r i t y ; it is used to s o a k u p a n y idle
c P u time. It has been used to p r o d u c e a m i l l i o n - d i g i t
a p p r o x i m a t i o n to the c o n s t a n t e - 2, a n d is n o w
g e n e r a t i n g c o m p o s i t e p s e u d o p r i m e s (base 2).
1800 commands
4.3 CPU hours (aside from background)
70 connect hours
30 different users
75 logins

9.3 Command CPU Usage (cut off at 1%)
15.7% Ccompiler
15.2% users' programs
11.7% editor
5.8% Shell (used as a cornmand, including command times)
5.3% chess
3.3% list directory
3.1% document formatter
1.6% backup dumper
1.8% assembler

1.7%
1.6%
1.6%
1.6%
1.4%
1.3%
1.3%
1.1%
1.0%

Fortran compiler
remove file
tape archive
file system consistency
check
library maintainer
concatenate/printfiles
paginate and print file
print disk usage
copy file

9.4 Command Accesses (cut off at 1%)
15.3%
9.6%
6.3%
6.3%
6.0%
6.0%
3.3%
3.2%
3.1%
1.8%
1.8%
1.6%

editor
list directory
remove file
C compiler
concatenate/printfile
users' programs
list people logged on
system
rename/move file
file status
library maintainer
document formatter
execute another command conditionally

1.6%
1.6%
1.5%
1.4%
1.4%
1.4%
1.2%
1.1%
1.1%
1.1%

debugger
Shell (used as a command)
print disk availability
list processes executing
assembler
print arguments
copy file
paginate and print file
print current date/time
file system consistency
check
1.0% tape archive

ficulties such as p o w e r d i p s a n d i n e x p l i c a b l e p r o c e s s o r
i n t e r r u p t s to r a n d o m locations. T h e r e m a i n d e r are
s o f t w a r e failures. T h e longest u n i n t e r r u p t e d up time
was a b o u t two weeks. Service calls average one every
t h r e e weeks, b u t are h e a v i l y clustered. T o t a l up time
has been a b o u t 98 p e r c e n t o f o u r 24-hour, 365-day
schedule.
Acknowledgments. W e are grateful to R . H . C a n a d a y ,
L.L. Cherry, a n d L.E. M c M a h o n for their c o n t r i b u tions to uNIX. W e are p a r t i c u l a r l y a p p r e c i a t i v e o f the
inventiveness, t h o u g h t f u l criticism, a n d c o n s t a n t supp o r t o f R. M o r r i s , M . D . M c I l r o y , a n d J.F. O s s a n n a .

References
1. Digital Equipment Corporation. PDP-I1/40 Processor
Handbook, 1972, and PDP-I1/45 Processor Handbook, 1971.
2. Deutsch, L.P., and Lampson, B.W. An online editor. Comm.
ACM 10, 12 (Dec. 1967), 793-799, 803.
3. Richards, M. BCPL: A tool for compiler writing and system
programming. Proc. AFIPS 1969 SJCC, Vol. 34, AFIPS Press,
Montvale, N.J., pp. 557-566.
4. McClure, R.M. TMG--A syntax directed compiler. Proc.
ACM 20th Nat. Conf., ACM, 1965, New York, pp. 262-274.
5. Hall, A.D. The M6 macroprocessor. Computing Science Tech.
Rep.#2, Bell Telephone Laboratories, 1969.
6. Ritchie, D.M. C reference manual. Unpublished memorandum,
Bell Telephone Laboratories, 1973.
7. Aleph-null. Computer Recreations. So[?ware Practice and
Experience 1, 2 (Apr.-June 1971), 201-204.
8. Deutsch, L.P., and Lampson, B.W. SDS 930 time-sharing
system preliminary reference manual. Doc. 30.10.10, Project G ENI E,
U of California at Berkeley, Apr. 1965.
9. Feiertag, R.J., and Organick, E.I. The Multics input-output
system. Proc. Third Syrup. on Oper. Syst. Princ., Oct. 18-20, 1971,
ACM, New York, pp. 35-41.
10. Bobrow, D.G., Burchfiel, J.D., Murphy, D.L., and Tomlinson,
R.S. TENEX, a paged time sharing system tbr the PDP-10. Comm.
ACM15, 3 (Mar. 1972), 135-143.

9.5 Reliability
O u r statistics on reliability are m u c h m o r e subjective
t h a n the others. T h e f o l l o w i n g results are true to the
best o f o u r c o m b i n e d recollections. T h e t i m e s p a n is
over one y e a r with a very early vintage 11/45.
T h e r e has been one loss o f a file system (one d i s k
o u t o f five) caused b y s o f t w a r e i n a b i l i t y to c o p e with
a h a r d w a r e p r o b l e m c a u s i n g r e p e a t e d p o w e r fail traps.
Files on t h a t d i s k were b a c k e d up three days.
A " c r a s h " is an u n s c h e d u l e d system r e b o o t o r
halt. T h e r e is a b o u t one crash every o t h e r d a y ; a b o u t
t w o - t h i r d s o f t h e m are c a u s e d by h a r d w a r e - r e l a t e d dif-

375

Communications
of
the ACM

July 1974
Volume 17
Number 7

THE STRUCTURE OF THE "THE"-MULTIPROGRAMMING SYSTEM

EWDI 96

Edsger W.Dijkstra
Technological University
EINDHOVEN
The Netherlands

Summary
A multiprogramming system is described in
which all activities are divided over a number of
sequential processes. These sequential processes
are placed at various hierarchical levels, in each
of which one or more independent abstractions have
been implemented. The hierarchical structure proved
to be vital for the verification of the logical
soundness of the design and the correctness of its
implementation.
Introduction
Papers "reporting on timely research and
development efforts" being explicitly asked for, I
shall try to present a progress report on the multiprogramming effort at the Department of Mathematics
at the Technological University, Eindhoven, the
Netherlands.
Having very limited resources (viz. a group of
six people of, on the average, half time availability) and wishing to contribute to the art of system
design -including all the stages of conception,
construction and verification- we are faced with the
problem of how to get the necessary experience. To
solve this problem we have adopted the following
three guiding principles:
I) Select a project as advanced as you can
conceive, as ambitious as you can justify, in the
hope that routine work can be kept to a minimum;
hold out against all pressure to incorporate such
system expansions that would only result into a
purely quantitative increase of the total amount of
work to be done.
2) Select a machine with sound basic characteristics (e.g. an interrupt system to fall in love
with is certainly an inspiring feature); from then
onwards try to keep the specific properties of the
configuration for which you are preparing the system
out of your considerations as long as possible.
3) Be aware of the fact that experience does by
no means automatically lead to wisdom and understanding; in other words, make a conscious effort to
learn as much as possible from your precious
experiences.
Accordingly, I shall try to go beyond just
reporting what we have done and how, and I shall
try to formulate as well what we have learned.
I should like to end the introduction with two
short remarks on working conditions, remarks I make
for the sake of completeness. I shall not stress
these points any further.

The one remark is that production speed is
severely degraded if one works with half time people
who have other obligations as well. This is at least
a factor four, probably it is worse. The people
themselves lose time and energy in switching over,
the group as a whole loses decision speed as discussions~ when needed, have often to be postponed
until all people concerned are available.
The other remark is that the members of the
group (mostly mathematicians) have previously
enjoyed as good students a university training of
5 to 8 years and are of Master's or Ph.D. level.
I mention this explicitly because at least in my
country the intellectual level needed for system
design is in general grossly underestimated. I am
more than ever convinced that this type of work is
just difficult and that every effort to do it with
other than the best people is doomed to either
failure or moderate success at enormous expenses.
The Tool and the Goal
The system has been designed for a Dutch
machine, the EL X8 (N.V.Electrologica, Rijswijk
(ZH)). Characteristics of our configuration are:
I) core memory cycle time 2.5 mms., 27 bits; at
present 32K.
2) drum of 512K words, 1024 words per track, rev.
time 40 ms.
3) an indirect addressing mechanism very well
suited for stack implementation
4) a sound system for commanding peripherals and
controlling of interrupts
5) a potentially great number of low capacity
channels; ten of them are used (3 paper tape readers
at 1000 char/sac; 3 paper tape punches at 150 char/
sec; 2 teleprinters; a plotter; a line printer)
6) absence of a number of not unusual awkward
features.
The primary goal of the system is to process
smoothly a continuous flow of user programs as a
service to the University. A multiprogramming
system has been chosen with the following objectives
in mind:
I) a reduction of turn around time for programs of
short duration
2) economic use of peripheral devices
3)automatic control of backing store to be combined
with economic use of the central processor
4) the economic feasibility to use the machine for
those applications for which only the flexibility of

a general purpose computer is needed but (as a r u l e )
not the capacity nor the processing power.

The system is not intended as a multi-access
system. There is no common data base via which
independent users can communicate with each other:
they only share the configuration and a procedure
library (that includes a translator for ALGOL 60
extended with complex numbers). The system does not
cater for user programs written in machine language.
Compared with larger efforts one can state that
quantitatively speaking the goals have been set as
modest as the equipment and our other resources.
Qualitatively speaking, I am afraid, we got more and
more immodest as the work progressed.

minutes (classical) inspection at the machine and
each of them correspondingly easy to remedy. At the
moment of writing the testing is not yet completed,
but the resulting system will be guaranteed to be
flawless. When the system has been delivered we shall
not live in the perpetual fear that a system derailment may still occur in an unlikely situation such as
might result from an unhappy "coincidence" of two or
more critical occurrences, for we shall have proved
the correctness of the system with a rigour and
explicitness that is unusual for the great majority
of mathematical proofs.
A Survey of the System Structure

A Progress Report
Storage Allocation.
We have made some minor mistakes of the usual
type (such as paying too much attention to speeding
up what was not the real bottle neck) and two major
ones.
Our f i r s t
m a j o r m i s t a k e has been t h a t f o r t o o
long a time we confined our attention to "a perfect
installation": by the time we considered how to make
the best of it when, say, one of the peripherals
broke down, we were faced with nasty problems.
Taking care of the "pathology" took more energy than
we had expected and part of our troubles were a
direct consequence of our earlier ingenuity, i.e.
the complexity of the situation into which the system
could have manoeuvred itself. Had we paid attention
to the pathology at an earlier stage of the design,
our management rules would certainly have been less
refined.

The second major mistake has been that we
conceived and programmed the major part of the system
without giving more than scanty thought to the problem of debugging it. For the fact that this mistake
had no serious consequences -on the contrary~ one
might argue as an afterthought- I must decline all
credit. I feel more like having passed through the
eye of the needle...
As captain of the crew I had had extensive
experience (dating back to 1958) in making basic
software dealing with real time interrupts and I
knew by bitter experience that as a result of the
irreproducibility of the interrupt moments, a program
error could present itself misleadingly like an
occasional machine malfunctioning. As a result I was
terribly afraid. Having fears regarding the possibility of debugging we decided to be as careful as
possible and -prevention is better than cure~- to
try to prevent nasty bugs from entering the construction.
This decision, inspired by fear, is at the
bottom of what I regard as the group's main contribution to the art of system design. We have found
that it is possible to design a refined multiprogramming system in such a way that its logical soundness
can be proved a priori and that its implementation
admits exhaustive testing. The only errors that
showed up during testing were trivial coding errors
(occurring with a density of one error per 500
instructions), each of them located within 10

In the classical yon Neumann machine information
is identified by the address of the memory location
containing the information. When we started to think
about the automatic control of secondary storage
we were familiar with a system (viz. GIER ALGOL)
in which all information was identified by its
drum address (as in the classical yon Neumann
machine) and in which the function of the core
memory was nothing more than to make the information
"page wise" accessible.
We have followed another approach and as it
turned out, to great advantage. In our terminology
we made a strict distinction between memory units
(we called them "pages" and had "core pages" and
"drum pages") and corresponding information units
(for lack of a better word we called them "segments")
a segment just fitting in a page. For segments we
created a completely independent identification
mechanism in which the number of possible segment
identifiers is much larger than the total number of
pages in primary and secondary store. The segment
identifier gives fast access to a so-called "segment
variable" in core whose value denotes whether the
segment is still empty or not and if not empty, in
which page (or pages) it can be found.
As a consequence of this approach: if a segment
of information, residing in a core page, has to be
dumped onto the drum in order to make the core page
available for other use, there is no need to return
the segment to the same drum page as it originally
came from. In fact, this freedom is exploited: among
the free drum pages the one with minimum latency
time is selected.
A next consequence is the total absence of a
drum allocation problem: there is not the slightest
reason why, say, a program should occupy consecutive
drum pages. In a multiprogramming environment this
is very convenient.
Processor Allocation.
We have given full recognition to the fact that
in a single sequential process (such as performed by
a sequential automaton) only the time succession of
the various states has a logical meaning, but not
the actual speed with which the sequential process
is performed. Therefore we have arranged the whole

system as a society of sequential processes, progressing with undefined speed ratios. To each user
program, accepted by the system, corresponds a
sequential process, to each input peripheral
corresponds a sequential process (buffering input
streams in synchronism with the execution of the
input commands), to each output peripheral corresponds a sequential process (unbuffering output
streams in synchronism with the execution of the
output commands); furthermore we have the "segment
controller" associated with the drum and the
"message interpreter" associated with the console
keyboard.
This enabled us to design the whole system in
terms of these abstract "sequential processes".
Their harmonious co-operation is regulated by means
of explicit mutual synchronization statements. On
the one hand, this explicit mutual synchronization
is necessary, as we do not make any assumption about
speed ratios, on the other hand this mutual synchronization is possible because "delaying the progress
of a process temporarily" can never be harmful to
the interior logic of the process delayed. The
fundamental consequence of this approach -viz. the
explicit mutual synchronization- is that the
harmonious co-operation of a set of such sequential
processes can be established by discrete reasoning;
as a further consequence the whole harmonious
society of co-operating sequential processes is
independent of the actual number of processors
available to carry out these processes, provided
the processors available can switch from process
to process.
System Hierarchy.
The total system admits a strict hierarchical
structure.
At level 0 we find the responsibility for
processor allocation to one of the processes whose
dynamic progress is logically permissible (i.e. in
view of the explicit mutual synchronization). At
this level the interrupt of the real time clock is
processed, introduced to prevent any process to
monopolize processing power. At this level a priority
rule is incorporated to achieve quick response of the
system where this is needed. Our first abstraction
has been achieved, above level 0 the number of
processors actually shared is no longer relevant. At
the higher levels we find the activity of the
different sequential processes, the actual processor
having lost its identity, having disappeared from
the picture.
At level I we have the so-called "segment
controller", a sequential process synchronized with
respect to the drum interrupt and the sequential
processes on higher levels. At level I we find the
responsibility to cater for the bookkeeping resulting
from the automatic backing store. At this level our
next abstraction has been achieved: at all higher
levels identification of information takes place in
terms of segments, the actual storage pages having
lost their identity, having disappeared from the

picture.
At level 2 we find the "message interpreter",
taking care of the allocation of the console keyboard
via which conversations between te operator and any
of the higher level processes can be carried out. The
message interpreter works in close synchronism with
the operator: when the operator presses a key, a
character is sent to the machine together with an
interrupt signal to announce this next keyboard
character, while the actual printing is then done
on account of an output command generated by the
machine under control of the message interpreter.
(As far as the hardware is concerned the console
teleprinter is regarded as two independent peripherals: an input keyboard and an output printer.) If
one of the processes opens a conversation it identifies itself for the benefit of the operator in the
opening sentence of this conversation. If, however,
the operator opens a conversation he must identify
the process he is addressing, in the opening sentence
of the conversation, i.e. this opening sentence must
be interpreted before it is known to which of the
processes the conversation is addressed~ There lies
the logical reason to introduce a separate sequential
process for the console teleprinter, a reason that
is reflected in its name "message interpreter". Above
level 2 it is as if each process had its private
conversational console. The fact that they share the
same physical console is translated into a resource
restriction of the form "only one conversation at a
time", a restriction that is satisfied via mutual
synchronization. At this level the next abstraction
has been implemented: at the higher levels the actual
console teleprinter has lost its identity. (If the
message interpreter had not been on a higher level
than the segment controller, then the only way to
implement it would have been to make a permanent
reservation in core for it; as the conversational
vocabulary might get large (as soon as our operators
wish to be addressed in fancy messages) this would
result in too heavy a permanent demand upon core
storage. Therefore the vocabulary in which the
messages are expressed is stored on segments, i.e.
as information units that can reside on the drum as
well. For this reason the message interpreter is of
a level one higher than the segment controller.)
At level 3 we find the sequential processes
associated with buffering of input streams and
unbuffering of output streams. At this level the next
abstraction is effected, viz. the abstraction of the
actual peripherals used, that are allocated at this
level to the "logical communication units" in terms
of which is worked in the still higher levels. The
sequential processes associated with the peripherals
are of a level above the message interpreter, because
they must be able to converse with the operator (e.g.
in the case of detected malfunctioning). The limited
number of peripherals again acts as a resource
restriction for the processes at higher levels, to be
satisfied by mutual synchronization between them.
At level 4 we find the independent user programs,
at level 5 the operator (not implemented by us).

The system structure has been described at
length in order to make the next section intelligible,
Design Experience
The conception stage took a long time. During
that period of time the concepts have been born in
terms of which we sketched the system in the previous
section. Furthermore we learnt the art of reasoning
by which we could deduce from our requirements the
way in which the processes should influence each
other as regards mutual synchronization so that
these requirements would be met. (The requirements
being that no information can be used before it has
been produced, that no peripheral can be set to two
tasks simultaneously, etc.) Finally we learnt the
art of reasoning by which we could prove that the
society composed of processes thus mutually synchronized by each other, would indeed in its time
behaviour satisfy all requirements.
The construction stage has been rather traditional, perhaps even old-fashioned: plain machine
code. Reprogramming on account of a change of
specifications has been rare, a circumstance that
must have contributed greatly to the feasibility of
the "steam method". The fact that the first two
stages took more time than planned was somewhat
compensated by a delay in the delivery of the machine.
In the verification stage we had, during short
shots, the machine completely at our disposal, shots
during which we worked with a virgin machine without
any software aids for debugging. Starting at level 0
the system has been tested, each time adding (a
portion of) the next level only after the previous
level had been thoroughly tested. Each test shot
itself contained on top of the (partial) system to
be tested a number of testing processes with a double
function. Firstly they had to force the system into
all different relevant states, secondly they had to
verify that the system continued to react according
to specification.
I shall not deny that the construction of these
testing programmes has been a major intellectual
effort: to convince oneself that one has not overlooked "e relevant state" and to convince oneself
that the testing programmes generate them all is no
simple matter. The encourageing thing is that (as
far as we are aware~) it could be done.
This fact was one of the happy consequences of
the hierarchical structure.
Testing level 0 (the real time clock and processor allocation) implied a number of testing
sequential processes on top of it, inspecting together that under all circumstances processor time
was divided among them according to the rules. This
being established, sequential processes as such had
been implemented.
Testing the segment controller at level I
meant that all "relevant states" could be formulated
in terms of sequential processes making (in various
combinations) demands on core pages, situations that

could be provoked by explicit synchronizing among
the testing programs. At that stage the existence
of the real time clock -although interrupting all
the time- was so immaterial that one of the testers
indeed forgot its existence~
By that time we had implemented the correct
reaction upon the (mutually unsynchronized) interrupts from the real time clock and the drum. If we
had not introduced the separate levels 0 and I and
if we had not created a terminology (viz. that of
the rather abstract sequential processes) in which
the existence of the clock interrupt could be discarded, but had tried instead to make in a nonhierarchical construction the central processor
directly react upon any weird time succession of
these two interrupts, the number of "relevant states"
would have exploded to such a height that exhaustive
testing would have been an illusion. (Apart from that
it is doubtful wether we would have had the means to
generate them all, drum and clock speed being outside
our control.)
For the sake of completeness I must mention
a further happy consequence. As stated before, above
level I core and drum pages have lost their identity
and buffering of input and output streams (at level
3) therefore occurs in terms of segments. While
testing at level 2 or 3 the drum channel hardware
broke down for quite some time, but testing could
proceed by restricting the number of segments so
that they all could be held in core. If building
up the line printer output streams had been implemented as "dumping onto the drum" and the actual printing as "printing from the drum" this advantage would
have been denied to us.
Conclusion
As far as program verification is concerned I
present nothing essentially new. In testing a
general purpose object (be it a piece of hardware,
a program, a machine or a system) one cannot subject
it to all possible cases: for a computer this would
imply that one feeds it with ell possible programs!
Therefore one must test it with a set of relevant
test cases. What is relevant or not, cannot be
decided as long as one regards the mechanism as a
black box, in other words it has to follow from the
internal structure of the mechanism to be tested. It
seems the designer's responsibility to construct his
mechanism in such a way -i.e. so effectively structured- that at each stage of the testing procedure
the number of relevant test cases is so small that
he can try them all and that what is being tested is
so perspicuous that it is clear that he has not
overlooked a situation. I have presented a survey of
our system because I think it a nice example of the
form that such a structure might take.
In my experience, I am sorry to say, industrial
software makers tend to react to it with mixed
feelings. On the one hand they are inclined to judge
that we have done a kind of model job, on the other
hand they express doubts whether the techniques used
are applicable outside the sheltered atmosphere of a

University Department and express the opinion that
we could only do it this way thanks to the modest
scope of the whole project. It is not my intention
to underestimate the organizing ability needed for
a much bigger job with ten or more times as many
people, but I should like to venture the opinion
that the larger the project, the more essential the
structuring~ A hierarchy of five logical levels
might then very well turn out to be of modest depth,
in particular when one designs the system more
consciously than we have done with the aim that the
software can be smoothly adapted to (perhaps drastic)
configuration expansions.
Acknowledqements
I should not like to publish this progress
report without expressing my great indebtedness to
my five collaborators C.Bron, A.N.Habermann, F.J.A.
Hendriks, C.Ligtmans and P.A.Voorhoeve. They have
contributed to ell the stages of the design, together
we learnt the art of reasoning needed. Construction
and verification is entirely their effort: if my
dreams have become true, this is due to their faith,
their talents and their persistent loyalty to the
whole project.
Finally I should like to thank the members of
the program committee who asked for more information
on the synchronizing primitives and some justification of my claim to be able to prove logical soundness a priori. In answer to this request the appendix
has been added, of which I hope that it gives the
desired information and justification.

"V(sem)" increases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is positive, the V-operation in question
has no further effect; if, however, the resulting
value of the semaphore concerned is non-positive,
one of the processes booked on its waiting list is
removed from this waiting list, i.e. its dynamic
progress is again logically permissible and in due
time a processor will be allocated to it (again, see
above "System Hierarchy", at level 0).
Corollary I:
If a semaphore value is nonpositive its absolute value equals the number of
processes booked on its waiting list.
Corollary 2:
The P-operation represents the
potential delay, the complementary V-operation
represents the removal of a barrier.
Note I:
P- and V-operations are "indivisible
actions", i.e. if they occur "simultaneously" in
parallel processes, they are non-interfering in the
sense that they can be regarded as being performed
the one after the other.
Note 2:
If the semaphore value resulting
from a V-operation is negative, its waiting list
did originally contain more than one process. It is
undefined -i.e. logically immaterial- which of the
waiting processes is then removed from the waiting
list.
Note 3:
A consequence of the mechanisms
described above is that a process whose dynamic
progress is permissible can only loose this status
by actually progressing, i.e. by performance of a
P-operation on a semaphore with a value that is
initially non-positive.

Appendix
The S~nchronizinq Primitives.
Explicit mutual synchronization of parallel
sequential processes is implemented via so-called
"semaphores". They are special purpose integer
variables allocated in the universe in which the
processes are embedded, they are initialized (with
the value 0 or I) before the parallel processes
themselves are started. After this initialization
the parallel processes will access the semaphores
only via two very specific operations, the so-called
synchronizing primitives. For historical reasons
they are called the P-operation and the V-operation.
A process, "Q" say, that performs the operation
"P(sem)" decreases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is non-negative, process Q can continue
with the execution of its next statement; if,
however, the resulting value is negative, process
Q is stopped and booked on a waiting list associated
with the semaphore concerned. Until further notice
(i.e. a V-operation on this very same semaphore)
dynamic progress of process Q is not logically
permissible and no processor will be allocated to
it (see above "System Hierarchy", at level 0).
A process, "R" say, that performs the operation

During system conception it transpired that we
used the semaphores in two completely different
ways. The difference is so marked that, looking
back, one wonders whether it was really fair to
present the two ways as a usage of the very same
primitives. On the one hand we have the semaphores
used for mutual exclusion, on the other hand the
private semaphores.
The Mutual Exclusion.
In the following program we indicate two
parallel, cyclic processes (between the brackets
"parbeRin" and '~arend") that come into action
after the surrounding universe has been introduced
and initialized.
begin semaphore mutex; mutex := I;
parbegin
beqin L I : P(mutex); critical section I; V(mutsx);
remainder of cycle I; ~oto LI
end;
beqin L2: P(mutex); critical section 2; V(mutex);
remainder of cycle 2; qoto L2
end
parend
end
As a result of the P- and V-operations on

"mutex" the actions, marked as "critical sections"
exclude each other mutually in time; the scheme
given allows straightforward extension to more than
two parallel processes, the maximum value of mutex
= I, the minimum value = - (n - I) if we have n
parallel processes.
Critical sections are used always and only for
the purpose of unambiguous inspection and modification of the state variables (allocated in the
surrounding universe) that describe the current
state of the system (as far as needed for the
regulation of the harmonious co-operation between
the various processes).
The Private Semaphores.
Each sequential process has associated with it
a number of private semaphores and no other process
will ever perform a P-operation on them. The universe
initializes them with the value = O, their maximum
value = I, their minimum value = - I.
Whenever a process reaches a stage where the
permission for dynamic progress depends on current
values of state variables, it follows the pattern:

P(mutex);
"inspection and modification of state variables
including a conditional V(private semaphore)";

V(mutex);
P(private semaphore)
If the inspection learns that the process in
question should continue, it performs the operation
"V(private semaphore)" -the semaphore value then
changes from 0 to I-, otherwise this V-operation
is skipped, leaving to the other processes the
obligation to perform this V-operation at a suitable
moment. The absence or presence of this obligation
is reflected in the final values of the state
variables upon leaving the critical section.
Whenever a process reaches a stage where as a
result of its progress possibly one (or more)
blocked processes should now get permission to
continue, it follows the pattern

P(mutex);
"modification and inspection of state variables
including zero or more V-operations on private
semaphores of other processes";

V(mutex)
By the introduction of suitable state variables
and appropriate programming of the critical sections
any strategy assigning peripherals, buffer areas etc.
can be implemented.
The amount of coding and reasoning can be
greatly reduced by the observation that in the two
complementary critical sections sketched above, the
same inspection can be performed by the introduction
of the notion of "an unstable situation", such as
a free reader and a process needing a reader.
Whenever an unstable situation emerges it is
removed (including ome or more V-operations on

private semaphores) in the very same critical
section in which it has been created.
Provinq the Harmonious Co-operation.
The sequential processes in the system can all
be regarded as cyclic processes in which a certain
neutral point can be marked, the so-called "homing
position", in which all processes are when the
system is at rest.
When a cyclic process leaves its homing position
"it accepts a task", when the task has been performed
end not earlier, the process returns to its homing
position. Each cyclic process has a specific task
processing power (e.g. the execution of a user
program or unbuffering a portion of printer output,
etc.)
The harmonious co-operation is mainly proved
in roughly three stages.
I)
It is proved that although a process
performing a task may generate in doing so a finite
number of tasks for other processes, a single
initial task cannot give rise to an infinite number
of task generations. The proof is simple as
processes can only generate tasks for processes
at lower levels of the hierarchy so that circularity
is excluded. (If a process needing a segment from
the drum has generated a task for the segment
controller, special precautions have been taken to
ensure that the segment asked for remains in core
at least until the requesting process has effectively
accessed the segment concerned. Without this precaution finite tasks could be forced to generate an
infinite number of tasks for the segment controller
and the system could get stuck in an unproductive
page flutter.)
2)
It is proved that it is impossible
that all processes have returned to their homing
position while somewhere in the system is still
pending a generated but unaccepted task. (This is
proved via instability of the situation just
described.)
3)
It is proved that after the acceptance
of an initial task all processes eventually will be
(again) in their homing position. Each process
blocked in the course of task execution relies on the
other processes for removal of the barrier. Essentially, the proof in question is a demonstratmon of the
absence of "circular waits": process P waiting for
process Q waiting for process R waiting for process
P. (Our usual term for the circular wait is "the
Deadly Embrace".) In a more general society than
our system this proof turned out to be e proof by
induction (on the level of hierarchy, starting at
the lowest level) as A.N.Habermann has shown in his
doctoral thesis.

THE WORKING SET MODEL FOR PROGRAM BEHAVIOR
Peter J. Denning
Massachusetts Institute of Technology
Cambridge, Massachusetts

SUMMARY

We claim neither is adequate.

Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate
model for program behavior.
In this paper a new
model is developed, the "working set model", which
enables us to decide which information is in use
by a running program and which is not.
Such knowledge is vital for dynamic management of paged
memories.
The working set of pages associated
with a process, defined to be the collection of its
most recently used pages, is a useful allocation
concept. A proposal for an easy-to-implement
allocation policy is set forth; this policy is
unique, inasmuch as it blends into one decision
function the heretofore independent activities of
process-scheduling and memory-management.

Because resources are multiplexed, each user
is given the illusion that he has a complete computing system at his sole disposal: a virtual
computer.
For our purposes, the basic elements of
a virtual computer are its virtual processor and
an "infinite" one-level virtual memory.
Dynamic
"advice" regarding resource requirements cannot be
obtained successfully from users for several
reasons:
i. A user may build his program on the work
of others, frequently sharing procedures
whose time and storage requirements may be
either unknown or, because of data dependence, indeterminate.
Therefore he cannot
be expected to estimate processor-memory
needs.
2. It is not clear what sort of "advice" might
be solicited.
Nor is it clear how the
operating system should use it, for overhead incurred by using advice could well
negate any advantages attained.
3. Any advice acquired from a user would be
intended (by him) to optimize the environment for his own program.
Configuring
resources to suit individuals may interfere
with overall good service to the community
of users.
Thus it seems inadvisable at the present time to
permit users, at their discretion, to advise the
operating system of their needs.

INTRODUCTION
Resource allocation is tricky business.
In
recent years there has been much dialogue on the
topics of process scheduling and core memory management, yet development of techniques has progressed independently along both these lines. No
one will deny that a unified approach is needed.
Probably the most basic reason behind the absence
of a general treatment is the lack of an adequate
model for program behavior.
In this paper we develop a new model, the working set model, which
embodies certain important behavioral properties
of computations operating in a multiprogrammed environment, enabling us to decide which information
is in use by a running program and which is not.
We do not intend that the proposed model be considered "final"; rather, we hope to stimulate a
new kind of thinking, thinking that may be of considerable help in solving many operating system
design problems.
The working set is intended to model the behavior of programs in the general purpose computer
system, or computer utility.
For this reason we
assume that the operating system must on its own
determine the behavior of programs it runs; it
cannot count on outside help.
Two commonly proposed sources of externally-supplied dynamic allocation information are the user and the compiler.

Work reported herein was supported in part by
Project MAC, an M.I.T. research project sponsored
by the Advanced Projects Research Agency, Dept.
of Defense, under Office of Naval Research Contract Number Nonr-4102(Ol).

Likewise, compilers cannot be expected to
supply information, extracted from the structure
of the program , regarding resource requirements:
i. Programs will be modular in construction;
information about other modules may be unavailable at compilation time.
Because of
dependence on data there may be no way to
decide (until run time) just which modules
will be included in a computation.
2. Compilers cluttered with extra machinery
to predict memory needs will be slower in
operation.
Many users are less interested
in whether their programs operate efficiently than whether they operate at all, and
so are concerned with rapid compilation.
Furthermore, the compiler is an often-used
component of the operating system; if slow
and bulky, it can be a serious drain on
system resources.

**Ramamoorthy I has put forth a proposal for automatic segmentation of programs during compilation.

Therefore in this paper we are advocating mechanisms that monitor the behavior of a computation,
making allocation decisions on the basis of currently observed characteristics.
Only a mechanism
that oversees the behavior of a program in operation can cope with arbitrary interconnections of
arbitrary modules having arbitrary characteristics.
Our treatment proceeds as follows. First we
define the type of computer system in which our
ideas are developed. After a brief discussion of
previous work with the problems of dynamic memory
management, we define the working set model. We
discuss a method of implementing memory management
based on this model, and indicate how working set
notions can be used to blend process scheduling
and memory management into one decision function,
accounting simultaneously for both types of demand.
Finally we discuss how data sharing fits into the
working-set scheme.

THE FRAMEWORK
We assume that the reader is already familiar
with the concepts of a computer utility 2'3'4, of
segmentation and paging 5'6, of program and addressing structure 6'8, so we will only mention these
topics here. Briefly, each process has access to
its own private, segmented name space; each segment known to the process is sliced into equalsize units, called pages, to facilitate mapping it
into the paged main memory.
Associated with each
segment is a page table, whose entries point to

the segment's pages. An "in-core" bit in each
page table entry is turned ON whenever the designated page is present in main memory ; an attempt
to reference a page whose "in-core" bit is OFF
causes a page fault, initiating proceedings to
secure the missing page. Finally, a process has
three states of existence: running, when a processor is assigned to it; ready, when it would be
running if only a processor were available; or
blocked, when it has no need of a processor (for
example, during a page fault or during a console
interaction). When talking about processes in execution, we will have to distinguish between "process time" and "real time". Process time is time
as seen by a process unaware it is suspended; that
is, as if it executed without interruptions.
We restrict attention to a two-level memory
system, indicated by Figure I. Only data residing
in main memory is accessible to a processor; all
other data reside in auxiliary memory, which we
regard as having i nflnite capacity. There is a
time T, the traverse time, involved in transferring
a page between memories. T is measured from the
moment a page fault occurs until the moment the
missing page is in main memory ready for use. T
is actually the expectation of a random variable
composed of waits in queues and mechanical positioning delays.
Though it usually takes less time to
store into auxiliary memory than to read from it,
we shall regard the traverse time T to be the same
regardless of which direction a page is moved.
,
Consistent with current usage, we will use the
terms "core memory" and "main memory" interchangeably.

PROCESSORS

~

~ ~

~ n

data flow from main memory
(controlled b y ~, c o r emanager)
I

main memÂ°rY(contrD~edfl~ ~ ~ ~ n m ~ [ c Y i e s )

FIGURE i.

Two-level memory system.

A basic allocation problem, "core memory management", is that of deciding just which pages are to
occupy main memory.
The basic strategy advocated
here -- a compromise against a lot of expensive
,
main memory -- is to minimize page traffic . There
are two reasons for this:
i. The more the data traffic between the two
levels of memory, the more the computational overhead in deciding just what to move
and where to move it.
2. Because the traverse time T is long compared
to a memory cycle, too much data movement
can result in congestion and serious interference with processor efficiency.
Roughly speaking, a working set of pages is
the minimum collection of pages that must be loaded
in main memory for a process to operate efficiently,
without "unnecessary" page faults. According to
our definitions, a "process" and its "working set"
are but two manifestations of the same ongoing
computational activity.

PREVIOUS WORK
In this section we outline strategies that have
been set forth in the past for memory management;
the interested reader will be referred to the literature for detail.
We regard management of paged memories to operate in two stages:
I. Pagin_g in." locate the required page in
auxiliary memory, load it into main memory,
turn the "in-core" bit of the appropriate
page table entry ON.
2. Paging out: remove some page from main memory, turn the "in-core" bit of the appropriate page table entry OFF.
Management algorithms can be classified according
to their methods of paging in and paging out.
It
is a common characteristic of nearly every strategy
that paging in is done on demand; that is, no action
is taken to load a page into memory until some
process attempts to reference it. To date there
have been no proposals recommending look-ahead, or
anticipatory page-loading, because (as we have
stressed) there is no reliable advance source of
allocation information, be it the programmer or
the compiler.
Although the working set is the
desired information, it might still be futile to
pre-load pages: there is no guarantee that a process will not block shortly after resumption,
having referenced only a fraction of its working
set.
The operating system could devote its already precious time to activities more rewarding
than loading pages which may not be used.
Thus we
will assume that paging in is done on demand only,
via the page fault mechansim.

Since data is stored and transmitted in units of
pages, we can (without ambiguity) refer to data
movement as "page traffic".

The chief problem in memory management is not
deciding which pages to load; it is deciding which
pages ought to be removed.
For if the page with
the least likelihood of being used in the immediate
future is retired to auxiliary memory, the best
choice has been made.
Nearly every worker in the
field has recognized this.
Debate has arisen over
which strategy to employ for retiring pages; that
is, which page-turning, or replacement, algorithm
to use.
A good measure of performance for a paging
policy is page traffic (the number of pages per
unit time being moved between memories), since
erroneously removed pages add to the traffic of
returning pages.
In the following we will use this
as a basis of comparison for several strategies.
Random selection.
Whenever a fresh page of memory
is needed, a page is selected at random to be replaced.
Although utterly simple to implement, this
method frequently removes useful pages (which must
therefore be recalled) and so results in high page
traffic.
~!%~
selection. The pages of main memory are ordered in a cyclic list.
Suppose the M pages of
main memory are numbered 0,1,...,(M-I) and a
pointer k indicates that the k-th page was most
recently paged in. Whenever a fresh page of memory
is needed, [(k+l) mod M] 4 k, page k is retired,
and another page brought in to fill the now vacant
slot.
This method -- also utterly simple to realize -- is based on the principle that programs tend
to follow sequences of instructions, so that references in the immediate future will most likely
be close to present references.
So, assuming there
is this tendency for page references to cluster,
and assuming some kind of uniformity in process
scheduling techniques, the page which has been in
memory longest is least likely to be reused: hence
the cyclic list. We see two ways in which this
algorithm can fail. First we question its basic
assumption.
It is not at all clear that modular
programs, which execute numerous inter-module calls,
will indeed exhibit sequential instruction fetch
patterns.
The thread of control will not string
pages together; rather, it will entwine them intricately.
[Fine, Mclssac, and Jackson 9 have some
experimental evidence in support of this reasoning.]
Second, this algorithm is subject to overloading when used in multiprogrammed memories.
When core demand is too heavy, one cycle of the
list completes rapidly and the pages deleted are
still needed by their processes.
This can create
a self-intensifying crisis.
Programs, deprived of
still-needed pages, generate a plethora of page
faults; the resulting traffic of returning pages
displaces still other useful pages, leading to more
page faults, and so on.
Oldest-unused selection.
Each page table entry
contains a ~us~ ~ bit, set ON each time the page is
referenced.
At periodic intervals all the page
table entries are searched and usage records updated.
When a fresh page of memory is needed, the page unreferenced for the longest time is removed.
One can

see that this method is intrinsically reasonable by
considering the simple case of a computer where
there is exactly one process whose pages cannot all
fit into main memory.
In this case the most reasonable choice for a page to replace is the oldest
unused page.
Unfortunately this method too is susceptible to overloading when many processes compete
for main memory.
ATLAS ioo~ detection method.

The Ferranti ATLAS

computer I0 had proposed a page-turning policy that
attempted to detect loop behavior in page reference
patterns, then minimize page traffic by maximizing
the time between page transfers, that is, by removing pages not expected to be needed for the longest time.
It was successful -- only for looping
programs.
Performance was unimpressive for programs exhibiting random reference patterns.
Implementation was costly.
Various studies concerning behavior of paging
algorithms have appeared.
Fine, Mclssac and
Jackson 9 have investigated the effects of demand
paging and have questioned whether paging is beneficial at all. We do not feel that their conclusion applies to the kind of multiprogrammed environment we have described.
They studied fixed-size
programs, that quickly acquired and retained a
large fraction of their pages.
Highly interactive,
modular programs are likely to behave differently.
Not only may program size vary dynamically (according to data dependencies), but also such programs
should be using a small fraction of their pages at
any one time, and the membership in this set of
working pages should be changing constantly.

We define the working set of information W(t,T)
of a process at time t to be the collection of data
items referenced by the process during the proces
time interval (t-T~,t).
Thus, the data items a process has referenced
during the last T seconds of its execution comprise
its working set. q will be called the working set
parameter.
We will regard the data items in W(t,T)
as being pages ,although they could just as well
be any other named data objects.
The working set
size ~(t,~) is
(I)

~(t,T)

=

Number of pages in W(t,T)

A working set W(t,~) has two important, general
properties.
Both are properties of typical programs,
and need not hold in special cases.
PI. Size. It should be clear immediately that
~(t,0) = 0 since no page reference can occur
in zero time.
It should also be clear that
~(t,T) as a function of T is monotonically
increasing, since more pages can be referenced
in longer time intervals.
Because a process
will refer to its most-needed pages frequently
and its least-needed pages infrequently, we
expect ~(t,~) as a function of T to have a
steep initial rise which diminishes to a more
gradual rise.
The general character of ~(t,~)
is suggested by the smoothed curve of Figure 2.
00(t ,~-)

Belady II has compared some of the algorithms
mathematically.
His most important conclusion is
that the "ideal" algorithm should possess much of
the simplicity of Random or Cyclic selection (for
efficiency) and some, though not much, accumulation
of data on past reference patterns.
He has shown
that too much "historical" data can have adverse
effects (witness ATLAS).
In the next section we begin investigation of
the working set concept.
Even though the ideas
are not entirely new 12'13'14, there has been no
detailed documentation publicly available.

THE WORKING SET MODEL
From the programmer's standpoint, the working
set of information is the smallest collection of
procedure and data items that must be present in
main memory to assure efficient execution of his
program.
We have already stressed that there will
be no advance notice from either the programmer or
the compiler regarding what information "ought" to
be in main memory.
It is up to the operating
system to determine on the basis of page reference
patterns whether pages are in use.
Therefore the
working set of information associated with a process is, from the system standpoint, the set of
most recently referenced pages.

FIGURE 2.

Behavior of ~(t,T).

Program modularity enables us to
P2. Correlation.
say something about correlation between the
size of a working set at two times, t and (t+c~).
Correlation is useful in devising storage
allocators, for the higher the correlation
between ~(t,T) and w(t+c~,T), the better is
~(t,T) a prediction of ~(t+c~,T).
In modular
programs, control passes randomly from one
module to another; if T is chosen properly
(as discussed in the next section), it is more
likely that a working set will change size
smoothly, less likely that it will change size
abruptly.
Thus for small time separations
(say, 6 < < T), ~(t,~) and ~(t+c~,T) are highly

correlated, meaning that a measurement of ~(t,T)
will serve as a good estimate of the memory requirement during the process time interval (t,t+c0.
For large time separations ~ (say, ~ >> .[), control
will have passed through a great many modules
during the interval (t,t+c~); thus ~(t,T ) gives
little information about ~(t+C~,7), and so ~(t,7)
and ~(t+c~,7) have much less correlation than for
small 5. This behavior is suggested in Figure 3.
Correlation between
~(t,7) and ~(t+Cz,7)

Detecting W(t~T )
According to our definition, W(t,T) is the set
of its pages a process has referenced within the
last T seconds of its execution.
This suggests
that memory management can be controlled by hardware mechanisms, by associating with each page of
main memory a timer.
Each time a page is referenced, its timer is set to T and begins to run
down; if the timer succeeds in running down, a flag
is set to mark the page for removal whenever the
space is needed.
In the appendix we describe such
a hardware memory management mechanism, hardware
that can be housed within the memory boxes.
The
mechanism has two interesting features:
i. It operates asynchronously and independently
of the supervisor, whose only respsonsibility
in memory management is handling page faults.
Quite literally, memory manages itself.
2. Analog devices such as capacitative timers
could be used to measure intervals.

~D

c~
0
FIGURE 3.

Correlation between working set sizes,

Choice of T

Unfortunately it is not practical to add on
hardware to existing systems. We seek a method of
handling memory management within the software.
The procedure we propose here samples the page
table entries of pages in core memory at process
time intervals of ~ seconds (~ is called the
sampling interval) where ~ = y/K , K an integer
constant chosen to make the sampling intervals as
"fine grain" as desired.
On the basis of page
references during each of the last K sampling intervals, the working set W(t,l~) can be determined,
as follows.

The value ultimately selected for ~ will reflect
efficiency requirements and will be influenced by
"in- core"
system parameters such as core memory size and memI
ory traverse time.
For example, if T is too small,
pages may be removed from main memory while they
are still useful, and high page traffic may result
from returning pages.
If T is too large, pages
may remain in main memory long after last being
used, and wasted main memory may result.
Thus the
value of T will have to represent a compromise between too much page traffic and too much wasted
memory space.
0

l

pointer
to page

use bits

luol-,l

---

TYPICAL PAGE TABLE ENTRY

SHIFT AT END OF SAMPLING INTERVAL
The following consideration leads us to recommend for T a value comparable to the memory traverse time T (Figure I). Consider a process that
is running continuously, being interrupted only
for page faults.
Assuming that memory allocation
procedures balk at removing from main memory any
page in a working set, once a page has entered
W(t,T) it will remain in main memory for at least
T seconds.
Under the very worst of page-shuffling
conditions, a page could be dispatched to auxiliary memory and be recalled immediately; the time
for this round trip is two traverse times, 2T.
Therefore a highly-shuffled page would spend
roughly T/2T of its time in main memory.
So,
for example, if we wished to insure that a page is
available in main memory (when needed) for not less
than 50 per cent of the time, we would have to
choose â¢ ~ 2T.

FIGURE 4.

Page table entries for detecting W(t,Kcy).

As indicated by Figure 4, each page table entry
contains an "in-core" bit M, where M=i if and only
if the page is present in main memory.
It also
contains a string of use bits u0,ul,...,u K. Each
time a page reference occurs,

I ~ u 0.

At the end

of each sampling interval ~, the bit pattern contained in u0,ul,...,u K is shifted one position,
a 0 enters u 0, and u K ipapers.txt, 0)
open: File name too long
vm_fault returned -1
					returning to (632844) with r|w pages:
					r	vpage 0x60000	ppage 0x1
					r	vpage 0x60001	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
vm_destroy
infile size: 25
[0x6322a15a748c -> ftype: FILE_B
infile: true
block: 2
filename: papers.txt
]
[0x6322a15a7488 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a7484 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a7480 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a747c -> ftype: FILE_B
infile: true
block: 0
filename: ·-!boe!pee addresses used
where an even address is required. Such faults cause the
processor to trap to a system routine. When an illegal
action is caught, unless other arrangements have been
made, the system terminates the process and writes
the user's image on file core in the current directory. A
debugger can be used to determine the state of the
program at the time of the fault.
Programs which are looping, which produce unwanted output, or about which' the user has second
thoughts may be halted by the use of the interrupt
signal, which is generated by typing the "delete"
character. Unless special action has been taken, this
signal simply causes the program to cease execution
without producing a core image file.
There is also a quit signal which is used to force a
core image to be produced. Thus programs which loop
unexpectedly may be halted and the core image examined without prearrangement.
The hardware-generated faults and the interrupt and
quit signals can, by request, be either ignored or caught
by the process. For example, the Shell ignores quits to"
prevent a quit from logging the user out. The editor
catches interrupts and returns to its c o m m a n d level.
This is useful for stopping long printouts without losing
work in progress (the editor manipulates a copy of
the file it is editing). In systems without floating point
hardware, unimplemented instructions are caught, and
floating point instructions are interpreted.

8. Perspective
Perhaps paradoxically, the success of UNIX is largely
due to the fact that it was not designed to meet any
predefined objectives. The first version was written
when one of us (Thompson), dissatisfied with the
available computer facilities, discovered a little-used
Communications
of
the ACM

July 1974
Volume 17
Number 7

PDP-7 and set out to create a more hospitable environment. This essentially personal effort was sufficiently
successful to gain the interest of the remaining author
and others, and later to justify the acquisition of the
POP-11/20, specifically to support a text editing and
formatting system. When in turn the 11/20 was outgrown, UNIX had proved useful enough to persuade
management to invest in the PDP-11/45. Our goals
throughout the effort, when articulated at all, have
always concerned themselves with building a comfortable relationship with the machine and with exploring
ideas and inventions in operating systems. We have
not been faced with the need to satisfy someone else's
requirements, and for this freedom we are grateful.
Three considerations which influenced the design
of UNIX are visible in retrospect.
First, since we are programmers, we naturally
designed the system to make it easy to write, test, and
run programs. The most important expression of our
desire for programming convenience was that the
system was arranged for interactive use, even though
the original version only supported one user. We bebelieve that a properly-designed interactive system is
much more productive and satisfying to use than a
" b a t c h " system. Moreover such a system is rather
easily adaptable to noninteractive use, while the converse is not true.
Second, there have always been fairly severe size
constraints on the system and its software. Given the
partially antagonistic desires for reasonable efficiency
and expressive power, the size constraint has encouraged
not only economy but a certain elegance of design.
This may be a thinly disguised version of the "salvation through suffering" philosophy, but in our case it
worked.
Third, nearly from the start, the system was able to,
and did, maintain itself. This fact is more important
than it might seem. If designers of a system are forced
to use that system, they quickly become aware of its
functional and superficial deficiencies and are strongly
motivated to correct them before it is too late. Since
all source programs were always available and easily
modified on-line, we were willing to revise and rewrite
the system and its software when new ideas were
invented, discovered, or suggested by others.
The aspects of UNIX discussed in this paper exhibit
clearly at least the first two of these design considerations. The interface to the file system, for example, is
extremely convenient from a programming standpoint.
The lowest possible interface level is designed to
eliminate distinctions between the various devices and
files and between direct and sequential access. N o
large "access method" routines are required to insulate
the p r o g r a m m e r from the system calls; in fact, all
user programs either call the system directly or use a
small library program, only tens of instructions long,
which buffers a number of characters and reads or
writes them all at once.

Another important aspect of programming convenience is that there are no "control blocks" with a
complicated structure partially maintained by and depended on by the file system or other system calls.
Generally speaking, the contents of a program's address
space are the property of the program, and we have
tried to avoid placing restrictions on the data structures
within that address space.
Given the requirement that all programs should be
usable with any file or device as input or output, it is
also desirable from a space-efficiency standpoint to push
device-dependent considerations into the operating system itself. The only alternatives seem to be to load
routines for dealing with each device with all programs,
which is expensive in space, or to depend on some means
of dynamically linking to the routine appropriate to
each device when it is actually needed, which is expensive either in overhead or in hardware.
Likewise, the process control scheme and c o m m a n d
interface have proved both convenient and efficient.
Since the Shell operates as an ordinary, swappable user
program, it consumes no wired-down space in the
system proper, and it may be made as powerful as
desired at little cost. In particular, given the framework .
in which the Shell executes as a process which spawns
other processes to perform commands, the notions of
I/O redirection, background processes, c o m m a n d flies,
and user-selectable system interfaces all become essentially trivial to implement.

374

Communications
of
the ACM

8.1 Influences

The success of'UNIX lies not so much in new inventions but rather in the full exploitation of a carefully
selected set of fertile ideas, and especially in showing
that they can be keys to the implementation of a small
yet powerful operating system.
The fork operation, essentially as we implemented it,
was present in the Berkeley time-sharing system [8]. On
a number of points we were influenced by Multics, which
suggested the particular form of the I / o system calls
[9] and both the name of the Shell and its general functions. The notion that the Shell should create a process
for each c o m m a n d was also suggested to us by the
early design of Multics, although in that system it was
later dropped for efficiency reasons. A similar scheme
is used by TENEX [10].

9. Statistics

The following statistics from UNIX are presented to
show the scale of the system and to show how a system
of this scale is used. Those of our users not involved in
document preparation tend to use the system for program development, especially language work. There are
few important "applications" programs.

July 1974
Volume 17
Number 7

9.1 Overall
72 user population
14 maximum simultaneous users
300 directories
4400 files
34000 512-byte secondary storage blocks used
9.2 Per day (24-hour d a y , 7 - d a y week basis)
T h e r e is a " b a c k g r o u n d " process t h a t runs at the
lowest possible p r i o r i t y ; it is used to s o a k u p a n y idle
c P u time. It has been used to p r o d u c e a m i l l i o n - d i g i t
a p p r o x i m a t i o n to the c o n s t a n t e - 2, a n d is n o w
g e n e r a t i n g c o m p o s i t e p s e u d o p r i m e s (base 2).
1800 commands
4.3 CPU hours (aside from background)
70 connect hours
30 different users
75 logins

9.3 Command CPU Usage (cut off at 1%)
15.7% Ccompiler
15.2% users' programs
11.7% editor
5.8% Shell (used as a cornmand, including command times)
5.3% chess
3.3% list directory
3.1% document formatter
1.6% backup dumper
1.8% assembler

1.7%
1.6%
1.6%
1.6%
1.4%
1.3%
1.3%
1.1%
1.0%

Fortran compiler
remove file
tape archive
file system consistency
check
library maintainer
concatenate/printfiles
paginate and print file
print disk usage
copy file

9.4 Command Accesses (cut off at 1%)
15.3%
9.6%
6.3%
6.3%
6.0%
6.0%
3.3%
3.2%
3.1%
1.8%
1.8%
1.6%

editor
list directory
remove file
C compiler
concatenate/printfile
users' programs
list people logged on
system
rename/move file
file status
library maintainer
document formatter
execute another command conditionally

1.6%
1.6%
1.5%
1.4%
1.4%
1.4%
1.2%
1.1%
1.1%
1.1%

debugger
Shell (used as a command)
print disk availability
list processes executing
assembler
print arguments
copy file
paginate and print file
print current date/time
file system consistency
check
1.0% tape archive

ficulties such as p o w e r d i p s a n d i n e x p l i c a b l e p r o c e s s o r
i n t e r r u p t s to r a n d o m locations. T h e r e m a i n d e r are
s o f t w a r e failures. T h e longest u n i n t e r r u p t e d up time
was a b o u t two weeks. Service calls average one every
t h r e e weeks, b u t are h e a v i l y clustered. T o t a l up time
has been a b o u t 98 p e r c e n t o f o u r 24-hour, 365-day
schedule.
Acknowledgments. W e are grateful to R . H . C a n a d a y ,
L.L. Cherry, a n d L.E. M c M a h o n for their c o n t r i b u tions to uNIX. W e are p a r t i c u l a r l y a p p r e c i a t i v e o f the
inventiveness, t h o u g h t f u l criticism, a n d c o n s t a n t supp o r t o f R. M o r r i s , M . D . M c I l r o y , a n d J.F. O s s a n n a .

References
1. Digital Equipment Corporation. PDP-I1/40 Processor
Handbook, 1972, and PDP-I1/45 Processor Handbook, 1971.
2. Deutsch, L.P., and Lampson, B.W. An online editor. Comm.
ACM 10, 12 (Dec. 1967), 793-799, 803.
3. Richards, M. BCPL: A tool for compiler writing and system
programming. Proc. AFIPS 1969 SJCC, Vol. 34, AFIPS Press,
Montvale, N.J., pp. 557-566.
4. McClure, R.M. TMG--A syntax directed compiler. Proc.
ACM 20th Nat. Conf., ACM, 1965, New York, pp. 262-274.
5. Hall, A.D. The M6 macroprocessor. Computing Science Tech.
Rep.#2, Bell Telephone Laboratories, 1969.
6. Ritchie, D.M. C reference manual. Unpublished memorandum,
Bell Telephone Laboratories, 1973.
7. Aleph-null. Computer Recreations. So[?ware Practice and
Experience 1, 2 (Apr.-June 1971), 201-204.
8. Deutsch, L.P., and Lampson, B.W. SDS 930 time-sharing
system preliminary reference manual. Doc. 30.10.10, Project G ENI E,
U of California at Berkeley, Apr. 1965.
9. Feiertag, R.J., and Organick, E.I. The Multics input-output
system. Proc. Third Syrup. on Oper. Syst. Princ., Oct. 18-20, 1971,
ACM, New York, pp. 35-41.
10. Bobrow, D.G., Burchfiel, J.D., Murphy, D.L., and Tomlinson,
R.S. TENEX, a paged time sharing system tbr the PDP-10. Comm.
ACM15, 3 (Mar. 1972), 135-143.

9.5 Reliability
O u r statistics on reliability are m u c h m o r e subjective
t h a n the others. T h e f o l l o w i n g results are true to the
best o f o u r c o m b i n e d recollections. T h e t i m e s p a n is
over one y e a r with a very early vintage 11/45.
T h e r e has been one loss o f a file system (one d i s k
o u t o f five) caused b y s o f t w a r e i n a b i l i t y to c o p e with
a h a r d w a r e p r o b l e m c a u s i n g r e p e a t e d p o w e r fail traps.
Files on t h a t d i s k were b a c k e d up three days.
A " c r a s h " is an u n s c h e d u l e d system r e b o o t o r
halt. T h e r e is a b o u t one crash every o t h e r d a y ; a b o u t
t w o - t h i r d s o f t h e m are c a u s e d by h a r d w a r e - r e l a t e d dif-

375

Communications
of
the ACM

July 1974
Volume 17
Number 7

THE STRUCTURE OF THE "THE"-MULTIPROGRAMMING SYSTEM

EWDI 96

Edsger W.Dijkstra
Technological University
EINDHOVEN
The Netherlands

Summary
A multiprogramming system is described in
which all activities are divided over a number of
sequential processes. These sequential processes
are placed at various hierarchical levels, in each
of which one or more independent abstractions have
been implemented. The hierarchical structure proved
to be vital for the verification of the logical
soundness of the design and the correctness of its
implementation.
Introduction
Papers "reporting on timely research and
development efforts" being explicitly asked for, I
shall try to present a progress report on the multiprogramming effort at the Department of Mathematics
at the Technological University, Eindhoven, the
Netherlands.
Having very limited resources (viz. a group of
six people of, on the average, half time availability) and wishing to contribute to the art of system
design -including all the stages of conception,
construction and verification- we are faced with the
problem of how to get the necessary experience. To
solve this problem we have adopted the following
three guiding principles:
I) Select a project as advanced as you can
conceive, as ambitious as you can justify, in the
hope that routine work can be kept to a minimum;
hold out against all pressure to incorporate such
system expansions that would only result into a
purely quantitative increase of the total amount of
work to be done.
2) Select a machine with sound basic characteristics (e.g. an interrupt system to fall in love
with is certainly an inspiring feature); from then
onwards try to keep the specific properties of the
configuration for which you are preparing the system
out of your considerations as long as possible.
3) Be aware of the fact that experience does by
no means automatically lead to wisdom and understanding; in other words, make a conscious effort to
learn as much as possible from your precious
experiences.
Accordingly, I shall try to go beyond just
reporting what we have done and how, and I shall
try to formulate as well what we have learned.
I should like to end the introduction with two
short remarks on working conditions, remarks I make
for the sake of completeness. I shall not stress
these points any further.

The one remark is that production speed is
severely degraded if one works with half time people
who have other obligations as well. This is at least
a factor four, probably it is worse. The people
themselves lose time and energy in switching over,
the group as a whole loses decision speed as discussions~ when needed, have often to be postponed
until all people concerned are available.
The other remark is that the members of the
group (mostly mathematicians) have previously
enjoyed as good students a university training of
5 to 8 years and are of Master's or Ph.D. level.
I mention this explicitly because at least in my
country the intellectual level needed for system
design is in general grossly underestimated. I am
more than ever convinced that this type of work is
just difficult and that every effort to do it with
other than the best people is doomed to either
failure or moderate success at enormous expenses.
The Tool and the Goal
The system has been designed for a Dutch
machine, the EL X8 (N.V.Electrologica, Rijswijk
(ZH)). Characteristics of our configuration are:
I) core memory cycle time 2.5 mms., 27 bits; at
present 32K.
2) drum of 512K words, 1024 words per track, rev.
time 40 ms.
3) an indirect addressing mechanism very well
suited for stack implementation
4) a sound system for commanding peripherals and
controlling of interrupts
5) a potentially great number of low capacity
channels; ten of them are used (3 paper tape readers
at 1000 char/sac; 3 paper tape punches at 150 char/
sec; 2 teleprinters; a plotter; a line printer)
6) absence of a number of not unusual awkward
features.
The primary goal of the system is to process
smoothly a continuous flow of user programs as a
service to the University. A multiprogramming
system has been chosen with the following objectives
in mind:
I) a reduction of turn around time for programs of
short duration
2) economic use of peripheral devices
3)automatic control of backing store to be combined
with economic use of the central processor
4) the economic feasibility to use the machine for
those applications for which only the flexibility of

a general purpose computer is needed but (as a r u l e )
not the capacity nor the processing power.

The system is not intended as a multi-access
system. There is no common data base via which
independent users can communicate with each other:
they only share the configuration and a procedure
library (that includes a translator for ALGOL 60
extended with complex numbers). The system does not
cater for user programs written in machine language.
Compared with larger efforts one can state that
quantitatively speaking the goals have been set as
modest as the equipment and our other resources.
Qualitatively speaking, I am afraid, we got more and
more immodest as the work progressed.

minutes (classical) inspection at the machine and
each of them correspondingly easy to remedy. At the
moment of writing the testing is not yet completed,
but the resulting system will be guaranteed to be
flawless. When the system has been delivered we shall
not live in the perpetual fear that a system derailment may still occur in an unlikely situation such as
might result from an unhappy "coincidence" of two or
more critical occurrences, for we shall have proved
the correctness of the system with a rigour and
explicitness that is unusual for the great majority
of mathematical proofs.
A Survey of the System Structure

A Progress Report
Storage Allocation.
We have made some minor mistakes of the usual
type (such as paying too much attention to speeding
up what was not the real bottle neck) and two major
ones.
Our f i r s t
m a j o r m i s t a k e has been t h a t f o r t o o
long a time we confined our attention to "a perfect
installation": by the time we considered how to make
the best of it when, say, one of the peripherals
broke down, we were faced with nasty problems.
Taking care of the "pathology" took more energy than
we had expected and part of our troubles were a
direct consequence of our earlier ingenuity, i.e.
the complexity of the situation into which the system
could have manoeuvred itself. Had we paid attention
to the pathology at an earlier stage of the design,
our management rules would certainly have been less
refined.

The second major mistake has been that we
conceived and programmed the major part of the system
without giving more than scanty thought to the problem of debugging it. For the fact that this mistake
had no serious consequences -on the contrary~ one
might argue as an afterthought- I must decline all
credit. I feel more like having passed through the
eye of the needle...
As captain of the crew I had had extensive
experience (dating back to 1958) in making basic
software dealing with real time interrupts and I
knew by bitter experience that as a result of the
irreproducibility of the interrupt moments, a program
error could present itself misleadingly like an
occasional machine malfunctioning. As a result I was
terribly afraid. Having fears regarding the possibility of debugging we decided to be as careful as
possible and -prevention is better than cure~- to
try to prevent nasty bugs from entering the construction.
This decision, inspired by fear, is at the
bottom of what I regard as the group's main contribution to the art of system design. We have found
that it is possible to design a refined multiprogramming system in such a way that its logical soundness
can be proved a priori and that its implementation
admits exhaustive testing. The only errors that
showed up during testing were trivial coding errors
(occurring with a density of one error per 500
instructions), each of them located within 10

In the classical yon Neumann machine information
is identified by the address of the memory location
containing the information. When we started to think
about the automatic control of secondary storage
we were familiar with a system (viz. GIER ALGOL)
in which all information was identified by its
drum address (as in the classical yon Neumann
machine) and in which the function of the core
memory was nothing more than to make the information
"page wise" accessible.
We have followed another approach and as it
turned out, to great advantage. In our terminology
we made a strict distinction between memory units
(we called them "pages" and had "core pages" and
"drum pages") and corresponding information units
(for lack of a better word we called them "segments")
a segment just fitting in a page. For segments we
created a completely independent identification
mechanism in which the number of possible segment
identifiers is much larger than the total number of
pages in primary and secondary store. The segment
identifier gives fast access to a so-called "segment
variable" in core whose value denotes whether the
segment is still empty or not and if not empty, in
which page (or pages) it can be found.
As a consequence of this approach: if a segment
of information, residing in a core page, has to be
dumped onto the drum in order to make the core page
available for other use, there is no need to return
the segment to the same drum page as it originally
came from. In fact, this freedom is exploited: among
the free drum pages the one with minimum latency
time is selected.
A next consequence is the total absence of a
drum allocation problem: there is not the slightest
reason why, say, a program should occupy consecutive
drum pages. In a multiprogramming environment this
is very convenient.
Processor Allocation.
We have given full recognition to the fact that
in a single sequential process (such as performed by
a sequential automaton) only the time succession of
the various states has a logical meaning, but not
the actual speed with which the sequential process
is performed. Therefore we have arranged the whole

system as a society of sequential processes, progressing with undefined speed ratios. To each user
program, accepted by the system, corresponds a
sequential process, to each input peripheral
corresponds a sequential process (buffering input
streams in synchronism with the execution of the
input commands), to each output peripheral corresponds a sequential process (unbuffering output
streams in synchronism with the execution of the
output commands); furthermore we have the "segment
controller" associated with the drum and the
"message interpreter" associated with the console
keyboard.
This enabled us to design the whole system in
terms of these abstract "sequential processes".
Their harmonious co-operation is regulated by means
of explicit mutual synchronization statements. On
the one hand, this explicit mutual synchronization
is necessary, as we do not make any assumption about
speed ratios, on the other hand this mutual synchronization is possible because "delaying the progress
of a process temporarily" can never be harmful to
the interior logic of the process delayed. The
fundamental consequence of this approach -viz. the
explicit mutual synchronization- is that the
harmonious co-operation of a set of such sequential
processes can be established by discrete reasoning;
as a further consequence the whole harmonious
society of co-operating sequential processes is
independent of the actual number of processors
available to carry out these processes, provided
the processors available can switch from process
to process.
System Hierarchy.
The total system admits a strict hierarchical
structure.
At level 0 we find the responsibility for
processor allocation to one of the processes whose
dynamic progress is logically permissible (i.e. in
view of the explicit mutual synchronization). At
this level the interrupt of the real time clock is
processed, introduced to prevent any process to
monopolize processing power. At this level a priority
rule is incorporated to achieve quick response of the
system where this is needed. Our first abstraction
has been achieved, above level 0 the number of
processors actually shared is no longer relevant. At
the higher levels we find the activity of the
different sequential processes, the actual processor
having lost its identity, having disappeared from
the picture.
At level I we have the so-called "segment
controller", a sequential process synchronized with
respect to the drum interrupt and the sequential
processes on higher levels. At level I we find the
responsibility to cater for the bookkeeping resulting
from the automatic backing store. At this level our
next abstraction has been achieved: at all higher
levels identification of information takes place in
terms of segments, the actual storage pages having
lost their identity, having disappeared from the

picture.
At level 2 we find the "message interpreter",
taking care of the allocation of the console keyboard
via which conversations between te operator and any
of the higher level processes can be carried out. The
message interpreter works in close synchronism with
the operator: when the operator presses a key, a
character is sent to the machine together with an
interrupt signal to announce this next keyboard
character, while the actual printing is then done
on account of an output command generated by the
machine under control of the message interpreter.
(As far as the hardware is concerned the console
teleprinter is regarded as two independent peripherals: an input keyboard and an output printer.) If
one of the processes opens a conversation it identifies itself for the benefit of the operator in the
opening sentence of this conversation. If, however,
the operator opens a conversation he must identify
the process he is addressing, in the opening sentence
of the conversation, i.e. this opening sentence must
be interpreted before it is known to which of the
processes the conversation is addressed~ There lies
the logical reason to introduce a separate sequential
process for the console teleprinter, a reason that
is reflected in its name "message interpreter". Above
level 2 it is as if each process had its private
conversational console. The fact that they share the
same physical console is translated into a resource
restriction of the form "only one conversation at a
time", a restriction that is satisfied via mutual
synchronization. At this level the next abstraction
has been implemented: at the higher levels the actual
console teleprinter has lost its identity. (If the
message interpreter had not been on a higher level
than the segment controller, then the only way to
implement it would have been to make a permanent
reservation in core for it; as the conversational
vocabulary might get large (as soon as our operators
wish to be addressed in fancy messages) this would
result in too heavy a permanent demand upon core
storage. Therefore the vocabulary in which the
messages are expressed is stored on segments, i.e.
as information units that can reside on the drum as
well. For this reason the message interpreter is of
a level one higher than the segment controller.)
At level 3 we find the sequential processes
associated with buffering of input streams and
unbuffering of output streams. At this level the next
abstraction is effected, viz. the abstraction of the
actual peripherals used, that are allocated at this
level to the "logical communication units" in terms
of which is worked in the still higher levels. The
sequential processes associated with the peripherals
are of a level above the message interpreter, because
they must be able to converse with the operator (e.g.
in the case of detected malfunctioning). The limited
number of peripherals again acts as a resource
restriction for the processes at higher levels, to be
satisfied by mutual synchronization between them.
At level 4 we find the independent user programs,
at level 5 the operator (not implemented by us).

The system structure has been described at
length in order to make the next section intelligible,
Design Experience
The conception stage took a long time. During
that period of time the concepts have been born in
terms of which we sketched the system in the previous
section. Furthermore we learnt the art of reasoning
by which we could deduce from our requirements the
way in which the processes should influence each
other as regards mutual synchronization so that
these requirements would be met. (The requirements
being that no information can be used before it has
been produced, that no peripheral can be set to two
tasks simultaneously, etc.) Finally we learnt the
art of reasoning by which we could prove that the
society composed of processes thus mutually synchronized by each other, would indeed in its time
behaviour satisfy all requirements.
The construction stage has been rather traditional, perhaps even old-fashioned: plain machine
code. Reprogramming on account of a change of
specifications has been rare, a circumstance that
must have contributed greatly to the feasibility of
the "steam method". The fact that the first two
stages took more time than planned was somewhat
compensated by a delay in the delivery of the machine.
In the verification stage we had, during short
shots, the machine completely at our disposal, shots
during which we worked with a virgin machine without
any software aids for debugging. Starting at level 0
the system has been tested, each time adding (a
portion of) the next level only after the previous
level had been thoroughly tested. Each test shot
itself contained on top of the (partial) system to
be tested a number of testing processes with a double
function. Firstly they had to force the system into
all different relevant states, secondly they had to
verify that the system continued to react according
to specification.
I shall not deny that the construction of these
testing programmes has been a major intellectual
effort: to convince oneself that one has not overlooked "e relevant state" and to convince oneself
that the testing programmes generate them all is no
simple matter. The encourageing thing is that (as
far as we are aware~) it could be done.
This fact was one of the happy consequences of
the hierarchical structure.
Testing level 0 (the real time clock and processor allocation) implied a number of testing
sequential processes on top of it, inspecting together that under all circumstances processor time
was divided among them according to the rules. This
being established, sequential processes as such had
been implemented.
Testing the segment controller at level I
meant that all "relevant states" could be formulated
in terms of sequential processes making (in various
combinations) demands on core pages, situations that

could be provoked by explicit synchronizing among
the testing programs. At that stage the existence
of the real time clock -although interrupting all
the time- was so immaterial that one of the testers
indeed forgot its existence~
By that time we had implemented the correct
reaction upon the (mutually unsynchronized) interrupts from the real time clock and the drum. If we
had not introduced the separate levels 0 and I and
if we had not created a terminology (viz. that of
the rather abstract sequential processes) in which
the existence of the clock interrupt could be discarded, but had tried instead to make in a nonhierarchical construction the central processor
directly react upon any weird time succession of
these two interrupts, the number of "relevant states"
would have exploded to such a height that exhaustive
testing would have been an illusion. (Apart from that
it is doubtful wether we would have had the means to
generate them all, drum and clock speed being outside
our control.)
For the sake of completeness I must mention
a further happy consequence. As stated before, above
level I core and drum pages have lost their identity
and buffering of input and output streams (at level
3) therefore occurs in terms of segments. While
testing at level 2 or 3 the drum channel hardware
broke down for quite some time, but testing could
proceed by restricting the number of segments so
that they all could be held in core. If building
up the line printer output streams had been implemented as "dumping onto the drum" and the actual printing as "printing from the drum" this advantage would
have been denied to us.
Conclusion
As far as program verification is concerned I
present nothing essentially new. In testing a
general purpose object (be it a piece of hardware,
a program, a machine or a system) one cannot subject
it to all possible cases: for a computer this would
imply that one feeds it with ell possible programs!
Therefore one must test it with a set of relevant
test cases. What is relevant or not, cannot be
decided as long as one regards the mechanism as a
black box, in other words it has to follow from the
internal structure of the mechanism to be tested. It
seems the designer's responsibility to construct his
mechanism in such a way -i.e. so effectively structured- that at each stage of the testing procedure
the number of relevant test cases is so small that
he can try them all and that what is being tested is
so perspicuous that it is clear that he has not
overlooked a situation. I have presented a survey of
our system because I think it a nice example of the
form that such a structure might take.
In my experience, I am sorry to say, industrial
software makers tend to react to it with mixed
feelings. On the one hand they are inclined to judge
that we have done a kind of model job, on the other
hand they express doubts whether the techniques used
are applicable outside the sheltered atmosphere of a

University Department and express the opinion that
we could only do it this way thanks to the modest
scope of the whole project. It is not my intention
to underestimate the organizing ability needed for
a much bigger job with ten or more times as many
people, but I should like to venture the opinion
that the larger the project, the more essential the
structuring~ A hierarchy of five logical levels
might then very well turn out to be of modest depth,
in particular when one designs the system more
consciously than we have done with the aim that the
software can be smoothly adapted to (perhaps drastic)
configuration expansions.
Acknowledqements
I should not like to publish this progress
report without expressing my great indebtedness to
my five collaborators C.Bron, A.N.Habermann, F.J.A.
Hendriks, C.Ligtmans and P.A.Voorhoeve. They have
contributed to ell the stages of the design, together
we learnt the art of reasoning needed. Construction
and verification is entirely their effort: if my
dreams have become true, this is due to their faith,
their talents and their persistent loyalty to the
whole project.
Finally I should like to thank the members of
the program committee who asked for more information
on the synchronizing primitives and some justification of my claim to be able to prove logical soundness a priori. In answer to this request the appendix
has been added, of which I hope that it gives the
desired information and justification.

"V(sem)" increases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is positive, the V-operation in question
has no further effect; if, however, the resulting
value of the semaphore concerned is non-positive,
one of the processes booked on its waiting list is
removed from this waiting list, i.e. its dynamic
progress is again logically permissible and in due
time a processor will be allocated to it (again, see
above "System Hierarchy", at level 0).
Corollary I:
If a semaphore value is nonpositive its absolute value equals the number of
processes booked on its waiting list.
Corollary 2:
The P-operation represents the
potential delay, the complementary V-operation
represents the removal of a barrier.
Note I:
P- and V-operations are "indivisible
actions", i.e. if they occur "simultaneously" in
parallel processes, they are non-interfering in the
sense that they can be regarded as being performed
the one after the other.
Note 2:
If the semaphore value resulting
from a V-operation is negative, its waiting list
did originally contain more than one process. It is
undefined -i.e. logically immaterial- which of the
waiting processes is then removed from the waiting
list.
Note 3:
A consequence of the mechanisms
described above is that a process whose dynamic
progress is permissible can only loose this status
by actually progressing, i.e. by performance of a
P-operation on a semaphore with a value that is
initially non-positive.

Appendix
The S~nchronizinq Primitives.
Explicit mutual synchronization of parallel
sequential processes is implemented via so-called
"semaphores". They are special purpose integer
variables allocated in the universe in which the
processes are embedded, they are initialized (with
the value 0 or I) before the parallel processes
themselves are started. After this initialization
the parallel processes will access the semaphores
only via two very specific operations, the so-called
synchronizing primitives. For historical reasons
they are called the P-operation and the V-operation.
A process, "Q" say, that performs the operation
"P(sem)" decreases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is non-negative, process Q can continue
with the execution of its next statement; if,
however, the resulting value is negative, process
Q is stopped and booked on a waiting list associated
with the semaphore concerned. Until further notice
(i.e. a V-operation on this very same semaphore)
dynamic progress of process Q is not logically
permissible and no processor will be allocated to
it (see above "System Hierarchy", at level 0).
A process, "R" say, that performs the operation

During system conception it transpired that we
used the semaphores in two completely different
ways. The difference is so marked that, looking
back, one wonders whether it was really fair to
present the two ways as a usage of the very same
primitives. On the one hand we have the semaphores
used for mutual exclusion, on the other hand the
private semaphores.
The Mutual Exclusion.
In the following program we indicate two
parallel, cyclic processes (between the brackets
"parbeRin" and '~arend") that come into action
after the surrounding universe has been introduced
and initialized.
begin semaphore mutex; mutex := I;
parbegin
beqin L I : P(mutex); critical section I; V(mutsx);
remainder of cycle I; ~oto LI
end;
beqin L2: P(mutex); critical section 2; V(mutex);
remainder of cycle 2; qoto L2
end
parend
end
As a result of the P- and V-operations on

"mutex" the actions, marked as "critical sections"
exclude each other mutually in time; the scheme
given allows straightforward extension to more than
two parallel processes, the maximum value of mutex
= I, the minimum value = - (n - I) if we have n
parallel processes.
Critical sections are used always and only for
the purpose of unambiguous inspection and modification of the state variables (allocated in the
surrounding universe) that describe the current
state of the system (as far as needed for the
regulation of the harmonious co-operation between
the various processes).
The Private Semaphores.
Each sequential process has associated with it
a number of private semaphores and no other process
will ever perform a P-operation on them. The universe
initializes them with the value = O, their maximum
value = I, their minimum value = - I.
Whenever a process reaches a stage where the
permission for dynamic progress depends on current
values of state variables, it follows the pattern:

P(mutex);
"inspection and modification of state variables
including a conditional V(private semaphore)";

V(mutex);
P(private semaphore)
If the inspection learns that the process in
question should continue, it performs the operation
"V(private semaphore)" -the semaphore value then
changes from 0 to I-, otherwise this V-operation
is skipped, leaving to the other processes the
obligation to perform this V-operation at a suitable
moment. The absence or presence of this obligation
is reflected in the final values of the state
variables upon leaving the critical section.
Whenever a process reaches a stage where as a
result of its progress possibly one (or more)
blocked processes should now get permission to
continue, it follows the pattern

P(mutex);
"modification and inspection of state variables
including zero or more V-operations on private
semaphores of other processes";

V(mutex)
By the introduction of suitable state variables
and appropriate programming of the critical sections
any strategy assigning peripherals, buffer areas etc.
can be implemented.
The amount of coding and reasoning can be
greatly reduced by the observation that in the two
complementary critical sections sketched above, the
same inspection can be performed by the introduction
of the notion of "an unstable situation", such as
a free reader and a process needing a reader.
Whenever an unstable situation emerges it is
removed (including ome or more V-operations on

private semaphores) in the very same critical
section in which it has been created.
Provinq the Harmonious Co-operation.
The sequential processes in the system can all
be regarded as cyclic processes in which a certain
neutral point can be marked, the so-called "homing
position", in which all processes are when the
system is at rest.
When a cyclic process leaves its homing position
"it accepts a task", when the task has been performed
end not earlier, the process returns to its homing
position. Each cyclic process has a specific task
processing power (e.g. the execution of a user
program or unbuffering a portion of printer output,
etc.)
The harmonious co-operation is mainly proved
in roughly three stages.
I)
It is proved that although a process
performing a task may generate in doing so a finite
number of tasks for other processes, a single
initial task cannot give rise to an infinite number
of task generations. The proof is simple as
processes can only generate tasks for processes
at lower levels of the hierarchy so that circularity
is excluded. (If a process needing a segment from
the drum has generated a task for the segment
controller, special precautions have been taken to
ensure that the segment asked for remains in core
at least until the requesting process has effectively
accessed the segment concerned. Without this precaution finite tasks could be forced to generate an
infinite number of tasks for the segment controller
and the system could get stuck in an unproductive
page flutter.)
2)
It is proved that it is impossible
that all processes have returned to their homing
position while somewhere in the system is still
pending a generated but unaccepted task. (This is
proved via instability of the situation just
described.)
3)
It is proved that after the acceptance
of an initial task all processes eventually will be
(again) in their homing position. Each process
blocked in the course of task execution relies on the
other processes for removal of the barrier. Essentially, the proof in question is a demonstratmon of the
absence of "circular waits": process P waiting for
process Q waiting for process R waiting for process
P. (Our usual term for the circular wait is "the
Deadly Embrace".) In a more general society than
our system this proof turned out to be e proof by
induction (on the level of hierarchy, starting at
the lowest level) as A.N.Habermann has shown in his
doctoral thesis.

THE WORKING SET MODEL FOR PROGRAM BEHAVIOR
Peter J. Denning
Massachusetts Institute of Technology
Cambridge, Massachusetts

SUMMARY

We claim neither is adequate.

Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate
model for program behavior.
In this paper a new
model is developed, the "working set model", which
enables us to decide which information is in use
by a running program and which is not.
Such knowledge is vital for dynamic management of paged
memories.
The working set of pages associated
with a process, defined to be the collection of its
most recently used pages, is a useful allocation
concept. A proposal for an easy-to-implement
allocation policy is set forth; this policy is
unique, inasmuch as it blends into one decision
function the heretofore independent activities of
process-scheduling and memory-management.

Because resources are multiplexed, each user
is given the illusion that he has a complete computing system at his sole disposal: a virtual
computer.
For our purposes, the basic elements of
a virtual computer are its virtual processor and
an "infinite" one-level virtual memory.
Dynamic
"advice" regarding resource requirements cannot be
obtained successfully from users for several
reasons:
i. A user may build his program on the work
of others, frequently sharing procedures
whose time and storage requirements may be
either unknown or, because of data dependence, indeterminate.
Therefore he cannot
be expected to estimate processor-memory
needs.
2. It is not clear what sort of "advice" might
be solicited.
Nor is it clear how the
operating system should use it, for overhead incurred by using advice could well
negate any advantages attained.
3. Any advice acquired from a user would be
intended (by him) to optimize the environment for his own program.
Configuring
resources to suit individuals may interfere
with overall good service to the community
of users.
Thus it seems inadvisable at the present time to
permit users, at their discretion, to advise the
operating system of their needs.

INTRODUCTION
Resource allocation is tricky business.
In
recent years there has been much dialogue on the
topics of process scheduling and core memory management, yet development of techniques has progressed independently along both these lines. No
one will deny that a unified approach is needed.
Probably the most basic reason behind the absence
of a general treatment is the lack of an adequate
model for program behavior.
In this paper we develop a new model, the working set model, which
embodies certain important behavioral properties
of computations operating in a multiprogrammed environment, enabling us to decide which information
is in use by a running program and which is not.
We do not intend that the proposed model be considered "final"; rather, we hope to stimulate a
new kind of thinking, thinking that may be of considerable help in solving many operating system
design problems.
The working set is intended to model the behavior of programs in the general purpose computer
system, or computer utility.
For this reason we
assume that the operating system must on its own
determine the behavior of programs it runs; it
cannot count on outside help.
Two commonly proposed sources of externally-supplied dynamic allocation information are the user and the compiler.

Work reported herein was supported in part by
Project MAC, an M.I.T. research project sponsored
by the Advanced Projects Research Agency, Dept.
of Defense, under Office of Naval Research Contract Number Nonr-4102(Ol).

Likewise, compilers cannot be expected to
supply information, extracted from the structure
of the program , regarding resource requirements:
i. Programs will be modular in construction;
information about other modules may be unavailable at compilation time.
Because of
dependence on data there may be no way to
decide (until run time) just which modules
will be included in a computation.
2. Compilers cluttered with extra machinery
to predict memory needs will be slower in
operation.
Many users are less interested
in whether their programs operate efficiently than whether they operate at all, and
so are concerned with rapid compilation.
Furthermore, the compiler is an often-used
component of the operating system; if slow
and bulky, it can be a serious drain on
system resources.

**Ramamoorthy I has put forth a proposal for automatic segmentation of programs during compilation.

Therefore in this paper we are advocating mechanisms that monitor the behavior of a computation,
making allocation decisions on the basis of currently observed characteristics.
Only a mechanism
that oversees the behavior of a program in operation can cope with arbitrary interconnections of
arbitrary modules having arbitrary characteristics.
Our treatment proceeds as follows. First we
define the type of computer system in which our
ideas are developed. After a brief discussion of
previous work with the problems of dynamic memory
management, we define the working set model. We
discuss a method of implementing memory management
based on this model, and indicate how working set
notions can be used to blend process scheduling
and memory management into one decision function,
accounting simultaneously for both types of demand.
Finally we discuss how data sharing fits into the
working-set scheme.

THE FRAMEWORK
We assume that the reader is already familiar
with the concepts of a computer utility 2'3'4, of
segmentation and paging 5'6, of program and addressing structure 6'8, so we will only mention these
topics here. Briefly, each process has access to
its own private, segmented name space; each segment known to the process is sliced into equalsize units, called pages, to facilitate mapping it
into the paged main memory.
Associated with each
segment is a page table, whose entries point to

the segment's pages. An "in-core" bit in each
page table entry is turned ON whenever the designated page is present in main memory ; an attempt
to reference a page whose "in-core" bit is OFF
causes a page fault, initiating proceedings to
secure the missing page. Finally, a process has
three states of existence: running, when a processor is assigned to it; ready, when it would be
running if only a processor were available; or
blocked, when it has no need of a processor (for
example, during a page fault or during a console
interaction). When talking about processes in execution, we will have to distinguish between "process time" and "real time". Process time is time
as seen by a process unaware it is suspended; that
is, as if it executed without interruptions.
We restrict attention to a two-level memory
system, indicated by Figure I. Only data residing
in main memory is accessible to a processor; all
other data reside in auxiliary memory, which we
regard as having i nflnite capacity. There is a
time T, the traverse time, involved in transferring
a page between memories. T is measured from the
moment a page fault occurs until the moment the
missing page is in main memory ready for use. T
is actually the expectation of a random variable
composed of waits in queues and mechanical positioning delays.
Though it usually takes less time to
store into auxiliary memory than to read from it,
we shall regard the traverse time T to be the same
regardless of which direction a page is moved.
,
Consistent with current usage, we will use the
terms "core memory" and "main memory" interchangeably.

PROCESSORS

~

~ ~

~ n

data flow from main memory
(controlled b y ~, c o r emanager)
I

main memÂ°rY(contrD~edfl~ ~ ~ ~ n m ~ [ c Y i e s )

FIGURE i.

Two-level memory system.

A basic allocation problem, "core memory management", is that of deciding just which pages are to
occupy main memory.
The basic strategy advocated
here -- a compromise against a lot of expensive
,
main memory -- is to minimize page traffic . There
are two reasons for this:
i. The more the data traffic between the two
levels of memory, the more the computational overhead in deciding just what to move
and where to move it.
2. Because the traverse time T is long compared
to a memory cycle, too much data movement
can result in congestion and serious interference with processor efficiency.
Roughly speaking, a working set of pages is
the minimum collection of pages that must be loaded
in main memory for a process to operate efficiently,
without "unnecessary" page faults. According to
our definitions, a "process" and its "working set"
are but two manifestations of the same ongoing
computational activity.

PREVIOUS WORK
In this section we outline strategies that have
been set forth in the past for memory management;
the interested reader will be referred to the literature for detail.
We regard management of paged memories to operate in two stages:
I. Pagin_g in." locate the required page in
auxiliary memory, load it into main memory,
turn the "in-core" bit of the appropriate
page table entry ON.
2. Paging out: remove some page from main memory, turn the "in-core" bit of the appropriate page table entry OFF.
Management algorithms can be classified according
to their methods of paging in and paging out.
It
is a common characteristic of nearly every strategy
that paging in is done on demand; that is, no action
is taken to load a page into memory until some
process attempts to reference it. To date there
have been no proposals recommending look-ahead, or
anticipatory page-loading, because (as we have
stressed) there is no reliable advance source of
allocation information, be it the programmer or
the compiler.
Although the working set is the
desired information, it might still be futile to
pre-load pages: there is no guarantee that a process will not block shortly after resumption,
having referenced only a fraction of its working
set.
The operating system could devote its already precious time to activities more rewarding
than loading pages which may not be used.
Thus we
will assume that paging in is done on demand only,
via the page fault mechansim.

Since data is stored and transmitted in units of
pages, we can (without ambiguity) refer to data
movement as "page traffic".

The chief problem in memory management is not
deciding which pages to load; it is deciding which
pages ought to be removed.
For if the page with
the least likelihood of being used in the immediate
future is retired to auxiliary memory, the best
choice has been made.
Nearly every worker in the
field has recognized this.
Debate has arisen over
which strategy to employ for retiring pages; that
is, which page-turning, or replacement, algorithm
to use.
A good measure of performance for a paging
policy is page traffic (the number of pages per
unit time being moved between memories), since
erroneously removed pages add to the traffic of
returning pages.
In the following we will use this
as a basis of comparison for several strategies.
Random selection.
Whenever a fresh page of memory
is needed, a page is selected at random to be replaced.
Although utterly simple to implement, this
method frequently removes useful pages (which must
therefore be recalled) and so results in high page
traffic.
~!%~
selection. The pages of main memory are ordered in a cyclic list.
Suppose the M pages of
main memory are numbered 0,1,...,(M-I) and a
pointer k indicates that the k-th page was most
recently paged in. Whenever a fresh page of memory
is needed, [(k+l) mod M] 4 k, page k is retired,
and another page brought in to fill the now vacant
slot.
This method -- also utterly simple to realize -- is based on the principle that programs tend
to follow sequences of instructions, so that references in the immediate future will most likely
be close to present references.
So, assuming there
is this tendency for page references to cluster,
and assuming some kind of uniformity in process
scheduling techniques, the page which has been in
memory longest is least likely to be reused: hence
the cyclic list. We see two ways in which this
algorithm can fail. First we question its basic
assumption.
It is not at all clear that modular
programs, which execute numerous inter-module calls,
will indeed exhibit sequential instruction fetch
patterns.
The thread of control will not string
pages together; rather, it will entwine them intricately.
[Fine, Mclssac, and Jackson 9 have some
experimental evidence in support of this reasoning.]
Second, this algorithm is subject to overloading when used in multiprogrammed memories.
When core demand is too heavy, one cycle of the
list completes rapidly and the pages deleted are
still needed by their processes.
This can create
a self-intensifying crisis.
Programs, deprived of
still-needed pages, generate a plethora of page
faults; the resulting traffic of returning pages
displaces still other useful pages, leading to more
page faults, and so on.
Oldest-unused selection.
Each page table entry
contains a ~us~ ~ bit, set ON each time the page is
referenced.
At periodic intervals all the page
table entries are searched and usage records updated.
When a fresh page of memory is needed, the page unreferenced for the longest time is removed.
One can

see that this method is intrinsically reasonable by
considering the simple case of a computer where
there is exactly one process whose pages cannot all
fit into main memory.
In this case the most reasonable choice for a page to replace is the oldest
unused page.
Unfortunately this method too is susceptible to overloading when many processes compete
for main memory.
ATLAS ioo~ detection method.

The Ferranti ATLAS

computer I0 had proposed a page-turning policy that
attempted to detect loop behavior in page reference
patterns, then minimize page traffic by maximizing
the time between page transfers, that is, by removing pages not expected to be needed for the longest time.
It was successful -- only for looping
programs.
Performance was unimpressive for programs exhibiting random reference patterns.
Implementation was costly.
Various studies concerning behavior of paging
algorithms have appeared.
Fine, Mclssac and
Jackson 9 have investigated the effects of demand
paging and have questioned whether paging is beneficial at all. We do not feel that their conclusion applies to the kind of multiprogrammed environment we have described.
They studied fixed-size
programs, that quickly acquired and retained a
large fraction of their pages.
Highly interactive,
modular programs are likely to behave differently.
Not only may program size vary dynamically (according to data dependencies), but also such programs
should be using a small fraction of their pages at
any one time, and the membership in this set of
working pages should be changing constantly.

We define the working set of information W(t,T)
of a process at time t to be the collection of data
items referenced by the process during the proces
time interval (t-T~,t).
Thus, the data items a process has referenced
during the last T seconds of its execution comprise
its working set. q will be called the working set
parameter.
We will regard the data items in W(t,T)
as being pages ,although they could just as well
be any other named data objects.
The working set
size ~(t,~) is
(I)

~(t,T)

=

Number of pages in W(t,T)

A working set W(t,~) has two important, general
properties.
Both are properties of typical programs,
and need not hold in special cases.
PI. Size. It should be clear immediately that
~(t,0) = 0 since no page reference can occur
in zero time.
It should also be clear that
~(t,T) as a function of T is monotonically
increasing, since more pages can be referenced
in longer time intervals.
Because a process
will refer to its most-needed pages frequently
and its least-needed pages infrequently, we
expect ~(t,~) as a function of T to have a
steep initial rise which diminishes to a more
gradual rise.
The general character of ~(t,~)
is suggested by the smoothed curve of Figure 2.
00(t ,~-)

Belady II has compared some of the algorithms
mathematically.
His most important conclusion is
that the "ideal" algorithm should possess much of
the simplicity of Random or Cyclic selection (for
efficiency) and some, though not much, accumulation
of data on past reference patterns.
He has shown
that too much "historical" data can have adverse
effects (witness ATLAS).
In the next section we begin investigation of
the working set concept.
Even though the ideas
are not entirely new 12'13'14, there has been no
detailed documentation publicly available.

THE WORKING SET MODEL
From the programmer's standpoint, the working
set of information is the smallest collection of
procedure and data items that must be present in
main memory to assure efficient execution of his
program.
We have already stressed that there will
be no advance notice from either the programmer or
the compiler regarding what information "ought" to
be in main memory.
It is up to the operating
system to determine on the basis of page reference
patterns whether pages are in use.
Therefore the
working set of information associated with a process is, from the system standpoint, the set of
most recently referenced pages.

FIGURE 2.

Behavior of ~(t,T).

Program modularity enables us to
P2. Correlation.
say something about correlation between the
size of a working set at two times, t and (t+c~).
Correlation is useful in devising storage
allocators, for the higher the correlation
between ~(t,T) and w(t+c~,T), the better is
~(t,T) a prediction of ~(t+c~,T).
In modular
programs, control passes randomly from one
module to another; if T is chosen properly
(as discussed in the next section), it is more
likely that a working set will change size
smoothly, less likely that it will change size
abruptly.
Thus for small time separations
(say, 6 < < T), ~(t,~) and ~(t+c~,T) are highly

correlated, meaning that a measurement of ~(t,T)
will serve as a good estimate of the memory requirement during the process time interval (t,t+c0.
For large time separations ~ (say, ~ >> .[), control
will have passed through a great many modules
during the interval (t,t+c~); thus ~(t,T ) gives
little information about ~(t+C~,7), and so ~(t,7)
and ~(t+c~,7) have much less correlation than for
small 5. This behavior is suggested in Figure 3.
Correlation between
~(t,7) and ~(t+Cz,7)

Detecting W(t~T )
According to our definition, W(t,T) is the set
of its pages a process has referenced within the
last T seconds of its execution.
This suggests
that memory management can be controlled by hardware mechanisms, by associating with each page of
main memory a timer.
Each time a page is referenced, its timer is set to T and begins to run
down; if the timer succeeds in running down, a flag
is set to mark the page for removal whenever the
space is needed.
In the appendix we describe such
a hardware memory management mechanism, hardware
that can be housed within the memory boxes.
The
mechanism has two interesting features:
i. It operates asynchronously and independently
of the supervisor, whose only respsonsibility
in memory management is handling page faults.
Quite literally, memory manages itself.
2. Analog devices such as capacitative timers
could be used to measure intervals.

~D

c~
0
FIGURE 3.

Correlation between working set sizes,

Choice of T

Unfortunately it is not practical to add on
hardware to existing systems. We seek a method of
handling memory management within the software.
The procedure we propose here samples the page
table entries of pages in core memory at process
time intervals of ~ seconds (~ is called the
sampling interval) where ~ = y/K , K an integer
constant chosen to make the sampling intervals as
"fine grain" as desired.
On the basis of page
references during each of the last K sampling intervals, the working set W(t,l~) can be determined,
as follows.

The value ultimately selected for ~ will reflect
efficiency requirements and will be influenced by
"in- core"
system parameters such as core memory size and memI
ory traverse time.
For example, if T is too small,
pages may be removed from main memory while they
are still useful, and high page traffic may result
from returning pages.
If T is too large, pages
may remain in main memory long after last being
used, and wasted main memory may result.
Thus the
value of T will have to represent a compromise between too much page traffic and too much wasted
memory space.
0

l

pointer
to page

use bits

luol-,l

---

TYPICAL PAGE TABLE ENTRY

SHIFT AT END OF SAMPLING INTERVAL
The following consideration leads us to recommend for T a value comparable to the memory traverse time T (Figure I). Consider a process that
is running continuously, being interrupted only
for page faults.
Assuming that memory allocation
procedures balk at removing from main memory any
page in a working set, once a page has entered
W(t,T) it will remain in main memory for at least
T seconds.
Under the very worst of page-shuffling
conditions, a page could be dispatched to auxiliary memory and be recalled immediately; the time
for this round trip is two traverse times, 2T.
Therefore a highly-shuffled page would spend
roughly T/2T of its time in main memory.
So,
for example, if we wished to insure that a page is
available in main memory (when needed) for not less
than 50 per cent of the time, we would have to
choose â¢ ~ 2T.

FIGURE 4.

Page table entries for detecting W(t,Kcy).

As indicated by Figure 4, each page table entry
contains an "in-core" bit M, where M=i if and only
if the page is present in main memory.
It also
contains a string of use bits u0,ul,...,u K. Each
time a page reference occurs,

I ~ u 0.

At the end

of each sampling interval ~, the bit pattern contained in u0,ul,...,u K is shifted one position,
a 0 enters u 0, and u K ipapers.txt
]
[0x6322a15a9bbc -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a9bb8 -> ftype: SWAP
infile: true
block: 11
filename: @SWAP
]
[0x6322a15a9bb4 -> ftype: SWAP
infile: false
block: 10
filename: @SWAP
]
[0x6322a15a9bb0 -> ftype: SWAP
infile: true
block: f
filename: @SWAP
]
[0x6322a15a7478 -> ftype: SWAP
infile: false
block: 5
filename: @SWAP
]
[0x6322a15a7474 -> ftype: SWAP
infile: false
block: 4
filename: @SWAP
]
[0x6322a15a9bd8 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a63ac -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a9bd4 -> ftype: FILE_B
infile: true
block: 2
filename: papers.txt
]
[0x6322a15a63b0 -> ftype: FILE_B
infile: true
block: 2
filename: papers.txt
]
[0x6322a15a84e0 -> ftype: SWAP
infile: true
block: 6
filename: @SWAP
]
[0x6322a15a84e4 -> ftype: SWAP
infile: false
block: 7
filename: @SWAP
]
[0x6322a15a84e8 -> ftype: SWAP
infile: true
block: 8
filename: @SWAP
]
[0x6322a15a84ec -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f0 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f8 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a84fc -> ftype: FILE_B
infile: true
block: 2
filename: papers.txt
]
[0x6322a15a8500 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a8504 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a7470 -> ftype: SWAP
infile: false
block: 3
filename: @SWAP
]
[0x6322a15a8508 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bc0 -> ftype: FILE_B
infile: true
block: 2
filename: papers.txt
]
[0x6322a15a9bc4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bc8 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bdc -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a63a0 -> ftype: SWAP
infile: true
block: c
filename: @SWAP
]
[0x6322a15a9bcc -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bd0 -> ftype: FILE_B
infile: false
block: 1
filename: papers.txt
]
[0x6322a15a9be0 -> ftype: FILE_B
infile: true
block: 0
filename: qapers.txt
]
[0x6322a15a63a4 -> ftype: SWAP
infile: true
block: d
filename: @SWAP
]
[0x6322a15a63a8 -> ftype: SWAP
infile: true
block: e
filename: @SWAP
]
filemap: 
[·-!boe!pee addresses used
where an even address is required. Such faults cause the
processor to trap to a system routine. When an illegal
action is caught, unless other arrangements have been
made, the system terminates the process and writes
the user's image on file core in the current directory. A
debugger can be used to determine the state of the
program at the time of the fault.
Programs which are looping, which produce unwanted output, or about which' the user has second
thoughts may be halted by the use of the interrupt
signal, which is generated by typing the "delete"
character. Unless special action has been taken, this
signal simply causes the program to cease execution
without producing a core image file.
There is also a quit signal which is used to force a
core image to be produced. Thus programs which loop
unexpectedly may be halted and the core image examined without prearrangement.
The hardware-generated faults and the interrupt and
quit signals can, by request, be either ignored or caught
by the process. For example, the Shell ignores quits to"
prevent a quit from logging the user out. The editor
catches interrupts and returns to its c o m m a n d level.
This is useful for stopping long printouts without losing
work in progress (the editor manipulates a copy of
the file it is editing). In systems without floating point
hardware, unimplemented instructions are caught, and
floating point instructions are interpreted.

8. Perspective
Perhaps paradoxically, the success of UNIX is largely
due to the fact that it was not designed to meet any
predefined objectives. The first version was written
when one of us (Thompson), dissatisfied with the
available computer facilities, discovered a little-used
Communications
of
the ACM

July 1974
Volume 17
Number 7

PDP-7 and set out to create a more hospitable environment. This essentially personal effort was sufficiently
successful to gain the interest of the remaining author
and others, and later to justify the acquisition of the
POP-11/20, specifically to support a text editing and
formatting system. When in turn the 11/20 was outgrown, UNIX had proved useful enough to persuade
management to invest in the PDP-11/45. Our goals
throughout the effort, when articulated at all, have
always concerned themselves with building a comfortable relationship with the machine and with exploring
ideas and inventions in operating systems. We have
not been faced with the need to satisfy someone else's
requirements, and for this freedom we are grateful.
Three considerations which influenced the design
of UNIX are visible in retrospect.
First, since we are programmers, we naturally
designed the system to make it easy to write, test, and
run programs. The most important expression of our
desire for programming convenience was that the
system was arranged for interactive use, even though
the original version only supported one user. We bebelieve that a properly-designed interactive system is
much more productive and satisfying to use than a
" b a t c h " system. Moreover such a system is rather
easily adaptable to noninteractive use, while the converse is not true.
Second, there have always been fairly severe size
constraints on the system and its software. Given the
partially antagonistic desires for reasonable efficiency
and expressive power, the size constraint has encouraged
not only economy but a certain elegance of design.
This may be a thinly disguised version of the "salvation through suffering" philosophy, but in our case it
worked.
Third, nearly from the start, the system was able to,
and did, maintain itself. This fact is more important
than it might seem. If designers of a system are forced
to use that system, they quickly become aware of its
functional and superficial deficiencies and are strongly
motivated to correct them before it is too late. Since
all source programs were always available and easily
modified on-line, we were willing to revise and rewrite
the system and its software when new ideas were
invented, discovered, or suggested by others.
The aspects of UNIX discussed in this paper exhibit
clearly at least the first two of these design considerations. The interface to the file system, for example, is
extremely convenient from a programming standpoint.
The lowest possible interface level is designed to
eliminate distinctions between the various devices and
files and between direct and sequential access. N o
large "access method" routines are required to insulate
the p r o g r a m m e r from the system calls; in fact, all
user programs either call the system directly or use a
small library program, only tens of instructions long,
which buffers a number of characters and reads or
writes them all at once.

Another important aspect of programming convenience is that there are no "control blocks" with a
complicated structure partially maintained by and depended on by the file system or other system calls.
Generally speaking, the contents of a program's address
space are the property of the program, and we have
tried to avoid placing restrictions on the data structures
within that address space.
Given the requirement that all programs should be
usable with any file or device as input or output, it is
also desirable from a space-efficiency standpoint to push
device-dependent considerations into the operating system itself. The only alternatives seem to be to load
routines for dealing with each device with all programs,
which is expensive in space, or to depend on some means
of dynamically linking to the routine appropriate to
each device when it is actually needed, which is expensive either in overhead or in hardware.
Likewise, the process control scheme and c o m m a n d
interface have proved both convenient and efficient.
Since the Shell operates as an ordinary, swappable user
program, it consumes no wired-down space in the
system proper, and it may be made as powerful as
desired at little cost. In particular, given the framework .
in which the Shell executes as a process which spawns
other processes to perform commands, the notions of
I/O redirection, background processes, c o m m a n d flies,
and user-selectable system interfaces all become essentially trivial to implement.

374

Communications
of
the ACM

8.1 Influences

The success of'UNIX lies not so much in new inventions but rather in the full exploitation of a carefully
selected set of fertile ideas, and especially in showing
that they can be keys to the implementation of a small
yet powerful operating system.
The fork operation, essentially as we implemented it,
was present in the Berkeley time-sharing system [8]. On
a number of points we were influenced by Multics, which
suggested the particular form of the I / o system calls
[9] and both the name of the Shell and its general functions. The notion that the Shell should create a process
for each c o m m a n d was also suggested to us by the
early design of Multics, although in that system it was
later dropped for efficiency reasons. A similar scheme
is used by TENEX [10].

9. Statistics

The following statistics from UNIX are presented to
show the scale of the system and to show how a system
of this scale is used. Those of our users not involved in
document preparation tend to use the system for program development, especially language work. There are
few important "applications" programs.

July 1974
Volume 17
Number 7

9.1 Overall
72 user population
14 maximum simultaneous users
300 directories
4400 files
34000 512-byte secondary storage blocks used
9.2 Per day (24-hour d a y , 7 - d a y week basis)
T h e r e is a " b a c k g r o u n d " process t h a t runs at the
lowest possible p r i o r i t y ; it is used to s o a k u p a n y idle
c P u time. It has been used to p r o d u c e a m i l l i o n - d i g i t
a p p r o x i m a t i o n to the c o n s t a n t e - 2, a n d is n o w
g e n e r a t i n g c o m p o s i t e p s e u d o p r i m e s (base 2).
1800 commands
4.3 CPU hours (aside from background)
70 connect hours
30 different users
75 logins

9.3 Command CPU Usage (cut off at 1%)
15.7% Ccompiler
15.2% users' programs
11.7% editor
5.8% Shell (used as a cornmand, including command times)
5.3% chess
3.3% list directory
3.1% document formatter
1.6% backup dumper
1.8% assembler

1.7%
1.6%
1.6%
1.6%
1.4%
1.3%
1.3%
1.1%
1.0%

Fortran compiler
remove file
tape archive
file system consistency
check
library maintainer
concatenate/printfiles
paginate and print file
print disk usage
copy file

9.4 Command Accesses (cut off at 1%)
15.3%
9.6%
6.3%
6.3%
6.0%
6.0%
3.3%
3.2%
3.1%
1.8%
1.8%
1.6%

editor
list directory
remove file
C compiler
concatenate/printfile
users' programs
list people logged on
system
rename/move file
file status
library maintainer
document formatter
execute another command conditionally

1.6%
1.6%
1.5%
1.4%
1.4%
1.4%
1.2%
1.1%
1.1%
1.1%

debugger
Shell (used as a command)
print disk availability
list processes executing
assembler
print arguments
copy file
paginate and print file
print current date/time
file system consistency
check
1.0% tape archive

ficulties such as p o w e r d i p s a n d i n e x p l i c a b l e p r o c e s s o r
i n t e r r u p t s to r a n d o m locations. T h e r e m a i n d e r are
s o f t w a r e failures. T h e longest u n i n t e r r u p t e d up time
was a b o u t two weeks. Service calls average one every
t h r e e weeks, b u t are h e a v i l y clustered. T o t a l up time
has been a b o u t 98 p e r c e n t o f o u r 24-hour, 365-day
schedule.
Acknowledgments. W e are grateful to R . H . C a n a d a y ,
L.L. Cherry, a n d L.E. M c M a h o n for their c o n t r i b u tions to uNIX. W e are p a r t i c u l a r l y a p p r e c i a t i v e o f the
inventiveness, t h o u g h t f u l criticism, a n d c o n s t a n t supp o r t o f R. M o r r i s , M . D . M c I l r o y , a n d J.F. O s s a n n a .

References
1. Digital Equipment Corporation. PDP-I1/40 Processor
Handbook, 1972, and PDP-I1/45 Processor Handbook, 1971.
2. Deutsch, L.P., and Lampson, B.W. An online editor. Comm.
ACM 10, 12 (Dec. 1967), 793-799, 803.
3. Richards, M. BCPL: A tool for compiler writing and system
programming. Proc. AFIPS 1969 SJCC, Vol. 34, AFIPS Press,
Montvale, N.J., pp. 557-566.
4. McClure, R.M. TMG--A syntax directed compiler. Proc.
ACM 20th Nat. Conf., ACM, 1965, New York, pp. 262-274.
5. Hall, A.D. The M6 macroprocessor. Computing Science Tech.
Rep.#2, Bell Telephone Laboratories, 1969.
6. Ritchie, D.M. C reference manual. Unpublished memorandum,
Bell Telephone Laboratories, 1973.
7. Aleph-null. Computer Recreations. So[?ware Practice and
Experience 1, 2 (Apr.-June 1971), 201-204.
8. Deutsch, L.P., and Lampson, B.W. SDS 930 time-sharing
system preliminary reference manual. Doc. 30.10.10, Project G ENI E,
U of California at Berkeley, Apr. 1965.
9. Feiertag, R.J., and Organick, E.I. The Multics input-output
system. Proc. Third Syrup. on Oper. Syst. Princ., Oct. 18-20, 1971,
ACM, New York, pp. 35-41.
10. Bobrow, D.G., Burchfiel, J.D., Murphy, D.L., and Tomlinson,
R.S. TENEX, a paged time sharing system tbr the PDP-10. Comm.
ACM15, 3 (Mar. 1972), 135-143.

9.5 Reliability
O u r statistics on reliability are m u c h m o r e subjective
t h a n the others. T h e f o l l o w i n g results are true to the
best o f o u r c o m b i n e d recollections. T h e t i m e s p a n is
over one y e a r with a very early vintage 11/45.
T h e r e has been one loss o f a file system (one d i s k
o u t o f five) caused b y s o f t w a r e i n a b i l i t y to c o p e with
a h a r d w a r e p r o b l e m c a u s i n g r e p e a t e d p o w e r fail traps.
Files on t h a t d i s k were b a c k e d up three days.
A " c r a s h " is an u n s c h e d u l e d system r e b o o t o r
halt. T h e r e is a b o u t one crash every o t h e r d a y ; a b o u t
t w o - t h i r d s o f t h e m are c a u s e d by h a r d w a r e - r e l a t e d dif-

375

Communications
of
the ACM

July 1974
Volume 17
Number 7

THE STRUCTURE OF THE "THE"-MULTIPROGRAMMING SYSTEM

EWDI 96

Edsger W.Dijkstra
Technological University
EINDHOVEN
The Netherlands

Summary
A multiprogramming system is described in
which all activities are divided over a number of
sequential processes. These sequential processes
are placed at various hierarchical levels, in each
of which one or more independent abstractions have
been implemented. The hierarchical structure proved
to be vital for the verification of the logical
soundness of the design and the correctness of its
implementation.
Introduction
Papers "reporting on timely research and
development efforts" being explicitly asked for, I
shall try to present a progress report on the multiprogramming effort at the Department of Mathematics
at the Technological University, Eindhoven, the
Netherlands.
Having very limited resources (viz. a group of
six people of, on the average, half time availability) and wishing to contribute to the art of system
design -including all the stages of conception,
construction and verification- we are faced with the
problem of how to get the necessary experience. To
solve this problem we have adopted the following
three guiding principles:
I) Select a project as advanced as you can
conceive, as ambitious as you can justify, in the
hope that routine work can be kept to a minimum;
hold out against all pressure to incorporate such
system expansions that would only result into a
purely quantitative increase of the total amount of
work to be done.
2) Select a machine with sound basic characteristics (e.g. an interrupt system to fall in love
with is certainly an inspiring feature); from then
onwards try to keep the specific properties of the
configuration for which you are preparing the system
out of your considerations as long as possible.
3) Be aware of the fact that experience does by
no means automatically lead to wisdom and understanding; in other words, make a conscious effort to
learn as much as possible from your precious
experiences.
Accordingly, I shall try to go beyond just
reporting what we have done and how, and I shall
try to formulate as well what we have learned.
I should like to end the introduction with two
short remarks on working conditions, remarks I make
for the sake of completeness. I shall not stress
these points any further.

The one remark is that production speed is
severely degraded if one works with half time people
who have other obligations as well. This is at least
a factor four, probably it is worse. The people
themselves lose time and energy in switching over,
the group as a whole loses decision speed as discussions~ when needed, have often to be postponed
until all people concerned are available.
The other remark is that the members of the
group (mostly mathematicians) have previously
enjoyed as good students a university training of
5 to 8 years and are of Master's or Ph.D. level.
I mention this explicitly because at least in my
country the intellectual level needed for system
design is in general grossly underestimated. I am
more than ever convinced that this type of work is
just difficult and that every effort to do it with
other than the best people is doomed to either
failure or moderate success at enormous expenses.
The Tool and the Goal
The system has been designed for a Dutch
machine, the EL X8 (N.V.Electrologica, Rijswijk
(ZH)). Characteristics of our configuration are:
I) core memory cycle time 2.5 mms., 27 bits; at
present 32K.
2) drum of 512K words, 1024 words per track, rev.
time 40 ms.
3) an indirect addressing mechanism very well
suited for stack implementation
4) a sound system for commanding peripherals and
controlling of interrupts
5) a potentially great number of low capacity
channels; ten of them are used (3 paper tape readers
at 1000 char/sac; 3 paper tape punches at 150 char/
sec; 2 teleprinters; a plotter; a line printer)
6) absence of a number of not unusual awkward
features.
The primary goal of the system is to process
smoothly a continuous flow of user programs as a
service to the University. A multiprogramming
system has been chosen with the following objectives
in mind:
I) a reduction of turn around time for programs of
short duration
2) economic use of peripheral devices
3)automatic control of backing store to be combined
with economic use of the central processor
4) the economic feasibility to use the machine for
those applications for which only the flexibility of

a general purpose computer is needed but (as a r u l e )
not the capacity nor the processing power.

The system is not intended as a multi-access
system. There is no common data base via which
independent users can communicate with each other:
they only share the configuration and a procedure
library (that includes a translator for ALGOL 60
extended with complex numbers). The system does not
cater for user programs written in machine language.
Compared with larger efforts one can state that
quantitatively speaking the goals have been set as
modest as the equipment and our other resources.
Qualitatively speaking, I am afraid, we got more and
more immodest as the work progressed.

minutes (classical) inspection at the machine and
each of them correspondingly easy to remedy. At the
moment of writing the testing is not yet completed,
but the resulting system will be guaranteed to be
flawless. When the system has been delivered we shall
not live in the perpetual fear that a system derailment may still occur in an unlikely situation such as
might result from an unhappy "coincidence" of two or
more critical occurrences, for we shall have proved
the correctness of the system with a rigour and
explicitness that is unusual for the great majority
of mathematical proofs.
A Survey of the System Structure

A Progress Report
Storage Allocation.
We have made some minor mistakes of the usual
type (such as paying too much attention to speeding
up what was not the real bottle neck) and two major
ones.
Our f i r s t
m a j o r m i s t a k e has been t h a t f o r t o o
long a time we confined our attention to "a perfect
installation": by the time we considered how to make
the best of it when, say, one of the peripherals
broke down, we were faced with nasty problems.
Taking care of the "pathology" took more energy than
we had expected and part of our troubles were a
direct consequence of our earlier ingenuity, i.e.
the complexity of the situation into which the system
could have manoeuvred itself. Had we paid attention
to the pathology at an earlier stage of the design,
our management rules would certainly have been less
refined.

The second major mistake has been that we
conceived and programmed the major part of the system
without giving more than scanty thought to the problem of debugging it. For the fact that this mistake
had no serious consequences -on the contrary~ one
might argue as an afterthought- I must decline all
credit. I feel more like having passed through the
eye of the needle...
As captain of the crew I had had extensive
experience (dating back to 1958) in making basic
software dealing with real time interrupts and I
knew by bitter experience that as a result of the
irreproducibility of the interrupt moments, a program
error could present itself misleadingly like an
occasional machine malfunctioning. As a result I was
terribly afraid. Having fears regarding the possibility of debugging we decided to be as careful as
possible and -prevention is better than cure~- to
try to prevent nasty bugs from entering the construction.
This decision, inspired by fear, is at the
bottom of what I regard as the group's main contribution to the art of system design. We have found
that it is possible to design a refined multiprogramming system in such a way that its logical soundness
can be proved a priori and that its implementation
admits exhaustive testing. The only errors that
showed up during testing were trivial coding errors
(occurring with a density of one error per 500
instructions), each of them located within 10

In the classical yon Neumann machine information
is identified by the address of the memory location
containing the information. When we started to think
about the automatic control of secondary storage
we were familiar with a system (viz. GIER ALGOL)
in which all information was identified by its
drum address (as in the classical yon Neumann
machine) and in which the function of the core
memory was nothing more than to make the information
"page wise" accessible.
We have followed another approach and as it
turned out, to great advantage. In our terminology
we made a strict distinction between memory units
(we called them "pages" and had "core pages" and
"drum pages") and corresponding information units
(for lack of a better word we called them "segments")
a segment just fitting in a page. For segments we
created a completely independent identification
mechanism in which the number of possible segment
identifiers is much larger than the total number of
pages in primary and secondary store. The segment
identifier gives fast access to a so-called "segment
variable" in core whose value denotes whether the
segment is still empty or not and if not empty, in
which page (or pages) it can be found.
As a consequence of this approach: if a segment
of information, residing in a core page, has to be
dumped onto the drum in order to make the core page
available for other use, there is no need to return
the segment to the same drum page as it originally
came from. In fact, this freedom is exploited: among
the free drum pages the one with minimum latency
time is selected.
A next consequence is the total absence of a
drum allocation problem: there is not the slightest
reason why, say, a program should occupy consecutive
drum pages. In a multiprogramming environment this
is very convenient.
Processor Allocation.
We have given full recognition to the fact that
in a single sequential process (such as performed by
a sequential automaton) only the time succession of
the various states has a logical meaning, but not
the actual speed with which the sequential process
is performed. Therefore we have arranged the whole

system as a society of sequential processes, progressing with undefined speed ratios. To each user
program, accepted by the system, corresponds a
sequential process, to each input peripheral
corresponds a sequential process (buffering input
streams in synchronism with the execution of the
input commands), to each output peripheral corresponds a sequential process (unbuffering output
streams in synchronism with the execution of the
output commands); furthermore we have the "segment
controller" associated with the drum and the
"message interpreter" associated with the console
keyboard.
This enabled us to design the whole system in
terms of these abstract "sequential processes".
Their harmonious co-operation is regulated by means
of explicit mutual synchronization statements. On
the one hand, this explicit mutual synchronization
is necessary, as we do not make any assumption about
speed ratios, on the other hand this mutual synchronization is possible because "delaying the progress
of a process temporarily" can never be harmful to
the interior logic of the process delayed. The
fundamental consequence of this approach -viz. the
explicit mutual synchronization- is that the
harmonious co-operation of a set of such sequential
processes can be established by discrete reasoning;
as a further consequence the whole harmonious
society of co-operating sequential processes is
independent of the actual number of processors
available to carry out these processes, provided
the processors available can switch from process
to process.
System Hierarchy.
The total system admits a strict hierarchical
structure.
At level 0 we find the responsibility for
processor allocation to one of the processes whose
dynamic progress is logically permissible (i.e. in
view of the explicit mutual synchronization). At
this level the interrupt of the real time clock is
processed, introduced to prevent any process to
monopolize processing power. At this level a priority
rule is incorporated to achieve quick response of the
system where this is needed. Our first abstraction
has been achieved, above level 0 the number of
processors actually shared is no longer relevant. At
the higher levels we find the activity of the
different sequential processes, the actual processor
having lost its identity, having disappeared from
the picture.
At level I we have the so-called "segment
controller", a sequential process synchronized with
respect to the drum interrupt and the sequential
processes on higher levels. At level I we find the
responsibility to cater for the bookkeeping resulting
from the automatic backing store. At this level our
next abstraction has been achieved: at all higher
levels identification of information takes place in
terms of segments, the actual storage pages having
lost their identity, having disappeared from the

picture.
At level 2 we find the "message interpreter",
taking care of the allocation of the console keyboard
via which conversations between te operator and any
of the higher level processes can be carried out. The
message interpreter works in close synchronism with
the operator: when the operator presses a key, a
character is sent to the machine together with an
interrupt signal to announce this next keyboard
character, while the actual printing is then done
on account of an output command generated by the
machine under control of the message interpreter.
(As far as the hardware is concerned the console
teleprinter is regarded as two independent peripherals: an input keyboard and an output printer.) If
one of the processes opens a conversation it identifies itself for the benefit of the operator in the
opening sentence of this conversation. If, however,
the operator opens a conversation he must identify
the process he is addressing, in the opening sentence
of the conversation, i.e. this opening sentence must
be interpreted before it is known to which of the
processes the conversation is addressed~ There lies
the logical reason to introduce a separate sequential
process for the console teleprinter, a reason that
is reflected in its name "message interpreter". Above
level 2 it is as if each process had its private
conversational console. The fact that they share the
same physical console is translated into a resource
restriction of the form "only one conversation at a
time", a restriction that is satisfied via mutual
synchronization. At this level the next abstraction
has been implemented: at the higher levels the actual
console teleprinter has lost its identity. (If the
message interpreter had not been on a higher level
than the segment controller, then the only way to
implement it would have been to make a permanent
reservation in core for it; as the conversational
vocabulary might get large (as soon as our operators
wish to be addressed in fancy messages) this would
result in too heavy a permanent demand upon core
storage. Therefore the vocabulary in which the
messages are expressed is stored on segments, i.e.
as information units that can reside on the drum as
well. For this reason the message interpreter is of
a level one higher than the segment controller.)
At level 3 we find the sequential processes
associated with buffering of input streams and
unbuffering of output streams. At this level the next
abstraction is effected, viz. the abstraction of the
actual peripherals used, that are allocated at this
level to the "logical communication units" in terms
of which is worked in the still higher levels. The
sequential processes associated with the peripherals
are of a level above the message interpreter, because
they must be able to converse with the operator (e.g.
in the case of detected malfunctioning). The limited
number of peripherals again acts as a resource
restriction for the processes at higher levels, to be
satisfied by mutual synchronization between them.
At level 4 we find the independent user programs,
at level 5 the operator (not implemented by us).

The system structure has been described at
length in order to make the next section intelligible,
Design Experience
The conception stage took a long time. During
that period of time the concepts have been born in
terms of which we sketched the system in the previous
section. Furthermore we learnt the art of reasoning
by which we could deduce from our requirements the
way in which the processes should influence each
other as regards mutual synchronization so that
these requirements would be met. (The requirements
being that no information can be used before it has
been produced, that no peripheral can be set to two
tasks simultaneously, etc.) Finally we learnt the
art of reasoning by which we could prove that the
society composed of processes thus mutually synchronized by each other, would indeed in its time
behaviour satisfy all requirements.
The construction stage has been rather traditional, perhaps even old-fashioned: plain machine
code. Reprogramming on account of a change of
specifications has been rare, a circumstance that
must have contributed greatly to the feasibility of
the "steam method". The fact that the first two
stages took more time than planned was somewhat
compensated by a delay in the delivery of the machine.
In the verification stage we had, during short
shots, the machine completely at our disposal, shots
during which we worked with a virgin machine without
any software aids for debugging. Starting at level 0
the system has been tested, each time adding (a
portion of) the next level only after the previous
level had been thoroughly tested. Each test shot
itself contained on top of the (partial) system to
be tested a number of testing processes with a double
function. Firstly they had to force the system into
all different relevant states, secondly they had to
verify that the system continued to react according
to specification.
I shall not deny that the construction of these
testing programmes has been a major intellectual
effort: to convince oneself that one has not overlooked "e relevant state" and to convince oneself
that the testing programmes generate them all is no
simple matter. The encourageing thing is that (as
far as we are aware~) it could be done.
This fact was one of the happy consequences of
the hierarchical structure.
Testing level 0 (the real time clock and processor allocation) implied a number of testing
sequential processes on top of it, inspecting together that under all circumstances processor time
was divided among them according to the rules. This
being established, sequential processes as such had
been implemented.
Testing the segment controller at level I
meant that all "relevant states" could be formulated
in terms of sequential processes making (in various
combinations) demands on core pages, situations that

could be provoked by explicit synchronizing among
the testing programs. At that stage the existence
of the real time clock -although interrupting all
the time- was so immaterial that one of the testers
indeed forgot its existence~
By that time we had implemented the correct
reaction upon the (mutually unsynchronized) interrupts from the real time clock and the drum. If we
had not introduced the separate levels 0 and I and
if we had not created a terminology (viz. that of
the rather abstract sequential processes) in which
the existence of the clock interrupt could be discarded, but had tried instead to make in a nonhierarchical construction the central processor
directly react upon any weird time succession of
these two interrupts, the number of "relevant states"
would have exploded to such a height that exhaustive
testing would have been an illusion. (Apart from that
it is doubtful wether we would have had the means to
generate them all, drum and clock speed being outside
our control.)
For the sake of completeness I must mention
a further happy consequence. As stated before, above
level I core and drum pages have lost their identity
and buffering of input and output streams (at level
3) therefore occurs in terms of segments. While
testing at level 2 or 3 the drum channel hardware
broke down for quite some time, but testing could
proceed by restricting the number of segments so
that they all could be held in core. If building
up the line printer output streams had been implemented as "dumping onto the drum" and the actual printing as "printing from the drum" this advantage would
have been denied to us.
Conclusion
As far as program verification is concerned I
present nothing essentially new. In testing a
general purpose object (be it a piece of hardware,
a program, a machine or a system) one cannot subject
it to all possible cases: for a computer this would
imply that one feeds it with ell possible programs!
Therefore one must test it with a set of relevant
test cases. What is relevant or not, cannot be
decided as long as one regards the mechanism as a
black box, in other words it has to follow from the
internal structure of the mechanism to be tested. It
seems the designer's responsibility to construct his
mechanism in such a way -i.e. so effectively structured- that at each stage of the testing procedure
the number of relevant test cases is so small that
he can try them all and that what is being tested is
so perspicuous that it is clear that he has not
overlooked a situation. I have presented a survey of
our system because I think it a nice example of the
form that such a structure might take.
In my experience, I am sorry to say, industrial
software makers tend to react to it with mixed
feelings. On the one hand they are inclined to judge
that we have done a kind of model job, on the other
hand they express doubts whether the techniques used
are applicable outside the sheltered atmosphere of a

University Department and express the opinion that
we could only do it this way thanks to the modest
scope of the whole project. It is not my intention
to underestimate the organizing ability needed for
a much bigger job with ten or more times as many
people, but I should like to venture the opinion
that the larger the project, the more essential the
structuring~ A hierarchy of five logical levels
might then very well turn out to be of modest depth,
in particular when one designs the system more
consciously than we have done with the aim that the
software can be smoothly adapted to (perhaps drastic)
configuration expansions.
Acknowledqements
I should not like to publish this progress
report without expressing my great indebtedness to
my five collaborators C.Bron, A.N.Habermann, F.J.A.
Hendriks, C.Ligtmans and P.A.Voorhoeve. They have
contributed to ell the stages of the design, together
we learnt the art of reasoning needed. Construction
and verification is entirely their effort: if my
dreams have become true, this is due to their faith,
their talents and their persistent loyalty to the
whole project.
Finally I should like to thank the members of
the program committee who asked for more information
on the synchronizing primitives and some justification of my claim to be able to prove logical soundness a priori. In answer to this request the appendix
has been added, of which I hope that it gives the
desired information and justification.

"V(sem)" increases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is positive, the V-operation in question
has no further effect; if, however, the resulting
value of the semaphore concerned is non-positive,
one of the processes booked on its waiting list is
removed from this waiting list, i.e. its dynamic
progress is again logically permissible and in due
time a processor will be allocated to it (again, see
above "System Hierarchy", at level 0).
Corollary I:
If a semaphore value is nonpositive its absolute value equals the number of
processes booked on its waiting list.
Corollary 2:
The P-operation represents the
potential delay, the complementary V-operation
represents the removal of a barrier.
Note I:
P- and V-operations are "indivisible
actions", i.e. if they occur "simultaneously" in
parallel processes, they are non-interfering in the
sense that they can be regarded as being performed
the one after the other.
Note 2:
If the semaphore value resulting
from a V-operation is negative, its waiting list
did originally contain more than one process. It is
undefined -i.e. logically immaterial- which of the
waiting processes is then removed from the waiting
list.
Note 3:
A consequence of the mechanisms
described above is that a process whose dynamic
progress is permissible can only loose this status
by actually progressing, i.e. by performance of a
P-operation on a semaphore with a value that is
initially non-positive.

Appendix
The S~nchronizinq Primitives.
Explicit mutual synchronization of parallel
sequential processes is implemented via so-called
"semaphores". They are special purpose integer
variables allocated in the universe in which the
processes are embedded, they are initialized (with
the value 0 or I) before the parallel processes
themselves are started. After this initialization
the parallel processes will access the semaphores
only via two very specific operations, the so-called
synchronizing primitives. For historical reasons
they are called the P-operation and the V-operation.
A process, "Q" say, that performs the operation
"P(sem)" decreases the value of the semaphore called
"sem" by I. If the resulting value of the semaphore
concerned is non-negative, process Q can continue
with the execution of its next statement; if,
however, the resulting value is negative, process
Q is stopped and booked on a waiting list associated
with the semaphore concerned. Until further notice
(i.e. a V-operation on this very same semaphore)
dynamic progress of process Q is not logically
permissible and no processor will be allocated to
it (see above "System Hierarchy", at level 0).
A process, "R" say, that performs the operation

During system conception it transpired that we
used the semaphores in two completely different
ways. The difference is so marked that, looking
back, one wonders whether it was really fair to
present the two ways as a usage of the very same
primitives. On the one hand we have the semaphores
used for mutual exclusion, on the other hand the
private semaphores.
The Mutual Exclusion.
In the following program we indicate two
parallel, cyclic processes (between the brackets
"parbeRin" and '~arend") that come into action
after the surrounding universe has been introduced
and initialized.
begin semaphore mutex; mutex := I;
parbegin
beqin L I : P(mutex); critical section I; V(mutsx);
remainder of cycle I; ~oto LI
end;
beqin L2: P(mutex); critical section 2; V(mutex);
remainder of cycle 2; qoto L2
end
parend
end
As a result of the P- and V-operations on

"mutex" the actions, marked as "critical sections"
exclude each other mutually in time; the scheme
given allows straightforward extension to more than
two parallel processes, the maximum value of mutex
= I, the minimum value = - (n - I) if we have n
parallel processes.
Critical sections are used always and only for
the purpose of unambiguous inspection and modification of the state variables (allocated in the
surrounding universe) that describe the current
state of the system (as far as needed for the
regulation of the harmonious co-operation between
the various processes).
The Private Semaphores.
Each sequential process has associated with it
a number of private semaphores and no other process
will ever perform a P-operation on them. The universe
initializes them with the value = O, their maximum
value = I, their minimum value = - I.
Whenever a process reaches a stage where the
permission for dynamic progress depends on current
values of state variables, it follows the pattern:

P(mutex);
"inspection and modification of state variables
including a conditional V(private semaphore)";

V(mutex);
P(private semaphore)
If the inspection learns that the process in
question should continue, it performs the operation
"V(private semaphore)" -the semaphore value then
changes from 0 to I-, otherwise this V-operation
is skipped, leaving to the other processes the
obligation to perform this V-operation at a suitable
moment. The absence or presence of this obligation
is reflected in the final values of the state
variables upon leaving the critical section.
Whenever a process reaches a stage where as a
result of its progress possibly one (or more)
blocked processes should now get permission to
continue, it follows the pattern

P(mutex);
"modification and inspection of state variables
including zero or more V-operations on private
semaphores of other processes";

V(mutex)
By the introduction of suitable state variables
and appropriate programming of the critical sections
any strategy assigning peripherals, buffer areas etc.
can be implemented.
The amount of coding and reasoning can be
greatly reduced by the observation that in the two
complementary critical sections sketched above, the
same inspection can be performed by the introduction
of the notion of "an unstable situation", such as
a free reader and a process needing a reader.
Whenever an unstable situation emerges it is
removed (including ome or more V-operations on

private semaphores) in the very same critical
section in which it has been created.
Provinq the Harmonious Co-operation.
The sequential processes in the system can all
be regarded as cyclic processes in which a certain
neutral point can be marked, the so-called "homing
position", in which all processes are when the
system is at rest.
When a cyclic process leaves its homing position
"it accepts a task", when the task has been performed
end not earlier, the process returns to its homing
position. Each cyclic process has a specific task
processing power (e.g. the execution of a user
program or unbuffering a portion of printer output,
etc.)
The harmonious co-operation is mainly proved
in roughly three stages.
I)
It is proved that although a process
performing a task may generate in doing so a finite
number of tasks for other processes, a single
initial task cannot give rise to an infinite number
of task generations. The proof is simple as
processes can only generate tasks for processes
at lower levels of the hierarchy so that circularity
is excluded. (If a process needing a segment from
the drum has generated a task for the segment
controller, special precautions have been taken to
ensure that the segment asked for remains in core
at least until the requesting process has effectively
accessed the segment concerned. Without this precaution finite tasks could be forced to generate an
infinite number of tasks for the segment controller
and the system could get stuck in an unproductive
page flutter.)
2)
It is proved that it is impossible
that all processes have returned to their homing
position while somewhere in the system is still
pending a generated but unaccepted task. (This is
proved via instability of the situation just
described.)
3)
It is proved that after the acceptance
of an initial task all processes eventually will be
(again) in their homing position. Each process
blocked in the course of task execution relies on the
other processes for removal of the barrier. Essentially, the proof in question is a demonstratmon of the
absence of "circular waits": process P waiting for
process Q waiting for process R waiting for process
P. (Our usual term for the circular wait is "the
Deadly Embrace".) In a more general society than
our system this proof turned out to be e proof by
induction (on the level of hierarchy, starting at
the lowest level) as A.N.Habermann has shown in his
doctoral thesis.

THE WORKING SET MODEL FOR PROGRAM BEHAVIOR
Peter J. Denning
Massachusetts Institute of Technology
Cambridge, Massachusetts

SUMMARY

We claim neither is adequate.

Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate
model for program behavior.
In this paper a new
model is developed, the "working set model", which
enables us to decide which information is in use
by a running program and which is not.
Such knowledge is vital for dynamic management of paged
memories.
The working set of pages associated
with a process, defined to be the collection of its
most recently used pages, is a useful allocation
concept. A proposal for an easy-to-implement
allocation policy is set forth; this policy is
unique, inasmuch as it blends into one decision
function the heretofore independent activities of
process-scheduling and memory-management.

Because resources are multiplexed, each user
is given the illusion that he has a complete computing system at his sole disposal: a virtual
computer.
For our purposes, the basic elements of
a virtual computer are its virtual processor and
an "infinite" one-level virtual memory.
Dynamic
"advice" regarding resource requirements cannot be
obtained successfully from users for several
reasons:
i. A user may build his program on the work
of others, frequently sharing procedures
whose time and storage requirements may be
either unknown or, because of data dependence, indeterminate.
Therefore he cannot
be expected to estimate processor-memory
needs.
2. It is not clear what sort of "advice" might
be solicited.
Nor is it clear how the
operating system should use it, for overhead incurred by using advice could well
negate any advantages attained.
3. Any advice acquired from a user would be
intended (by him) to optimize the environment for his own program.
Configuring
resources to suit individuals may interfere
with overall good service to the community
of users.
Thus it seems inadvisable at the present time to
permit users, at their discretion, to advise the
operating system of their needs.

INTRODUCTION
Resource allocation is tricky business.
In
recent years there has been much dialogue on the
topics of process scheduling and core memory management, yet development of techniques has progressed independently along both these lines. No
one will deny that a unified approach is needed.
Probably the most basic reason behind the absence
of a general treatment is the lack of an adequate
model for program behavior.
In this paper we develop a new model, the working set model, which
embodies certain important behavioral properties
of computations operating in a multiprogrammed environment, enabling us to decide which information
is in use by a running program and which is not.
We do not intend that the proposed model be considered "final"; rather, we hope to stimulate a
new kind of thinking, thinking that may be of considerable help in solving many operating system
design problems.
The working set is intended to model the behavior of programs in the general purpose computer
system, or computer utility.
For this reason we
assume that the operating system must on its own
determine the behavior of programs it runs; it
cannot count on outside help.
Two commonly proposed sources of externally-supplied dynamic allocation information are the user and the compiler.

Work reported herein was supported in part by
Project MAC, an M.I.T. research project sponsored
by the Advanced Projects Research Agency, Dept.
of Defense, under Office of Naval Research Contract Number Nonr-4102(Ol).

Likewise, compilers cannot be expected to
supply information, extracted from the structure
of the program , regarding resource requirements:
i. Programs will be modular in construction;
information about other modules may be unavailable at compilation time.
Because of
dependence on data there may be no way to
decide (until run time) just which modules
will be included in a computation.
2. Compilers cluttered with extra machinery
to predict memory needs will be slower in
operation.
Many users are less interested
in whether their programs operate efficiently than whether they operate at all, and
so are concerned with rapid compilation.
Furthermore, the compiler is an often-used
component of the operating system; if slow
and bulky, it can be a serious drain on
system resources.

**Ramamoorthy I has put forth a proposal for automatic segmentation of programs during compilation.

Therefore in this paper we are advocating mechanisms that monitor the behavior of a computation,
making allocation decisions on the basis of currently observed characteristics.
Only a mechanism
that oversees the behavior of a program in operation can cope with arbitrary interconnections of
arbitrary modules having arbitrary characteristics.
Our treatment proceeds as follows. First we
define the type of computer system in which our
ideas are developed. After a brief discussion of
previous work with the problems of dynamic memory
management, we define the working set model. We
discuss a method of implementing memory management
based on this model, and indicate how working set
notions can be used to blend process scheduling
and memory management into one decision function,
accounting simultaneously for both types of demand.
Finally we discuss how data sharing fits into the
working-set scheme.

THE FRAMEWORK
We assume that the reader is already familiar
with the concepts of a computer utility 2'3'4, of
segmentation and paging 5'6, of program and addressing structure 6'8, so we will only mention these
topics here. Briefly, each process has access to
its own private, segmented name space; each segment known to the process is sliced into equalsize units, called pages, to facilitate mapping it
into the paged main memory.
Associated with each
segment is a page table, whose entries point to

the segment's pages. An "in-core" bit in each
page table entry is turned ON whenever the designated page is present in main memory ; an attempt
to reference a page whose "in-core" bit is OFF
causes a page fault, initiating proceedings to
secure the missing page. Finally, a process has
three states of existence: running, when a processor is assigned to it; ready, when it would be
running if only a processor were available; or
blocked, when it has no need of a processor (for
example, during a page fault or during a console
interaction). When talking about processes in execution, we will have to distinguish between "process time" and "real time". Process time is time
as seen by a process unaware it is suspended; that
is, as if it executed without interruptions.
We restrict attention to a two-level memory
system, indicated by Figure I. Only data residing
in main memory is accessible to a processor; all
other data reside in auxiliary memory, which we
regard as having i nflnite capacity. There is a
time T, the traverse time, involved in transferring
a page between memories. T is measured from the
moment a page fault occurs until the moment the
missing page is in main memory ready for use. T
is actually the expectation of a random variable
composed of waits in queues and mechanical positioning delays.
Though it usually takes less time to
store into auxiliary memory than to read from it,
we shall regard the traverse time T to be the same
regardless of which direction a page is moved.
,
Consistent with current usage, we will use the
terms "core memory" and "main memory" interchangeably.

PROCESSORS

~

~ ~

~ n

data flow from main memory
(controlled b y ~, c o r emanager)
I

main memÂ°rY(contrD~edfl~ ~ ~ ~ n m ~ [ c Y i e s )

FIGURE i.

Two-level memory system.

A basic allocation problem, "core memory management", is that of deciding just which pages are to
occupy main memory.
The basic strategy advocated
here -- a compromise against a lot of expensive
,
main memory -- is to minimize page traffic . There
are two reasons for this:
i. The more the data traffic between the two
levels of memory, the more the computational overhead in deciding just what to move
and where to move it.
2. Because the traverse time T is long compared
to a memory cycle, too much data movement
can result in congestion and serious interference with processor efficiency.
Roughly speaking, a working set of pages is
the minimum collection of pages that must be loaded
in main memory for a process to operate efficiently,
without "unnecessary" page faults. According to
our definitions, a "process" and its "working set"
are but two manifestations of the same ongoing
computational activity.

PREVIOUS WORK
In this section we outline strategies that have
been set forth in the past for memory management;
the interested reader will be referred to the literature for detail.
We regard management of paged memories to operate in two stages:
I. Pagin_g in." locate the required page in
auxiliary memory, load it into main memory,
turn the "in-core" bit of the appropriate
page table entry ON.
2. Paging out: remove some page from main memory, turn the "in-core" bit of the appropriate page table entry OFF.
Management algorithms can be classified according
to their methods of paging in and paging out.
It
is a common characteristic of nearly every strategy
that paging in is done on demand; that is, no action
is taken to load a page into memory until some
process attempts to reference it. To date there
have been no proposals recommending look-ahead, or
anticipatory page-loading, because (as we have
stressed) there is no reliable advance source of
allocation information, be it the programmer or
the compiler.
Although the working set is the
desired information, it might still be futile to
pre-load pages: there is no guarantee that a process will not block shortly after resumption,
having referenced only a fraction of its working
set.
The operating system could devote its already precious time to activities more rewarding
than loading pages which may not be used.
Thus we
will assume that paging in is done on demand only,
via the page fault mechansim.

Since data is stored and transmitted in units of
pages, we can (without ambiguity) refer to data
movement as "page traffic".

The chief problem in memory management is not
deciding which pages to load; it is deciding which
pages ought to be removed.
For if the page with
the least likelihood of being used in the immediate
future is retired to auxiliary memory, the best
choice has been made.
Nearly every worker in the
field has recognized this.
Debate has arisen over
which strategy to employ for retiring pages; that
is, which page-turning, or replacement, algorithm
to use.
A good measure of performance for a paging
policy is page traffic (the number of pages per
unit time being moved between memories), since
erroneously removed pages add to the traffic of
returning pages.
In the following we will use this
as a basis of comparison for several strategies.
Random selection.
Whenever a fresh page of memory
is needed, a page is selected at random to be replaced.
Although utterly simple to implement, this
method frequently removes useful pages (which must
therefore be recalled) and so results in high page
traffic.
~!%~
selection. The pages of main memory are ordered in a cyclic list.
Suppose the M pages of
main memory are numbered 0,1,...,(M-I) and a
pointer k indicates that the k-th page was most
recently paged in. Whenever a fresh page of memory
is needed, [(k+l) mod M] 4 k, page k is retired,
and another page brought in to fill the now vacant
slot.
This method -- also utterly simple to realize -- is based on the principle that programs tend
to follow sequences of instructions, so that references in the immediate future will most likely
be close to present references.
So, assuming there
is this tendency for page references to cluster,
and assuming some kind of uniformity in process
scheduling techniques, the page which has been in
memory longest is least likely to be reused: hence
the cyclic list. We see two ways in which this
algorithm can fail. First we question its basic
assumption.
It is not at all clear that modular
programs, which execute numerous inter-module calls,
will indeed exhibit sequential instruction fetch
patterns.
The thread of control will not string
pages together; rather, it will entwine them intricately.
[Fine, Mclssac, and Jackson 9 have some
experimental evidence in support of this reasoning.]
Second, this algorithm is subject to overloading when used in multiprogrammed memories.
When core demand is too heavy, one cycle of the
list completes rapidly and the pages deleted are
still needed by their processes.
This can create
a self-intensifying crisis.
Programs, deprived of
still-needed pages, generate a plethora of page
faults; the resulting traffic of returning pages
displaces still other useful pages, leading to more
page faults, and so on.
Oldest-unused selection.
Each page table entry
contains a ~us~ ~ bit, set ON each time the page is
referenced.
At periodic intervals all the page
table entries are searched and usage records updated.
When a fresh page of memory is needed, the page unreferenced for the longest time is removed.
One can

see that this method is intrinsically reasonable by
considering the simple case of a computer where
there is exactly one process whose pages cannot all
fit into main memory.
In this case the most reasonable choice for a page to replace is the oldest
unused page.
Unfortunately this method too is susceptible to overloading when many processes compete
for main memory.
ATLAS ioo~ detection method.

The Ferranti ATLAS

computer I0 had proposed a page-turning policy that
attempted to detect loop behavior in page reference
patterns, then minimize page traffic by maximizing
the time between page transfers, that is, by removing pages not expected to be needed for the longest time.
It was successful -- only for looping
programs.
Performance was unimpressive for programs exhibiting random reference patterns.
Implementation was costly.
Various studies concerning behavior of paging
algorithms have appeared.
Fine, Mclssac and
Jackson 9 have investigated the effects of demand
paging and have questioned whether paging is beneficial at all. We do not feel that their conclusion applies to the kind of multiprogrammed environment we have described.
They studied fixed-size
programs, that quickly acquired and retained a
large fraction of their pages.
Highly interactive,
modular programs are likely to behave differently.
Not only may program size vary dynamically (according to data dependencies), but also such programs
should be using a small fraction of their pages at
any one time, and the membership in this set of
working pages should be changing constantly.

We define the working set of information W(t,T)
of a process at time t to be the collection of data
items referenced by the process during the proces
time interval (t-T~,t).
Thus, the data items a process has referenced
during the last T seconds of its execution comprise
its working set. q will be called the working set
parameter.
We will regard the data items in W(t,T)
as being pages ,although they could just as well
be any other named data objects.
The working set
size ~(t,~) is
(I)

~(t,T)

=

Number of pages in W(t,T)

A working set W(t,~) has two important, general
properties.
Both are properties of typical programs,
and need not hold in special cases.
PI. Size. It should be clear immediately that
~(t,0) = 0 since no page reference can occur
in zero time.
It should also be clear that
~(t,T) as a function of T is monotonically
increasing, since more pages can be referenced
in longer time intervals.
Because a process
will refer to its most-needed pages frequently
and its least-needed pages infrequently, we
expect ~(t,~) as a function of T to have a
steep initial rise which diminishes to a more
gradual rise.
The general character of ~(t,~)
is suggested by the smoothed curve of Figure 2.
00(t ,~-)

Belady II has compared some of the algorithms
mathematically.
His most important conclusion is
that the "ideal" algorithm should possess much of
the simplicity of Random or Cyclic selection (for
efficiency) and some, though not much, accumulation
of data on past reference patterns.
He has shown
that too much "historical" data can have adverse
effects (witness ATLAS).
In the next section we begin investigation of
the working set concept.
Even though the ideas
are not entirely new 12'13'14, there has been no
detailed documentation publicly available.

THE WORKING SET MODEL
From the programmer's standpoint, the working
set of information is the smallest collection of
procedure and data items that must be present in
main memory to assure efficient execution of his
program.
We have already stressed that there will
be no advance notice from either the programmer or
the compiler regarding what information "ought" to
be in main memory.
It is up to the operating
system to determine on the basis of page reference
patterns whether pages are in use.
Therefore the
working set of information associated with a process is, from the system standpoint, the set of
most recently referenced pages.

FIGURE 2.

Behavior of ~(t,T).

Program modularity enables us to
P2. Correlation.
say something about correlation between the
size of a working set at two times, t and (t+c~).
Correlation is useful in devising storage
allocators, for the higher the correlation
between ~(t,T) and w(t+c~,T), the better is
~(t,T) a prediction of ~(t+c~,T).
In modular
programs, control passes randomly from one
module to another; if T is chosen properly
(as discussed in the next section), it is more
likely that a working set will change size
smoothly, less likely that it will change size
abruptly.
Thus for small time separations
(say, 6 < < T), ~(t,~) and ~(t+c~,T) are highly

correlated, meaning that a measurement of ~(t,T)
will serve as a good estimate of the memory requirement during the process time interval (t,t+c0.
For large time separations ~ (say, ~ >> .[), control
will have passed through a great many modules
during the interval (t,t+c~); thus ~(t,T ) gives
little information about ~(t+C~,7), and so ~(t,7)
and ~(t+c~,7) have much less correlation than for
small 5. This behavior is suggested in Figure 3.
Correlation between
~(t,7) and ~(t+Cz,7)

Detecting W(t~T )
According to our definition, W(t,T) is the set
of its pages a process has referenced within the
last T seconds of its execution.
This suggests
that memory management can be controlled by hardware mechanisms, by associating with each page of
main memory a timer.
Each time a page is referenced, its timer is set to T and begins to run
down; if the timer succeeds in running down, a flag
is set to mark the page for removal whenever the
space is needed.
In the appendix we describe such
a hardware memory management mechanism, hardware
that can be housed within the memory boxes.
The
mechanism has two interesting features:
i. It operates asynchronously and independently
of the supervisor, whose only respsonsibility
in memory management is handling page faults.
Quite literally, memory manages itself.
2. Analog devices such as capacitative timers
could be used to measure intervals.

~D

c~
0
FIGURE 3.

Correlation between working set sizes,

Choice of T

Unfortunately it is not practical to add on
hardware to existing systems. We seek a method of
handling memory management within the software.
The procedure we propose here samples the page
table entries of pages in core memory at process
time intervals of ~ seconds (~ is called the
sampling interval) where ~ = y/K , K an integer
constant chosen to make the sampling intervals as
"fine grain" as desired.
On the basis of page
references during each of the last K sampling intervals, the working set W(t,l~) can be determined,
as follows.

The value ultimately selected for ~ will reflect
efficiency requirements and will be influenced by
"in- core"
system parameters such as core memory size and memI
ory traverse time.
For example, if T is too small,
pages may be removed from main memory while they
are still useful, and high page traffic may result
from returning pages.
If T is too large, pages
may remain in main memory long after last being
used, and wasted main memory may result.
Thus the
value of T will have to represent a compromise between too much page traffic and too much wasted
memory space.
0

l

pointer
to page

use bits

luol-,l

---

TYPICAL PAGE TABLE ENTRY

SHIFT AT END OF SAMPLING INTERVAL
The following consideration leads us to recommend for T a value comparable to the memory traverse time T (Figure I). Consider a process that
is running continuously, being interrupted only
for page faults.
Assuming that memory allocation
procedures balk at removing from main memory any
page in a working set, once a page has entered
W(t,T) it will remain in main memory for at least
T seconds.
Under the very worst of page-shuffling
conditions, a page could be dispatched to auxiliary memory and be recalled immediately; the time
for this round trip is two traverse times, 2T.
Therefore a highly-shuffled page would spend
roughly T/2T of its time in main memory.
So,
for example, if we wished to insure that a page is
available in main memory (when needed) for not less
than 50 per cent of the time, we would have to
choose â¢ ~ 2T.

FIGURE 4.

Page table entries for detecting W(t,Kcy).

As indicated by Figure 4, each page table entry
contains an "in-core" bit M, where M=i if and only
if the page is present in main memory.
It also
contains a string of use bits u0,ul,...,u K. Each
time a page reference occurs,

I ~ u 0.

At the end

of each sampling interval ~, the bit pattern contained in u0,ul,...,u K is shifted one position,
a 0 enters u 0, and u K ipapers.txt -> [(0 -> (0, [0x6322a15a747c]))]]
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a7480, 0x6322a15a7484, 0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a7488, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a748c, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
core: 
[2 -> [0x6322a15a7474, 0x6322a15a7480, 0x6322a15a7484]]
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a7488, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
swfile: 
[11 -> [0x6322a15a9bb8]]
[10 -> [0x6322a15a9bb4]]
[3 -> [0x6322a15a7470]]
[4 -> [0x6322a15a7474]]
[5 -> [0x6322a15a7478]]
[6 -> [0x6322a15a84e0]]
[7 -> [0x6322a15a84e4]]
[8 -> [0x6322a15a84e8]]
[c -> [0x6322a15a63a0]]
[d -> [0x6322a15a63a4]]
[e -> [0x6322a15a63a8]]
[f -> [0x6322a15a9bb0]]
core after discard: 
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
ghost: 
[2 -> (papers.txt, 0)]
free_block: 
(12, 13, 14, 15, 16, 17, 18, 19, 1a, 1b, 1c, 1d, 1e, 1f, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 2a, 2b, 2c, 2d, 2e, 2f, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 3a, 3b, 3c, 3d, 3e, 3f, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 4a, 4b, 4c, 4d, 4e, 4f, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 5a, 5b, 5c, 5d, 5e, 5f, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 6a, 6b, 6c, 6d, 6e, 6f, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 7a, 7b, 7c, 7d, 7e, 7f, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 8a, 8b, 8c, 8d, 8e, 8f, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 9a, 9b, 9c, 9d, 9e, 9f, a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, aa, ab, ac, ad, ae, af, b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, ca, cb, cc, cd, ce, cf, d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, da, db, dc, dd, de, df, e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, ea, eb, ec, ed, ee, ef, f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, fa, fb, fc, fd, fe, ff, 0, 1, 2, 9, a, b, 3, 4, 5)
eblcnt: eb
vm_switch	(632845)
					returning to (632845) with r|w pages:
					rw	vpage 0x60003	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					rw	vpage 0x60009	ppage 0x2
					rw	vpage 0x6000a	ppage 0x2
vm_fault	(0x600070000, read)
core map: 
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
clock_q: 
(2, 3, 1)
pm_evict
core: 
ppage (evict): 3
[0 -> [0x6322a15a9be0]]
[3 -> [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bb8, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8]]
ppage = 3
pte = 0x6322a15a63ac
file_write	(papers.txt, 1)
filemap:
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
clock_q: 
(1, 2, 3)
alloc: 3
epage: 3
file_read	(papers.txt, 2)
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (1, [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
core map: 
[3 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[0 -> [0x6322a15a9be0]]
vm_fault returned 0
					returning to (632845) with r|w pages:
					rw	vpage 0x60003	ppage 0x2
					rw	vpage 0x60004	ppage 0x2
					rw	vpage 0x60005	ppage 0x2
					r	vpage 0x60007	ppage 0x3
					rw	vpage 0x60009	ppage 0x2
					rw	vpage 0x6000a	ppage 0x2
vm_destroy
infile size: 1d
[0x6322a15a9bbc -> ftype: FILE_B
infile: true
block: 1
filename: papers.txt
]
[0x6322a15a9bb8 -> ftype: SWAP
infile: true
block: 11
filename: @SWAP
]
[0x6322a15a9bb4 -> ftype: SWAP
infile: false
block: 10
filename: @SWAP
]
[0x6322a15a9bb0 -> ftype: SWAP
infile: true
block: f
filename: @SWAP
]
[0x6322a15a9bd8 -> ftype: FILE_B
infile: true
block: 1
filename: papers.txt
]
[0x6322a15a63ac -> ftype: FILE_B
infile: true
block: 1
filename: papers.txt
]
[0x6322a15a9bd4 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a63b0 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a84e0 -> ftype: SWAP
infile: true
block: 6
filename: @SWAP
]
[0x6322a15a84e4 -> ftype: SWAP
infile: false
block: 7
filename: @SWAP
]
[0x6322a15a84e8 -> ftype: SWAP
infile: true
block: 8
filename: @SWAP
]
[0x6322a15a84ec -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f0 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a84f8 -> ftype: FILE_B
infile: true
block: 1
filename: papers.txt
]
[0x6322a15a84fc -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a8500 -> ftype: FILE_B
infile: true
block: 1
filename: papers.txt
]
[0x6322a15a8504 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a8508 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bc0 -> ftype: FILE_B
infile: false
block: 2
filename: papers.txt
]
[0x6322a15a9bc4 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bc8 -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bdc -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a63a0 -> ftype: SWAP
infile: true
block: c
filename: @SWAP
]
[0x6322a15a9bcc -> ftype: FILE_B
infile: false
block: 0
filename: papers.txt
]
[0x6322a15a9bd0 -> ftype: FILE_B
infile: true
block: 1
filename: papers.txt
]
[0x6322a15a9be0 -> ftype: FILE_B
infile: true
block: 0
filename: qapers.txt
]
[0x6322a15a63a4 -> ftype: SWAP
infile: true
block: d
filename: @SWAP
]
[0x6322a15a63a8 -> ftype: SWAP
infile: true
block: e
filename: @SWAP
]
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a84ec, 0x6322a15a84f0, 0x6322a15a84f4, 0x6322a15a8504, 0x6322a15a8508, 0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a84f8, 0x6322a15a8500, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (3, [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
core: 
[3 -> [0x6322a15a63b0, 0x6322a15a84fc, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[0 -> [0x6322a15a9be0]]
swfile: 
[11 -> [0x6322a15a9bb8]]
[10 -> [0x6322a15a9bb4]]
[6 -> [0x6322a15a84e0]]
[7 -> [0x6322a15a84e4]]
[8 -> [0x6322a15a84e8]]
[c -> [0x6322a15a63a0]]
[d -> [0x6322a15a63a4]]
[e -> [0x6322a15a63a8]]
[f -> [0x6322a15a9bb0]]
core after discard: 
[3 -> [0x6322a15a63b0, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[0 -> [0x6322a15a9be0]]
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(0 -> (2, [0x6322a15a9bc4, 0x6322a15a9bc8, 0x6322a15a9bcc, 0x6322a15a9bdc])), (1 -> (3, [0x6322a15a63ac, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (3, [0x6322a15a63b0, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
ghost: 
[2 -> (papers.txt, 0)]
free_block: 
(12, 13, 14, 15, 16, 17, 18, 19, 1a, 1b, 1c, 1d, 1e, 1f, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 2a, 2b, 2c, 2d, 2e, 2f, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 3a, 3b, 3c, 3d, 3e, 3f, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 4a, 4b, 4c, 4d, 4e, 4f, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 5a, 5b, 5c, 5d, 5e, 5f, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 6a, 6b, 6c, 6d, 6e, 6f, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 7a, 7b, 7c, 7d, 7e, 7f, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 8a, 8b, 8c, 8d, 8e, 8f, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 9a, 9b, 9c, 9d, 9e, 9f, a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, aa, ab, ac, ad, ae, af, b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, ca, cb, cc, cd, ce, cf, d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, da, db, dc, dd, de, df, e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, ea, eb, ec, ed, ee, ef, f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, fa, fb, fc, fd, fe, ff, 0, 1, 2, 9, a, b, 3, 4, 5, 6, 7, 8)
eblcnt: ee
vm_switch	(632849)
					returning to (632849) with r|w pages:
					r	vpage 0x60004	ppage 0x3
vm_map		(0x600000000, 0)
core map: 
[3 -> [0x6322a15a63b0, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[0 -> [0x6322a15a9be0]]
clock_q: 
(1, 2, 3)
pm_evict
clock_q: 
(3, 1, 2)
alloc: 2
epage: 2
file_read	(<swap>, c)
filemap: 
[qapers.txt -> [(0 -> (0, [0x6322a15a9be0]))]]
[papers.txt -> [(1 -> (3, [0x6322a15a63ac, 0x6322a15a9bbc, 0x6322a15a9bd0, 0x6322a15a9bd8])), (2 -> (3, [0x6322a15a63b0, 0x6322a15a9bc0, 0x6322a15a9bd4]))]]
core map: 
[2 -> [0x6322a15a63a0]]
[3 -> [0x6322a15a63b0, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[0 -> [0x6322a15a9be0]]
file_str = qbqfst/uyu
vm_map returned 0x600050000
					returning to (632849) with r|w pages:
					r	vpage 0x60000	ppage 0x2
					r	vpage 0x60004	ppage 0x3
vm_map		(0x600010000, 0)
core map: 
[2 -> [0x6322a15a63a0]]
[3 -> [0x6322a15a63b0, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[0 -> [0x6322a15a9be0]]
clock_q: 
(3, 1, 2)
pm_evict
core: 
ppage (evict): 1
[2 -> [0x6322a15a63a0]]
[3 -> [0x6322a15a63b0, 0x6322a15a9bc0, 0x6322a15a9bd4]]
[0 -> [0x6322a15a9be0]]
